\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{xspace}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{nicefrac}
\usepackage{bm}

\usepackage{fullpage} % for convenience when writing, to be removed for proper editing

\numberwithin{equation}{section}

\usepackage[textwidth=2.0cm, textsize=tiny]{todonotes}
\newcommand{\gregoire}[2][]{\todo[color=orange!20,#1]{{\bf GP:} #2}}
\newcommand{\karim}[2][]{\todo[color=green!20,#1]{{\bf KL:} #2}}


% Importing all custom notations
\input{notations.tex}

% Theorem definition
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}

% changing equation numbering
\numberwithin{equation}{section}

\title{Covariance estimation with missing observation}
\author{Karim Lounici \and Grégoire Pacreau}
\date{March 2022}

\begin{document}

\maketitle

\section{Introduction}

    Let $X, X_1, \dots, X_n$ be i.i.d. zero mean vectors with unknown covariance matrix $\covariance = \expectation\left[ X \bigotimes X \right]$. Our objective is to estimate the unknown covariance matrix \covariance when the vectors $X_1, \dots, X_n$ are partially observed, that is, when some of their components are not or wrongly observed. More precisely, we consider the following framework. Denote by $X_i^{(j)}$ the $j$th component of the vector $X_i$. We assume that each component $X_i^{(j)}$ is observed independently with probability $\bernoulli \in (0,1]$. In the first part of our study, we will assume that the remaining components are not observed, i.e.:
    \begin{equation}
        \label{eqn:missing_values}
        Y_i^{(j)} = \bernoulli_{i,j}X_i^{(j)}, \qquad 1 \leq i \leq n, 1\leq j\leq p
    \end{equation}
    where $\delta_{ij}$ are independent realisations of a bernoulli random variable of parameter $\delta$. In the second part, we will assume the missing data is replaced by another independent distribution, representing either a poisoning of the data or random mistakes in measurements. The observations are then:
    \begin{equation}
        \label{eqn:contaminated}
        Y_i^{(j)} = \bernoulli_{i,j}X_i^{(j)} + (1-\bernoulli_{ij})\xi_i^{(j)}, \qquad 1 \leq i \leq n, 1\leq j\leq p
    \end{equation}
    where $\xi_1, \dots \xi_n$ are erroneous measurements following a i.i.d. subgaussian distribution.

    The missing values case corresponds to the Missing Completely At Random (MCAR) case in \cite{rubinInferenceMissingData1976}, compared with missing at random (MAR), where some dimensions are more prone to not being observed, or not missing at random (NMAR), where the lack observation is a deterministic function of the realisations of the random variables (not missing at random).\gregoire{faut-il expliquer plus pourquoi on se focalise sur ce problème?} Unbiased estimator based on maximum likelihood and the expectation maximisation algorithm exist to compute the variance with MCAR data \citep{jamshidianMLEstimationMean1999}. We focus instead on the unbiased estimator of \cite{louniciHighdimensionalCovarianceMatrix2014}, which avoids the many steps of EM like algorithms while guarantying a known rate of convergence and minimax error lower bound according to the missingness rate $\delta$. In this paper, we improve upon the theoretical results of \cite{louniciHighdimensionalCovarianceMatrix2014}.

    The contamination case relates to the Fully Independent Contamination Model (FICM) as described in \cite{alqallafPropagationOutliersMultivariate2009}, which is a direct extension of Huber contamination \citep{huberRobustEstimationLocation1964} on samples to a more cell-wise approach. While the latter is much studied in the litterature, with robust algorithms for estimating both means, such as Tukey's median \citep{tukeyNintherTechniqueLowEffort1978}, and covariance, such as the Minimum Covariance Determinant \citep{hubertMinimumCovarianceDeterminant2018} or Tukey's S-estimator \citep{rousseeuwRobustRegressionMeans1984}. On the other hand, the former is a more recent problem with much less litterature, as far as covariance estimation is concerned. Some papers propose to adapt Huber-style procedures to this generalisation \citep{farcomeniRobustConstrainedClustering2014, rousseeuwDetectingDeviatingData2018} or showcase expensive procedures based on expectation maximisation and the Mahalanobis distance \citep{raymaekersFastRobustCorrelation2021}. However, to our knowledge, only the consistency of these methods have been studied, without consideration for the rates of convergence.\gregoire{Doit-on mentionner l'absence d'étude dans le cas adversariel?}\gregoire{ajouter des exemples pratiques?}

    We focus on the high dimensional case, in which most procedures we cited fail due to both computational errors and high time requirements. In particular, we suppose that the true covariance matrix of the $X_i$ has a low rank structure. This makes the Mahalanobis distance based methods inpracticle, since it would require the inversion of a matrix with many clos to zero eigenvalues. Our proposal is base on a correction of the classical covariance estimator on $Y_1, \dots, Y_n$ first introduced in \cite{louniciHighdimensionalCovarianceMatrix2014} for the case with missing values. The procedure is based on the following observation, with $\Sigma^Y$ the covariance of the data with missing values and $\Sigma$ the true covariance:
    \begin{equation}
    \label{eqn:mvcorrection}
    \Sigma = \left( \delta^{-1} - \delta^{-2}\right) \diag (\Sigma^Y) + \delta^{-2} \Sigma^Y
    \end{equation}
    We then extend this correction to the case where the data is contaminated, first by introducing a new term, then by returning to a missing values problem by eliminating outliers using a detection procedure. We argue that the latter technique is more promissing, assuming a sufficiently accurate detection algorithm, since the existence of outliers cause the estimator to bear the full effect of the high dimension $p$, whereas ideally we would like to be constrained by the rank of the true covariance matrix.

\section{Missing Values}

    We place ourselves in the setting described in equation \ref{eqn:missing_values}. We provide an updated lower and upper bound to the estimator defined in \ref{eqn:mvcorrection}, which are sharper than those in \cite{louniciHighdimensionalCovarianceMatrix2014}.

    \subsection{Upper bound}
    \label{sec:upper_bound}

        Using the correction of equation \ref{eqn:mvcorrection} we are able to construct an unbiased estimator of the covariance matrix. In this section, we provide an upper bound of the estimation error in operator norm. This upper bound depends on the effective rank of $\Sigma$, the true covariance matrix, which is a measure of the intrinsic dimension of a symmetric matrix. The effective rank is defined as:
        \begin{equation}
            \bm{r}(\Sigma) := \frac{\expectation \norm{X}^2 }{\norm{\Sigma}} = \frac{\trace{\Sigma}}{\norm{\Sigma}}
        \end{equation}
        We can see that $0 \leq \bm{r}(\Sigma) \leq \text{rank}(\Sigma)$. Furthermore, $\bm{r}(\Sigma) \ll \text{rank}(\Sigma)$ for approximately low rank matrices, i.e. matrices with few eigenvalues significantly larger than $0$.
        
        
        \gregoire{Passer en espace de Hilbert}
        \begin{theorem}
        \label{th:upper}
        Let $X_1, \dots, X_n$ be i.i.d. subgaussian random variables in $\real^p$, with covariance matrix $\Sigma$, and let $\delta_i^j, i \in [1, n], j \in [1,p]$ be i.i.d bernoulli random variables with probability of success $\delta$. We write $Y_i = \delta_i \odot X_i$. Let $\hat{\Sigma}$ be the classical covariance estimator applied on $Y$ corrected as described in equation \ref{eqn:mvcorrection}. There exists an absolute constant $C$ such that, for $t>0$, with probability at least $1-e^{-t}$:
        \begin{equation}
            \norm{ \hat{\Sigma} - \Sigma} \leq C \frac{\norm{\Sigma}}{\delta}\left(\sqrt{\frac{\bm{r}(\Sigma)}{n}} \lor \frac{\bm{r}(\Sigma)}{n} \lor \sqrt{\frac{t}{n}} \lor \frac{t}{n}\right)
        \end{equation}
        \end{theorem}
        
        This fact is the consequence of two lemmas, under the same assumptions:
        \begin{lemma}
        \label{th:upper_Y}
            Under the same assumptions, let $\Sigma^Y = \expectation Y \otimes Y$ and $\hat{\Sigma}^Y = n^{-1}\sum_{i=1}^n Y_i\otimes Y_i$. There exist an absolute constant $c_1$ such that, for $t>0$, with probability at least $1-e^{-t}$:
            \begin{equation}
                \norm{\hat{\Sigma}^Y - \Sigma^Y} \leq c_1 \delta\norm{\Sigma}\left(\sqrt{\frac{\bm{r}(\Sigma)}{n}} \lor \frac{\bm{r}(\Sigma)}{n} \lor \sqrt{\frac{t}{n}} \lor \frac{t}{n}\right)
            \end{equation}
        \end{lemma}
        and
        \begin{lemma}
        Under the same assumptions and notations, there exist an absolute constant $c_2$ such that, for $t>0$, with probability at least $1-e^{-t}$:
        \label{th:upper_Y_diag}
        \begin{equation}
            \norm{\diag \left(\hat{\Sigma}^Y - \Sigma^Y\right)} \leq c_2 \max_j \Sigma_{jj} \left(\sqrt{\frac{t}{n}}\lor\frac{t}{n} \right)
        \end{equation}
        \end{lemma}
        We provide proof of these lemma and of the upper bound in section \ref{proof:upper}

    \subsection{Lower bound}
    \label{sec:lower_bound}

        \begin{theorem}
        \label{th:lower}
        Let $\mathcal{S}_r$ the set of all covariance matrices of rank $r$. Then:
        \begin{equation}
            \inf_{\hat{\Sigma}} \max_{\Sigma\in \mathcal{S}_r} \mathbb{P}_\Sigma \left(\norm{\hat{\Sigma} - \Sigma} \geq C \sqrt{\frac{r}{\delta n}} \right) \geq \beta
        \end{equation}
        for $C$ and $\beta$ two absolute constants and where $\inf_{\hat{\Sigma}}$ represents the infimum over all estimators of matrix$\Sigma$.
        \end{theorem}

        We see that the dependency in $\delta$ is the same in both upper and lower bounds. Furthermore, for $n$ much larger than $\bm{r}(\Sigma)$, which in the low rank case is very easy to verify, the square roots in the upper bound simplify, leading to the same structure as our lower bound. 

\section{Contaminations}

    Let us now look at the case where $Y_i^{(j)} = \delta_{ij}X^{(j)}_i + \varepsilon_{ij} \xi_i{j}$, where $X_1, \dots X_n$ are i.i.d. vectors of lax $\mathcal{N}(0, \Sigma)$ and $\xi_1, \dots \xi_n$ i.i.d. vectors following a subgaussian law of variance $\Lambda$, with $\Lambda$ a diagonal matrix of size $p$. Let also $\lambda = \max_i \Lambda_i = \norm{\Lambda}$. We suppose that the $X_i$ and the $\xi_i$ are mutually independent. However, the boolean random variables $\delta_{ij}$ and $\epsilon_{ij}$ cannot be both equal to 1 (a component cannot be both correctly observed and contaminated). This is a slight generalisation of the cell-wise contamination of \cite{alqallafPropagationOutliersMultivariate2009}, where $epsilon_{ij} = 1-\delta_{ij}$. This generalisation allows us to showcase the influence of a accurate filtering of the contaminated data, where in the filtered data we find true values with probability $\delta$ (preferably close to 1) and contaminated data with probability $\varepsilon$ (preferably close to 0). This means that we observe nothing with probability $1 - \delta - \varepsilon$.\gregoire{PEut-être que les notations $\varepsilon$ et $\delta$ portent à confusion avec les vecteurs $varepsilon_i$ et $\delta_i$?}
            
    Let $\Sigma^Y = \expectation \left( Y \otimes Y \right)$ and let $\hat{\Sigma}^Y = n^{-1}\sum_{i=1}^n Y_i \otimes Y_i$ the empirical covariance matrix. Assuming knowledge of $\Lambda$, we get the following correction formula (see section \ref{proof:formula_contaminated} for the detail):
    \begin{equation}
        \Sigma = (\delta^{-1} - \delta^{-2}) \diag \left(\Sigma^Y\right) + \delta^{-2} \Sigma^{Y} - \frac{\varepsilon}{\delta} \Lambda
    \end{equation}

    \subsection{Upper bound}

        We derive an upper bound through the following triangular inequality:        First, let us look at the error of estimation on $\Sigma^Y$. Here is a decomposition of this norm:
        \begin{equation}
            \begin{split}
            \norm{\hat{\Sigma}^Y - \Sigma^Y} & = \norm{\left(\hat{\Sigma}^\delta - \Sigma^\delta \right) + \left( \hat{\Lambda}^\varepsilon - \expectation \hat{\Lambda}^\varepsilon \right) + \hat{\Sigma}^{X, \xi, \delta, \varepsilon} }\\
            & \leq \norm{\hat{\Sigma}^\delta - \Sigma^\delta } + \norm{\hat{\Lambda}^\varepsilon - \expectation \hat{\Lambda}^\varepsilon } + \norm{\hat{\Sigma}^{X, \xi, \delta, \varepsilon} }
            \end{split}
        \end{equation}
        where the three empirical matrices are:
        \begin{enumerate}
            \item $\hat{\Sigma}^\delta = n^{-1} \sum_{i=1}^n (\delta_i \otimes \delta_i) \odot (X_i \otimes X_i)$, the empirical covariance matrix of the $\delta_i \otimes X_i$;
            \item $\hat{\Lambda}^\varepsilon = n^{-1} \sum_{i=1}^n \left( \varepsilon_i \otimes \varepsilon_i\right) \odot (\xi_i \otimes \xi_i)$, the empirical covariance of the $\varepsilon_i\odot \xi_i$;
            \item $\hat{\Sigma}^{X,\xi,\delta, \varepsilon} = n^{-1} \sum_{i=1}^n \left(\delta_i \otimes \varepsilon_i\right)\odot(X_i \otimes \xi_i) + (\varepsilon_i\otimes \delta_i)\odot(\xi_i \otimes X_i)$, the empirical covariance terms between the $\delta_i \otimes X_i$ and the $(1-\delta_i)\otimes\xi_i$, that should all convergence towards $0$.
        \end{enumerate}
        When bounding those three terms independently, and then by looking at the correction above, we find the following theorem:
        \begin{theorem}
        For $t > 0$, with probability $1 - e^{-t}$:
        \begin{equation}
            \begin{split}
            \delta\norm{\hat{\Sigma} - \Sigma} \lesssim & \left(\norm{\Sigma}+ \lambda \right)\left(\sqrt{\frac{t}{n}} \lor \frac{t}{n}\right)\\
            & + \norm{\Sigma} \left( \sqrt{\frac{\bm{r}(\Sigma)}{n}} \lor \frac{\bm{r}(\Sigma)}{n} \lor \sqrt{\frac{t}{n}} \lor \frac{t}{n}\right)\\
            & + \varepsilon p\sqrt{\lambda \norm{\Sigma}}\left(\sqrt{\frac{t}{n}} \lor \frac{t}{n}\right) \\
            & + \varepsilon \left( \sqrt{\frac{p}{n}} \lor \frac{p}{n} \lor \sqrt{\frac{t}{n}} \lor \frac{t}{n}\right)
            \end{split}
        \end{equation}
        \end{theorem}

        Observe that, even in the highly unlikely case we know about $\Lambda$, we feel the full weight of the dimension $p$ due to the contaminated data being of full rank. 

    \subsection{Lower bound}

        Just as before, we adapt the lower bound proof in the missing data case to the contaminated case.

        \begin{theorem}
        \begin{equation}
            \begin{split}
            \inf_{\hat{\Sigma}} \sup_{\mathbb{P}_\Sigma} \mathbb{P}_\Sigma \left[ \norm{\hat{\Sigma} - \Sigma} \geq C \left( \sqrt{\frac{r}{\delta n}} \land p \frac{r}{\varepsilon n}\right) \right] \geq \beta
            \end{split}
        \end{equation}
    \end{theorem}

    This lower bound also makes us bear the weight of the full dimensionalaty. Since in the high dimensional setting we seek to avoid this at all costs, we clearly need to artificially reduce the value $\varepsilon$ so that the terms in $p$ become negligeable in the upper bound. Regarding the lower bound, for $\varepsilon$ close enough to $0$, the term in $p$ will simplify if
    \begin{equation}
        \varepsilon \leq 1 \lor p \sqrt{\frac{\delta r}{n}}
    \end{equation}
    Paradoxically, the higher the number of samples, the more error is induced by the contamination.

\section{Filtering contaminations}

    We argue that cell-wise contamination can be managed by censuring contaminated data and using the missing value correction. A similar approach can be found in \cite{farcomeniRobustConstrainedClustering2014}, who develop a clustering method based on expectancy maximisation under the assumption that at least one sample from each cluster has no contamination.
    
    We combined the detection method of \cite{rousseeuwDetectingDeviatingData2018} with the missing value estimator by setting all contaminated values to zero and by counting the number of detected contaminated values to get an estimate of $\delta$. This estimator is called DDCMV in the rest of the paper. Another possible approach is to use the results of section \ref{sec:unknown_contamination} to randomly hide values in the dataset, so as to cover with high probability the outliers. This methods only works, however, if the contamination parameter is small. This estimator is called RandomMV in this paper.

    \subsection{Random covering of the error}
    \label{sec:unknown_contamination}

\karim{Qu'est-ce qu'il passe quand on fait: 1) une partition multinomiales (K classes) entrees de la matrices. 2) on calcule les estimateurs de la matrice de cov sur chaque partition. 3) on fait une sorte de mediane ou trimmed mean sur les $K$ estimateurs}

        In the previous sections, we supposed known the probability with which values are missed. Let us now set up the case with which several are contaminated at random, and we try to correct the estimation by randomly erasing values.

        In the case where the contamination probability $\varepsilon$ of a value in the dataset being recorded with error, but where the contaminated values' position in the dataset are unknown, a simple calculus informs us that in order to delete all contaminated values with probability at least $Q \in [0,1]$ one needs to set
        \begin{equation}
            \delta \leq \varepsilon^{-1}\left( 1 - M^{\nicefrac{1}{np}}\right)
        \end{equation}
        For too large an $\varepsilon$, the risk is that $\delta$ will become too small for any meaningful estimation. However, cell-wise outlier detection algorithms exist, such as the one in \cite{rousseeuwDetectingDeviatingData2018}. In section \ref{sec:empirical}, we will compare the performances of a random hiding of data with high probability and the hiding of detected outlier using this method.

% \section{Cell-wise contamination (OLD)}
% \label{sec:cell_wise}

%     We argue that the theory behind missing value estimators can help solve the problem of estimating covariances in a dataset under cell-wise contamination.
    
%     \subsection{Huber and Cell-wise contamination}
    
%     In 1967, Huber introduced a general framework for the contamination of a dataset known as $\varepsilon$-contamination \cite{huberRobustEstimationLocation1964}. This model supposes that among all rows of the dataset, an unknown but small proportion $\varepsilon$ are observed with gross error. Here is a formal definition of this form of contamination:
%     \begin{definition}
%         Let $\mathbb{P}_\Sigma$ and be the true distribution of the data, with $\Sigma$ the covariance matrix we want to estimate. Let $\mathbb{Q}$ be the distribution of the outliers. Let $X_1, \dots, X_n$ be our finite samples observed. Huber contamination means that there exists $Z_1, \dots, Z_n \sim \mathcal{B}(\varepsilon)$, such that the $(X_i, Z_i)$ are iid and, $\forall A \subset \real^p$:
%         \begin{equation}
%         \begin{cases}
%             \mathbb{P}(X_i \in A \vert Z_i = 0) = \mathbb{P}_\Sigma(A)\\
%             \mathbb{P}(X_i \in A \vert Z_i = 1) = \mathbb{Q}(A)
%         \end{cases}
%     \end{equation}
%     where $\mathcal{B}(\varepsilon)$ is the Bernoulli law of parameter $\varepsilon \in [0,1]$.
%     \end{definition}
%     This framework is well known in the field of robust statistics, with several unbiased estimators of both mean (such as Tukey's mean of median introduced in \cite{tukeyNintherTechniqueLowEffort1978}) or the covariance such as the Minimum Covariant Determinant \citep{hubertMinimumCovarianceDeterminant2018}, Tikey's S-estimator \citep{rousseeuwRobustRegressionMeans1984} or . However, there exist cases where contamination may appear independently from the row structure of our samples. \cite{alqallafPropagationOutliersMultivariate2009} presents a generalisation of the Huber $\varepsilon$-contamination, called Fully Independent Contamination Model (FICM), where instead of having entire samples being outliers, individual components may be outliers within a sample. We can present this new contamination as follows:
%     \begin{definition}
%     Using the same notations as before, FICM contamination entails that 
%     there exists a set of random variables 
%     $(Z_{i,j})_{1 \leq i \leq n, 1\leq j \leq p} \sim \mathcal{B}(\varepsilon)$, 
%     such that the $(X_i^{(j)}, Z_{i,j})$ are iid and, 
%     $\forall A \subset \real$:
%     \begin{equation}
%         \begin{cases}
%             \mathbb{P}(X_i^{(j)} \in A \vert Z_{i,j} = 0) = \mathbb{P}_{\Sigma, j}(A)\\
%             \mathbb{P}(X_i^{(j)} \in A \vert Z_{i,j} = 1) = \mathbb{Q}_j(A)
%         \end{cases}
%     \end{equation}
%     where $\mathbb{P}_{\Sigma, j}$ and $\mathbb{Q}_j$ are respectively the marginal distribution of $\mathbb{P}_\Sigma$ and $\mathbb{Q}$ at dimension $j$.
%     \end{definition}
%     For a pratical example of such a contamination, one can consider a dataset where each row corresponds to a series of measures by different sensors. There is no particular reason that an error on one sensor propagates to the others, thus abnormal readings will be restricted to a particular component of the sample.
    
%     This contamination, though similar to the Huber setting, cannot reasonably be handled in the same way. Indeed, for a given $\varepsilon$, the probability that a sample is contaminated is $1-(1-\varepsilon)^p$, which exceeds $50\%$ rapidly for high dimensional data.
    
%     Furthermore, this contamination impedes the estimation speed of covariance estimators for low rank data if the contaminated are left in the data :
%     \begin{theorem}
%     \label{th:lower_contaminated}
%     For $\Sigma$ a $\real^{p\times p}$ symmetric semi-definite matrix of rank $r < p$, and for $X$ a matrix of $n$ samples of a sub-gaussian distribution with covariance $\Sigma$ under $\varepsilon$-strong cell-wise contamination, where the contamination function is a centered sub-gaussian distribution :
%     \begin{equation}
%         \inf_{\hat{\Sigma}} \max_{\Sigma\in \mathcal{S}_r} \mathbb{P}_\Sigma \left(\norm{\hat{\Sigma} - \Sigma} \geq \right) \geq \beta
%     \end{equation}
%     \end{theorem}
%     where the dependency is in $p$ and no longer in $r$. This result shows the importance of detecting the contaminated values, in order to move from an contamination problem to a missing values problem. The demonstration of this theorem is provided in appendix \ref{proof:lower_contaminated}.

    \subsection{Robust Cell-Wise estimators}
    
    Since its introduction by \cite{alqallafPropagationOutliersMultivariate2009}, several estimators have been proposed to estimate the mean and covariance in a robust fashion under this contamination. In particular, the focus has been on detecting the outliers in the data using techniques similar to those developed for Huber contamination, but with extra steps \citep{farcomeniRobustConstrainedClustering2014, rousseeuwDetectingDeviatingData2018}. However, most techniques use altered expectation maximisation algorithms and the Mahalanobis distance \cite{raymaekersHandlingCellwiseOutliers2020}, which is impractical in both high dimensional and low rank settings. Indeed, the Mahalanobis distance requires the inversion of the covariance matrix, which is unstable in a low-rank setting setting.
    

    % 1. Of the importance of removing the missing values in a low rank setting
    % 2. comparison of estimators
    % 3. missing value estimator instead of mahalanobis based estimators
    % 
    % Justification : temps, robustesse à low rank, absorbe l'erreur sur Sigma.
    
    % Pour l'estimateur prenant en compte la contamination, montrer que pour des valeurs abherrantes il converge plus lentement, ce qui justifie le paradigme recherche de valeurs contaminée puis estimation missing values.

    
    

    \subsection{Adapting the Missing Values estimator}


    
\section{Experiments}
\label{sec:empirical}

    In this section, we test the missing value correction based estimators on two types of cell-wise contamination: one based on the bernoulli setup described in section \ref{sec:cell_wise} and another adversarial perturbation of the data. We compare the performance of the estimators with that of state of the art Huber robust estimator TSGS \citep{agostinelliRobustEstimationMultivariate2014} and cell-wise robust estimator DI \citep{raymaekersHandlingCellwiseOutliers2020}. To provide an idea of the perturbation caused by the contaminations, we also provide the monte-carlo estimated bias of the classical covariance estimator.
    
    \subsection{Contamination models}
    
        In our experiments, we test our methods on three contamination models: two FICM contaminations with the outliers following a isotropic gaussian or a uniform distribution, and an adversarial perturbation of the data.
        
        The adversarial model aims at disturbing the first eigenspace of the matrix. We find $\theta^{\text{adv}}$ a sparse projector of dimension $\epsilon * p$, such that $\theta^{\text{adv}}\theta^1 = 0$, ie the adversarial subspace and the first eigenspace are orthogonal.
    
    
    % of notice: numerical instability for low effective rank matrices, high computation cost
    
    
    % TODO: experiments on missing values
    
    % TODO: synthetic contamination 
    
    % TODO: real values (SP500?)

\section{Proof of upper bounds}
    \subsection{Tools and definitions}

    \subsubsection{Sub-exponential random vectors}
    
        We recall the definition and some basic properties of sub-exponential random vectors.
    
        \begin{definition}
            For any $\alpha \geq 1$, the $\psi_\alpha$-norms of a real-valued random variable $V$ are defined as:
            $$ \psinorm{V}{\alpha} = \inf \lbrace u> 0, \expectation\exp\left(\vert V \vert^\alpha / u^\alpha \right) \leq 2 \rbrace $$
            We say that a random variable $V$ with values in \real is sub-exponential if $\psinorm{V}{\alpha} < \infty$ for some $\alpha \geq 1$. If $\alpha = 2$, we say that $V$ is sub-Gaussian.
        \end{definition}
    
        Here are two well-known properties of sub-exponential random variables:
        \begin{enumerate}
            \item For any real-valued variable $V$ such that $\norm{V}_\alpha < \infty$ for some $\alpha > 1$, we have \[\expectation\left[ \vert V\vert^m\right] \leq 2 \frac{m}{\alpha}\Gamma\left( \frac{m}{\alpha}\right) \psinorm{V}{\alpha}^m \qquad \forall m \geq 1,\]
            where $\Gamma( \dot )$ is the Gamma function.
            \item If a real-valued random variable $V$ is sub-Gaussian, then $V^2$ is sub-exponential. Indeed, we have: \[\psinorm{V^2}{1} \leq 2 \psinorm{V}{2}^2\]
        \end{enumerate}
    
        \begin{definition}
            A random vector $X \in \real^p$ is sub-exponential if $\scalarproduct{X}{x}$ are sub-exponential random variables for $x\in \real^p$. The $\psi_\alpha$-norms of a random vector $X$ are defined as:
            \[\psinorm{X}{\alpha} = \sup_{x\in\real^p, \vert x \vert_2 = 1} \psinorm{\scalarproduct{X}{x}}{\alpha}, \qquad \alpha \geq 1\]
        \end{definition}
        
        Bernstein's inequality can be adapted to the matrix setup as follows (see corollary 5.17 in \cite{vershyninIntroductionNonasymptoticAnalysis2011}):
        \begin{proposition}
            \label{th:bernstein}
            Let $X_1, \dots X_n$ be sub-exponential random variables and $K = \max_i \psinorm{X_i}{1}$, then, for $t>0$, with probability at least $1-e^{-t}$:
            \begin{equation}
                \left\vert n^{-1}\sum_{i=1}^n Y_i \right\vert \leq CK\left(\sqrt{\frac{t}{n}}\lor \frac{t}{n} \right)
            \end{equation}
            where $C$ is an absolute constant.
        \end{proposition}

    \subsubsection{Talagrand's chaining and covariance concentration inequalities}

        Talagrand's work on generic chaining complexities for empirical processes allows for sharper upper bounds on covariance matrix estimation \citep{koltchinskiiConcentrationInequalitiesMoment2014}. In this section we introduce a form of Talagrand's theorem adapted to empirical processes.

        % SECTION HEAVILY INSPIRED BY THE PAPER
        Let $(T,d)$ be a metric space and let $N_n := 2^{2^n}, n\geq 1$ and $N_0 := 1$. We define an increasing sequence $\Delta_n$ of partitions of $T$ as admissible if, and only if, $\text{card}(\Delta_n) \leq N_n$. Given such a sequence, for $t \in T$, let $\Delta_n(t)$ be the unique set of a $\Delta_n$ containing $t$. Let us finally define:
        \begin{equation}
            \gamma_2(T,d) = \inf \sup_{t\in T} \sum_{n=0}^\infty 2^{\nicefrac{n}{2}} D\left(\Delta_n(t)\right),
        \end{equation}
        where $D$ denotes the diameter of a subset of $T$ and the infimum is taken over all admissible sequences. Talagrand's result can then be stated as follows:
        \begin{proposition}
            \label{th:talagrand}
            Let $X(t), t\in T$ be a centered Gaussian process and let, for $t,s \in T$,
            \begin{equation}
                d(t,s) := \expectation^{\nicefrac{1}{2}} \left(X(t)-X(s)\right)^2
            \end{equation}
            Then there exists an absolute constant $K>0$ such that
            \[\expectation \sup_{t\in T} X(t) \geq K^{-1} \gamma_2(T,d)\]
        \end{proposition}

        One can apply this fact to the case where $T = \mathcal{F}$ is a class of functions on a probability space $(S, \mathcal{A}, P)$ to provide upper bounds on the error when estimating covariance matrices (theorem 8, \cite{koltchinskiiConcentrationInequalitiesMoment2014}):
        \begin{proposition}
            Let $X, X_1, \dots, X_n$ be i.i.d. random variables in $S$ with distribution $P$. Let $\mathcal{F}$ be a class of measurable functions on $(S, \mathcal{A})$ such that $f\in \mathcal{F}$ implies $-f\in \mathcal{F}$ and $\expectation f(X) = 0$. Then there exist a constant $C$ such that, for $t>0$, with probability at least $1-e^{-t}$:
            \begin{equation}
                \label{th:koltchinskii}
                \sup_{f\in \mathcal{F}} \left\vert n^{-1} \sum_{i=1}^n f^2(X_i) - \expectation f^2(X) \right\vert \leq C \max \left\lbrace \sup_{f\in \mathcal{F}} \psinorm{f}{1}\frac{\gamma_2(\mathcal{F}, \psi_2)}{\sqrt{n}}, \frac{\gamma_2^2(\mathcal{F}, \psi_2)}{n}, \sup_{f\in \mathcal{F}} \psinorm{f}{1}^2\sqrt{\frac{t}{n}}, \sup_{f\in \mathcal{F}} \psinorm{f}{1}^2\frac{t}{n} \right\rbrace
            \end{equation}
        \end{proposition}

\subsection{Proof of theorem \ref{th:upper}}
\label{proof:upper}

    Let $X_1, \dots, X_n$ be i.i.d. zero mean vectors following a $\mathcal{N}(0, \Sigma)$ law, with $\Sigma$ an unknown positive definite Hermitian matrix of size $p\times p$. Let for $1\leq i\leq n$ and $1\leq j\leq p$, $\delta_{ij}$ follows an Bernoulli lax $\mathcal{B}(\delta)$, with $\delta\in [0,1]$, such that $\delta_{ij}$ is independent both from $X_i^{(j)}$, that is the $j$th component of $X_i$, and of any other Bernoulli random variable. Let finally $Y_i^{(h)} = \delta_{ij} X_i^{(j)}$ the observed random variable with missing values. We will denote by $\lesssim$ the fact that the left side term is dominated by the right side term.
    
    \subsubsection{Proof of lemma \ref{th:upper_Y}}
    \label{proof:upper_Y}

    By definition of the operator norm, we can express this error in terms of Rayleigh's quotient:
    \begin{equation} \label{eqn:rayleigh}
    \begin{split}
        \norm{n^{-1}\sum_i Y_i\otimes Y_i - \expectation\left[ Y\otimes Y\right]}_2 &= \max_{\norm{u} = 1} u \left( n^{-1}\sum_i Y_i\otimes Y_i - \expectation\left[ Y\otimes Y\right] \right)u^\top\\
        &= \max_{\norm{u}=1} n^{-1}\sum_i u (Y_i\otimes Y_i)u^\top - u  \expectation\left[ Y\otimes Y\right] u^\top\\
        &= \max_{\norm{u}=1} n^{-1}\sum_i \scalarproduct{\delta_i \odot X_i}{u}^2 - u^\top \expectation \left[Y\otimes Y\right] u
    \end{split}
    \end{equation}
    Let $X$ and $\tilde{\delta}$ be two random variables of same distribution to, respectively, the $X_i$ and $\delta_i$. We can rewrite the expectation as:
    \begin{equation}
    \begin{split}
        u^\top \expectation \left[ Y\otimes Y\right] u &= \expectation u^\top (\tilde{\delta} \odot X) \otimes (\tilde{\delta} \odot X) u\\
        &= \expectation \scalarproduct{\tilde{\delta} \odot X}{u}^2
    \end{split}
    \end{equation}
        
    Let $\mathcal{F} = \lbrace \scalarproduct{\cdot }{u}, \norm{u} \leq 1 \rbrace$. Since $X$ is subgaussian, $\tilde{\delta} \odot X$ is too. This means that the $\psi_1$ and $\psi_2$ norms of linear functionals $\scalarproduct{\tilde{\delta} \odot X}{u}$ are both equivalent to the $L_2$-norm. Thus:
    \begin{equation}
    \begin{split}
        \sup_{f\in \mathcal{F}} \psinorm{f}{1} \lesssim \sup_{\norm{u} \leq 1} \expectation^{\nicefrac{1}{2}} \scalarproduct{\tilde{\delta} \odot X}{u}^2\leq \expectation^{\nicefrac{1}{2}} \norm{\tilde{\delta}\odot X}^2 = \expectation^{\nicefrac{1}{2}} \sum_{i=1}^p \tilde{\delta}_i^2 X_i^2
    \end{split}
    \end{equation}
    Since $\tilde{\delta}$ is a Boolean vector, $\forall i, \tilde{\delta}_i^2 = \tilde{\delta}_i$. Thus, by the tower property:
    \begin{equation}
    \begin{split}
        \sup_{f\in \mathcal{F}} \psinorm{f}{1} & \lesssim \expectation^{\nicefrac{1}{2}} \expectation_{\tilde{\delta}} \sum_{i=1}^p \tilde{\delta}_i X_i^2\\
        & = \expectation^{\nicefrac{1}{2}} \delta \norm{X}^2\\
        & \leq \sqrt{\delta \norm{\Sigma}}
    \end{split}
    \end{equation}
    Now let us focus on $\gamma_2(\mathcal{F}, \psi_2)$. The norm equivalence and Talagrand's theorem (property \ref{th:talagrand}) gives:
    \begin{equation}
       \gamma_2(\mathcal{F}, \psi_2) \lesssim \gamma_2(\mathcal{F}, L_2) \lesssim \expectation \sup_{\norm{u} \leq 1} \scalarproduct{\tilde{\delta} \odot X}{u} \leq \sqrt{\delta} \expectation \norm{X}
    \end{equation}
    Thus, by proposition \ref{th:koltchinskii}, there exist an absolute constant $c_1$ such that, for $t>0$, with probability at least $1-e^{-t}$:
    \begin{equation}
    \begin{split}
        \expectation \norm{\hat{\Sigma}^Y - \Sigma^Y} & \lesssim \max \left\lbrace \sqrt{\delta} \norm{\Sigma}^{\nicefrac{1}{2}} \frac{\sqrt{\delta}\expectation\norm{X}}{\sqrt{n}} , \frac{\delta \expectation\norm{X}^2}{n}, \delta\norm{\Sigma}\sqrt{\frac{t}{n}}, \delta\norm{\Sigma} \frac{t}{n} \right\rbrace\\
        & = \delta \norm{\Sigma} \left( \sqrt{\frac{\bm{r}(\Sigma)}{n}} \lor \frac{\bm{r}(\Sigma)}{n} \lor \sqrt{\frac{t}{n}} \lor \frac{t}{n}\right)
    \end{split}
    \end{equation}
    
    \subsubsection{Proof of lemma \ref{th:upper_Y_diag}}
    
        Since taking the operator norm of a diagonal matrix is the same as taking the operator norm of the vector containing the diagonal values, we get:
        \begin{equation}
            \norm{\diag \left(n^{-1}\sum_1^n Y_i \otimes Y_i \right) - \diag\left( \expectation Y \otimes Y \right)} = \max_{j=\lbrace 1, \dots p\rbrace} \left\vert n^{-1}\sum_{i=1}^n \left(\delta_i^{(j)} X_i^{(j)}\right)^2 - \delta\Sigma_{jj}\right\vert
        \end{equation}
        Since, for a any given $j\in \lbrace 1 \dots \rbrace$, $X^{(j)}$ is sub-gaussian, we have:
        \begin{equation}
        \psinorm{\left(\delta_i^{(j)} X_i^{(j)}\right)^2}{1} \leq 2 \psinorm{\delta_i^{(j)} X_i^{(j)}}{2}^2 \leq 2 \psinorm{X_i^{(j)}}{2}^2 \leq 2c_2^{-1} \Sigma_{jj}
        \end{equation}
        for $c_2$ a constant. Bernstein's inequality as introduced in proposition \ref{th:bernstein} tells us that, for $t>0$, with probability at least $1-e^{-t}$, there exist an absolute constant $c_2$ such that:
        \begin{equation}
        \norm{\diag\left(\hat{\Sigma}^Y - \Sigma^Y\right)} \leq c_2 \max_j \Sigma_{jj} \left(\sqrt{\frac{t}{n}} \lor \frac{t}{n}\right)
        \end{equation}
        Furthermore, note that $\max_j \Sigma_{jj} \leq \norm{\Sigma}$.
        
    \subsubsection{Proof of theorem \ref{th:upper}}
    
        Now that we have proven lemmas \ref{th:upper_Y} and \ref{th:upper_Y_diag}, we can combine them to obtain the final upper bound.
        
        We are looking for an upper bound on:
        \begin{equation}
        \begin{split}
            \norm{\hat{\Sigma} - \Sigma} & = \norm{(\delta^{-1}-\delta{-2})\diag \left(\hat{\Sigma}^Y - \Sigma^Y\right) + \delta^{-2}\left(\hat{\Sigma}^Y - \Sigma^Y\right)}\\
            & \leq (\delta^{-1}-\delta^{-2}) \norm{\diag \left(\hat{\Sigma}^Y - \Sigma^Y\right)} + \delta^{-2} \norm{\hat{\Sigma}^Y - \Sigma^Y}\\
            & \leq \delta^{-1} \norm{\diag \left(\hat{\Sigma}^Y - \Sigma^Y\right)}+ \delta^{-2} \norm{\hat{\Sigma}^Y - \Sigma^Y}
        \end{split}
        \end{equation}
        Combining lemmas \ref{th:upper_Y} and \ref{th:upper_Y_diag} with a union bound argument, and by reajusting the constants, we get that, for $t>0$, with probability at least $1-e^{-t}$:
        \begin{equation}
            \norm{\hat{\Sigma} - \Sigma} \leq C \frac{\norm{\Sigma}}{\delta} \left( \sqrt{\frac{\bm{r}(\Sigma)}{n}} \lor \frac{\bm{r}(\Sigma)}{n} \lor \sqrt{\frac{t}{n}} \lor \frac{t}{n}\right)
        \end{equation}
        with $C > (c1 \lor c2)$ an absolute constant.
        
    \subsection{Proof of the upper bound in the contaminated case}
        
    \subsubsection{Bounding the error on the full matrix}
        
        Using the previous result, we know that, with probability at least $1-e^{-t}$ and for an absolute constant $C$:
        \begin{equation}
            \norm{\hat{\Sigma}^\delta - \Sigma^\delta } \leq \delta C \norm{\Sigma} \left( \sqrt{\frac{\bm{r}(\Sigma)}{n}} \lor \frac{\bm{r}(\Sigma)}{n} \lor \sqrt{\frac{t}{n}} \lor \frac{t}{n}\right)
        \end{equation}
        and
        \begin{equation}
            \norm{ \hat{\Lambda}^\varepsilon - \expectation \hat{\Lambda}^\varepsilon } \leq \varepsilon C  \left( \sqrt{\frac{p}{n}} \lor \frac{p}{n} \lor \sqrt{\frac{t}{n}} \lor \frac{t}{n}\right)
        \end{equation}
        Now we need to control the norm of $\hat{\Sigma}^{X, \xi, \delta, \varepsilon}$. Notice that this matrix has no diagonal terms, since $\delta_i^{(j)}$ and $\varepsilon_i^{(j)}$ cannot be both equal to $1$. For $l,k \in \lbrace 1, \dots, p \rbrace$, the distribution of the $d_i^l (1-d_i^k) X_i^{(l)}\xi_i^{(k)}$ is sub-exponential, since, as shown in appendix \ref{proof:orlicz_bernoulli}, 
        \begin{equation}
            \psinorm{\delta_i^l \varepsilon_i^k X_i^{(l)}\xi_i^{(k)}}{1} \leq \delta \varepsilon \psinorm{X_i^{(l)}}{2} \psinorm{\xi_i^{(k)}}{2} < \infty
        \end{equation}
        which comes from the fact both the $X_i$ and $\xi$ are sub-gaussian. Bernstein's inequality then gives:
        \begin{equation}
            \norm{\hat{\Sigma}^{X, \xi, \delta, \varepsilon}}_\text{max} \leq \max_{1 \leq j \leq p} c_3 \delta \varepsilon\sqrt{\lambda\Sigma_{jj}} \left( \sqrt{\frac{t}{n}} \lor \frac{t}{n}\right)
        \end{equation}
        
        \subsubsection{Bounding the diagonal terms}
        
        The diagonal term can be bounded using Bernstein's inequality. Since, for any given $j$, $X^{(j)} + \xi^{(j)}$ is sub-gaussian:
        \begin{equation}
            \begin{split}
            \psinorm{\left(\delta_i^{(j)}\odot X_i^{(j)} + \varepsilon_i \odot \xi_i^{(j)}\right)^2}{1} & \leq 2 \psinorm{\delta_i^{(j)} X_i^{(j)}+ \varepsilon_i \odot \xi_i^{(j)}}{2}^2 \leq \left(\psinorm{\delta_i^{(j)} X_i^{(j)}}{2} + \psinorm{\varepsilon_i^{(j)} \xi_i^{(j)}}{2}\right)^2\\
            & \leq 2 \left( \psinorm{X_i^{(j)}}{2} + \psinorm{\xi_i^{(j)}}{2} \right)^2 \leq 2c_2^{-1} (\sqrt{\Sigma_{jj}} + \sqrt{\lambda)})^2
            \end{split}
        \end{equation}
        we get, using Bernstein's inequality:
        \begin{equation}
            \norm{\diag\left(\hat{\Sigma}^Y - \Sigma^Y\right)} \leq c_2 \left(\max_j \sqrt{\Sigma_{jj}} + \sqrt{\lambda} \right)^2 \left(\sqrt{\frac{t}{n}} \lor \frac{t}{n}\right)
        \end{equation}
        
        Finally, similarly to the previous case and since the terms in $I_p$ simplify, we have that:
        \begin{equation}
            \norm{\hat{\Sigma} - \Sigma} = \norm{(\delta^{-1}-\delta^{-2})\diag \left(\hat{\Sigma}^Y - \Sigma^Y\right) + \delta^{-2}\left(\hat{\Sigma}^Y - \Sigma^Y\right)}
        \end{equation}
        
    \section{Proof of theorem \ref{th:lower}}
    \label{proof:lower_bound}
    
        This demonstration takes inspiration to the lower bound proof of \cite{louniciHighdimensionalCovarianceMatrix2014}, which we improved by changing the set of hypotheses with the ideas of \cite{koltchinskiiEstimationLowRankCovariance2015}.
        
        \subsection{Hypothesis construction in a Grassmannian manifold}
        
            Let $p\geq 2$ be the dimension of our observations and let $1\leq r \leq p$ be the intrinsic dimension of $\Sigma$. Although the problem at hand is $p$-dimensional, we are most interested in correctly estimating the $r$ eigenspaces related to the largest eigenvalues. We will thus look at $p$ dimensional matrices that are projection in $\real^p$ of $r$ dimensional kernels.
            
            We set $\gamma$ to be a constant larger than $0$. Let $H$ be a $p\times r$ matrix with orthonormal rows. Each matrix $H$ describes a subspace $U_H$ of $\real^p$, where $\dim(U_H) = r$ and $H^\top H$ is its projector in $\real^p$. The set of all $U_H$ is the Grassmannian manifold $G_r(\real^p)$, which is the set of all $r$-dimensional subspaces of $\real^p$. The Grassmannian manifold is a smooth manifold of dimension $d = r(p-r)$, where one can define a metric for all subspaces $U, \bar{U} \in G_r(\real^p)$:
            \begin{equation}
                d(U,\bar{u}) = \norm{P_U - P_{\bar{U}}}_F = \norm{H^\top H - \bar{H}^\top \bar{H}}
            \end{equation}
            where $P_U$ and $P_{\bar{U}}$ are the projectors to the subspaces $U$ and $\bar{U}$ respectively and $H$ and $\bar{H}$ are the $r\times p$ matrix with orthonormal rows associated with $U$ and $\bar{U}$ respectively. In the remainder of the proof, we will identify the projectors to the subspaces. A result on the entropy of Grassmanian manifolds \citep{pajorMetricEntropyGrassmann1998} shows that:
            
            %TODO: liaison à écrire
            \begin{proposition}
            \label{prop:entropy}
                For all $\varepsilon > 0$, there exists a family of orthonormal projectors $\mathcal{U} \subset G_r(\real^p)$ such that:
                \begin{equation}
                    \vert \mathcal{U} \vert \geq \left\lfloor \frac{\bar{c}}{\varepsilon}\right\rfloor^d
                \end{equation}
                and, $\forall P,Q \in G_r(\real^p), P \neq Q$,
                \begin{equation}
                    \bar{c} \varepsilon \sqrt{r} \leq \norm{P - Q}_F \leq \frac{\varepsilon \sqrt{r}}{\bar{c}}
                \end{equation}
                for some small enough absolute constant $\bar{c}$, where $\vert\mathcal{U}\vert$ is the cardinal of set $\mathcal{U}$.
            \end{proposition}
            Without loss of generality, we can suppose that the block matrix $P_1 = \begin{pmatrix}I_r & 0 \\ 0 & 0\end{pmatrix}$ belongs to the set. Indeed, change of basis doesn't impact the Frobenius norm.
            
            Let us then build such a set $\mathcal{U}$ of hypotheses. Let $\gamma = a \sqrt{\nicefrac{p}{\delta n}}$, for $a>0$ a constant. We set $N = \vert \mathcal{U}\vert$ and $\mathcal{U} = \lbrace P_1, \dots, P_N\rbrace$. Remember that $P_1$ is the diagonal matrix with $r$ diagonal values equal to $1$ and the rest to $0$. Let us define the family $\Sigma_1, \dots, \Sigma_N$ of symmetric $p\times p$ approximately low rank covariances matrices such that, $\forall j \in \lbrace 1, N\rbrace, \Sigma_j = \sigma I_p + \gamma P_j$, where $I_p$ is the $p$ dimensional identity matrix. These matrices are the superposition of two domains, one of low eigenvalues akin to a isotropic noise, and a one with large eigenvalues which acts as the signal. To ensure that the effective rank of those matrices is controlled by $r$, one must choose $\sigma$ sufficiently small.
            
            Then, we can see that, for $i,j \in \lbrace 1, \dots N \rbrace$, by setting $\varepsilon = \nicefrac{1}{2}$:
            \begin{equation}
            \label{eqn:min_dist_frobenius}
                \norm{\Sigma_i - \Sigma_j}_F^2 = \gamma^2 \norm{P_i - P_j}_F^2 > a^2\bar{c}^2\frac{pr}{2\delta n}
            \end{equation}
            
        \subsection{KL-divergence of hypotheses}
        
            Now that we have our candidate covariances $\Sigma_0, \dots, \Sigma_N$, let us define the associated distributions. For $j \in \lbrace 0, N\rbrace$, let $X_1, \dots X_n$ be i.i.d. random variables following a gaussian $\mathcal{N}(0, \Sigma_j)$ law. Let $\delta_1, \dots \delta_n$ be each vectors of $p$ i.i.d bernoulli random variables of probability of success $\delta$, and let $Y_1, \dots Y_n$ be random variables such that, $\forall i \in \lbrace 1, n \rbrace, Y_i = \delta_i \odot X_i$, with $\odot$ the Hadamard or term-by-term product. Let us also define as $\mathbb{P}_j$ the distribution of $Y_1, \dots Y_n$ and $\mathbb{P}_j^{(\delta)}$ the conditional distribution of the $Y_1, \dots Y_n$ knowing $\delta_1, \dots \delta_n$. Finally, let $\expectation_j$ be the expectation given the distribution associated with the $j$-th projector and $\expectation_\delta$ the expectation given $\delta_1, \dots \delta_n$.
            
            For $j\in \lbrace 2, \dots, N\rbrace$, let us compute the Kullback-Leibler divergence from $\mathbb{P}_1$ to $\mathbb{P}_j$.
            \begin{equation}
            \begin{split}
                \text{KL}(\mathbb{P}_1, \mathbb{P}_j) & = \expectation_1 \log\left(\frac{d\mathbb{P}_1}{d\mathbb{P}_j}\right) = \expectation_1 \log\left(\frac{d\mathbb{P}_\delta \otimes\mathbb{P}_1^{(\delta)}}{d\mathbb{P}_\delta \otimes\mathbb{P}_j^{(\delta)}}\right)\\
                & = \expectation_\delta \text{KL}(\mathbb{P}_1^{(\delta)}, \mathbb{P}_j^{(\delta)}) = \sum_{i=1}^n \expectation_\delta \text{KL}(\mathbb{P}_1^{(\delta_i)}, \mathbb{P}_j^{(\delta_i)})
            \end{split}
            \end{equation}
            Since $\forall i \in \lbrace 1, \dots, n\rbrace$, $Y_i\vert \delta_i \sim \mathcal{N}\left(0, (\delta_i \otimes \delta_i)\odot \Sigma\right)$, for all $j\in \lbrace 1, \dots N \rbrace$ and for each realisation $\delta(\omega) \in \lbrace 0, 1 \rbrace^p$, $\mathbb{P}_j \gg \mathbb{P}_1$, thus $\text{KL}(\mathbb{P}_1, \mathbb{P}_j) < \infty$.
            
            Define $J_i = \lbrace j: \delta_{i,k} = 1, 1 \leq k \leq p \rbrace$ the set of indices kept by vector $\delta_i$ and $d_i = \sum_{k=1}^p \delta_i^k \sim \mathcal{B}(p,\delta)$. Then, define the mapping $Q_i : \mathbb{R}^p \rightarrow \mathbb{R}^{d_i}$ such that $Q_i(x) = x_{J_i}$, such that $x_{J_i}$ is a $d_i$ dimensional vector containing the components of $x$ whose index are in $J_i$. Let $Q_i^* : \mathbb{R}^{d_i} \rightarrow \mathbb{R}^p$ the right inverse of $Q_i$.
            
            Note that $\forall j \in \lbrace 1, N-1\rbrace$, $\Sigma_j = (\sigma+\gamma) P_j + \sigma P_j^\perp$,\gregoire{Est-ce qu'on peut poser $\sigma=1$ dans le reste de la preuve? ça simplifirait la vie pour le controle du log det dans Pinsker} with $P_j^\perp$ the projector to the subspace of $\mathbb{R}^p$ orthogonal to the one described by $P_j$. Let us define $\Sigma_j^{(\delta_i)} = Q_i \Sigma_j Q_i^*$. Then, observe that $\Sigma_1^{(\delta_i)}$ is invertible, with inverse $Q_i \left(\frac{1}{\gamma+\sigma} P_1 + \frac{1}{\sigma}P_1^\perp\right)Q_i^*$ since $P_1$ and $P_1^\perp$ are diagonal matrices. We thus get, for $i\in \lbrace 1, \dots n \rbrace$:
            \begin{equation}
                \text{KL}(\mathbb{P}_1^{(\delta_i)}, \mathbb{P}_j^{(\delta_i)}) = \frac{1}{2} \left( \trace{\Sigma_1^{(\delta_i)^{-1}}\Sigma_j^{(\delta_i)} } - d_i - \log(\det (\Sigma_1^{(\delta_i)^{-1}}\Sigma_j^{(\delta_i)})) \right)
            \end{equation}
            
            First, using a result of linear algebra described in section \ref{proof:determinant}, we show that\gregoire{A revoir, des erreurs de raisonements dans la preuve}:
            \begin{equation}
                \label{eqn:pinsker}
                \begin{split}
                - \expectation_\delta \log(\det (\Sigma_1^{(\delta_i)^{-1}}\Sigma_j^{(\delta_i)})) & \leq (p-r) (\delta-1) \log \gamma + r \delta \log \sigma\\
                & \leq r \delta \log \gamma\\
                & \lesssim \frac{rp}{n}
                \end{split}
            \end{equation}
            
            Next, let us focus on bounding  $\frac{1}{2} \trace{\Sigma_1^{(\delta_i)^{-1}} (\Sigma_j^{(\delta_i)} - \Sigma_1^{(\delta_i)})}$. Using the fact that $\Sigma_1^{-1} = \frac{1}{\sigma+\gamma} P_1 + \frac{1}{\sigma} P_1^\perp$, we get:
            \begin{equation}
            \begin{split}
                \trace{\Sigma_1^{(\delta_i)^{-1}} (\Sigma_j^{(\delta_i)} - \Sigma_1^{(\delta_i)})} & = \frac{\gamma}{\sigma+\gamma} \trace{Q_i P_1 ( P_j - P_1) Q_i^*} + \frac{\gamma}{\sigma} \trace{Q_i P_1^\perp (P_j - P_1) Q_i^*}\\
                & = \frac{\gamma}{\sigma+\gamma} \left(\trace{Q_iP_1P_jQ_i^*} - \trace{Q_iP_1Q_i^*}\right)+ \frac{\gamma}{\sigma} \trace{Q_i\left(I_p - P_1\right) P_j Q_i^*}\\
                & = \left(\frac{\gamma}{\sigma+\gamma} - \frac{\gamma}{\sigma} \right) \left( \trace{Q_i P_1 P_j Q_i^*} - d_i\right)\\
                & = \frac{\gamma^2}{2(\sigma+\gamma)\sigma} \norm{Q_i(P_j - P_1)Q_i^*}_F^2
            \end{split}
            \end{equation}
            Finally, using the fact demonstrated in appendix \ref{proof:frobenius} and the upper bound of proposition \ref{prop:entropy}, we get that:
            \begin{equation}
            \begin{split}
                \text{KL}(\mathbb{P}_1, \mathbb{P}_j) & \leq \sum_{i=1}^n\expectation_\delta \frac{\gamma^2}{2(\sigma+\gamma)\sigma} \norm{Q_i(P_j - P_1)Q_i^*}_F^2\\
                & \leq \sum_{i=1}^n \frac{\gamma^2\delta}{2(\sigma+\gamma)\sigma} \norm{P_j - P_1}_F^2\\
                & \leq \sum_{i=1}^n \frac{\gamma^2 \delta r }{8\bar{c}^2(\sigma+\gamma)\sigma}\\
                & = \frac{a^2 r p}{8 \bar{c}^2(\sigma+\gamma)\sigma}
            \end{split}
            \end{equation}
            Thus, since $N \geq \lfloor 2 \bar{c} \rfloor ^{r(p-r)}$, and as we can set, w.l.o.g. $p > 2r$:
            \begin{equation}
                \text{KL}(\mathbb{P}_1, \mathbb{P}_j) \leq \alpha\log (N)
            \end{equation}
            for any $\alpha$ so long as $a>0$ is chosen small enough. Along with equation \ref{eqn:min_dist_frobenius}, according to theorem 2.5 of \cite{tsybakovNonparametricEstimators2009}, we get that:
            \begin{equation}
            \inf_{\hat{\Sigma}} \sup_{\mathbb{P}_\Sigma} \mathbb{P}_\Sigma \left( \norm{\hat{\Sigma} - \Sigma}_F^2 \geq C \frac{r}{\delta n}p \right) \geq \beta
            \end{equation}
            with $C$ and $\beta$ two absolute constants. This fact, in turn, implies the lower bound of theorem \ref{th:lower}, since, for all $\Sigma_1, \Sigma_2$ matrices of our hypothesis set:
            \begin{equation}
                \norm{\Sigma_1-\Sigma_2}^2 \geq C \frac{r}{\delta n}
            \end{equation}
            Indeed, otherwise, we would get:
            \begin{equation}
                \norm{\Sigma_1-\Sigma_2}_F^2 < p\norm{\Sigma_1-\Sigma_2}^2 < C\frac{r}{\delta n}p
            \end{equation}
            which contradicts equation \ref{eqn:min_dist_frobenius}.
        
        \subsection{Proof of theorem \ref{th:lower_contaminated}}
            \label{proof:lower_contaminated}
            
            This proof is based on that of theorem 2 in \cite{louniciHighdimensionalCovarianceMatrix2014}, where we change slightly the construction of the distributions of which we bound the KL-divergence.
            
            Let $X_1, \dots X_n \in \real^p$ be i.i.d. samples of $\mathcal{N}(0, \Sigma_j)$, with $\Sigma_j \in \mathcal{A}^0$. Let $\varepsilon_1, \dots \varepsilon_0$ be random vectors with i.i.d. entries $\varepsilon_i,j$ following $B(\varepsilon)$. Finally, let $\delta_i = 1-\varepsilon_i$, and :
            \begin{equation}
                Y_i = \delta_i \odot X_i + \varepsilon_i \odot \xi
            \end{equation}
            with $\xi_1, \dots \xi_n$ be i.i.d. samples of a distribution with variance the diagonal matrix $V$.
            
            We can show that the empirical variance $\Sigma_1^{(\delta_i)}$ is a diagonal matrix, with $d_i$ diagonal terms with value $\gamma$, $d^\prime_i$ values equal to 1 and $p-d_i-d'_i$ values equal to the variance of the noise term, where $d_i \sim \mathcal{B}(r, \delta)$ and $d'_i \sim \mathcal{B}(p-r, \delta)$ representing the values that the mask $\delta_i$ has kept in the subspace $P_1$ and $P_1^\top$ respectively, the rest being filled with the contamination. This shows that this matrix is invertible and, as done in the previous section, we get:
            
            \begin{equation}
                    \text{KL}\left( \mathbb{P}_1^{(\delta_i)}, \mathbb{P}_j^{(\delta_i)}\right) =  \frac{1}{2} \trace{\Sigma_1^{(\delta_i)^{-1}} (\Sigma_j^{(\delta_i)} - \Sigma_1^{(\delta_i)})}
            \end{equation}
            However, here we have that $\Sigma_1^-1 = (\delta_i \otimes \delta_i) \odot\left((1+\gamma)^{-1}P_1 + P_1^\top\right) + (\varepsilon_i \otimes \varepsilon_i)\odot V^{-1}$. Thus:
            \begin{equation}
                \begin{split}
                \trace{\Sigma_1^{(\delta_i)^{-1}} (\Sigma_j^{(\delta_i)} - \Sigma_1^{(\delta_i)})} = & \left(\frac{\gamma}{1+\gamma} - \gamma\right)\trace{(\delta_i \otimes \delta_i) \odot \left(P_1P_j - I_p\right)}\\
                & + \gamma\trace{(\varepsilon_i \otimes \varepsilon_i) \odot \left(V^{-1}\left(P_j - P_1\right)\right) }
                \end{split}
            \end{equation}
            We already know that
            \begin{equation}
                \begin{split}
                \expectation_\delta\trace{(\delta_i \otimes \delta_i) \odot \left(P_1P_j - I_p\right)} & = \expectation_\delta \norm{Q_i(P_j - P_1)Q_i^*}_F^2 \leq \frac{(1 - \varepsilon) r}{4\bar{c}^2}
            \end{split}
            \end{equation}
            Furthermore, due to the same reasoning as in appendix \ref{proof:frobenius} and using the fact that $V$ is diagonal:
            \begin{equation}
                \begin{split}
                \expectation_\varepsilon \trace{(\varepsilon_i \otimes \varepsilon_i) \odot \left(V^{-1}\left(P_j - P_1\right)\right)} & \leq \varepsilon \trace{V^{-1} \left( P_j - P_1 \right)}\\
                & \leq \varepsilon r \left(\max_i{V^{-1}_i} - \min_i{V^{-1}_i} \right)
                \end{split}
            \end{equation}
            which gives by property \ref{prop:entropy}:
            \begin{equation}
                \begin{split}
                \expectation_\varepsilon \trace{(\varepsilon_i \otimes \varepsilon_i) \odot \left(V^{-1}\left(P_j - P_1\right)\right)} \leq \left(\max_i{V^{-1}_i} - \min_i{V^{-1}_i} \right) \frac{\varepsilon \sqrt{r}}{2 \bar{c}}
                \end{split}
            \end{equation}
            Remember that $\delta = 1 - \varepsilon$. Let us also call $b=\left(\max_i{V^{-1}_i} - \min_i{V^{-1}_i} \right)$, which is a constant. Thus, by setting $\gamma = a \left(\sqrt{\frac{p}{\delta n}} \land \frac{p\sqrt{r}}{\varepsilon n}\right)$, we get:
            \begin{equation}
                \begin{split}
                \expectation_\delta \text{KL}\left(\mathbb{P}_1^{(\delta_i)}, \mathbb{P}_j^{(\delta_i)} \right) \leq \frac{rp}{n}\left[ \frac{a^2}{4\bar{c}^2(1+\gamma)} + \frac{ab}{2 \bar{c}} \right]
                \end{split}
            \end{equation}
            For $a$ sufficiently small we verify the upper bound condition of theorem 2.5 of \cite{tsybakovNonparametricEstimators2009}. Given this expression of $\gamma$, we find the lower bound condition: $\forall i, j \in \lbrace 1, N \rbrace$
            \begin{equation}
                \begin{split}
                \norm{\Sigma_i - \Sigma_j}_F^2 \leq \gamma^2 \norm{P_i - P_j}_F^2 \leq a^2 r p \left( \frac{1}{ (1-\varepsilon)n} \land \frac{pr}{\varepsilon^2 n^2}\right)
                \end{split}
            \end{equation}
            Thus, we have that:
            \begin{equation}
                \begin{split}
                \inf_{\hat{\Sigma}} \sup_{\mathbb{P}_\Sigma} \mathbb{P}_\Sigma \left[ \norm{\hat{\Sigma} - \Sigma}_F^2 \geq C \left( \frac{rp}{(1-\varepsilon) n} \land \left(\frac{rp}{\varepsilon n}\right)^2\right) \right] \geq \beta
                \end{split}
            \end{equation}
            and 
            \begin{equation}
                \begin{split}
                \inf_{\hat{\Sigma}} \sup_{\mathbb{P}_\Sigma} \mathbb{P}_\Sigma \left[ \norm{\hat{\Sigma} - \Sigma}^2 \geq C \left( \frac{r}{(1-\varepsilon) n} \land p \left(\frac{r}{\varepsilon n}\right)^2\right) \right] \geq \beta
                \end{split}
            \end{equation}
            
            
\section{Other proofs}
        
        \subsection[Orlicz 1-norm of the components of the covariance between contaminated and uncontaminated data]{Orlicz 1-norm of the components of $\Sigma^{X, \xi, \delta}$}
        \label{proof:orlicz_bernoulli}
            
            Let $X$ and $\xi$ be two one dimensional random variables following a sub-gaussian distribution, and let $d$ be a bernoulli random variable of mean $\delta$. The Orlicz $\psi_1$ norm of $d (1-d) X \xi$ is:
            \begin{equation}
                \begin{split}
                \psinorm{d (1-d) X \xi}{1} & = \inf \lbrace u> 0, \expectation\exp\left(\vert d (1-d) X \xi \vert / u \right) \leq 2 \rbrace\\
                    & = \inf \lbrace u> 0, \expectation\exp\left(d (1-d)\vert X \xi \vert / u \right) \leq 2 \rbrace
                    \end{split}
            \end{equation}
            Since the bernoulli variables are binary. By the tower property and Jensen equality, we have that, $\forall u$ such that the expectation is well defined :
            \begin{equation}
                \begin{split}
                \expectation\exp\left(d (1-d) \vert X \xi \vert / u \right) & = \expectation\expectation_\delta \exp\left(\vert d (1-d) X \xi \vert / u \right)\\
                & \geq \expectation\exp\left(\delta(1-\delta)\vert X \xi \vert / u \right)
                \end{split}
            \end{equation}
            which implies that
            \begin{equation}
                \lbrace u> 0, \expectation\exp\left(\delta (1-\delta)\vert X \xi \vert / u \right) \leq 2 \rbrace \subset \lbrace u> 0, \expectation\exp\left(d (1-d)\vert X \xi \vert / u \right) \leq 2 \rbrace 
            \end{equation}
            With a simple change of variable, one can see that:
            \begin{equation}
                \begin{split}
                \inf \lbrace u> 0, \expectation\exp\left(\delta (1-\delta)\vert X \xi \vert / u \right) \leq 2 \rbrace & = \delta(1-\delta)\inf\lbrace  u > 0, \expectation\exp\left(\vert X \xi \vert / u \right) \leq 2 \rbrace\\
                & = \delta(1-\delta) \psinorm{X \xi}{1}\\
                & \leq \delta (1-\delta) \psinorm{X}{2} \psinorm{\xi}{2}
                \end{split}
            \end{equation}
            Hence:
            \begin{equation}
                \inf \lbrace u> 0, \expectation\exp\left(d (1-d)\vert X \xi \vert / u \right) \leq 2 \rbrace \leq \delta (1-\delta) \psinorm{X}{2} \psinorm{\xi}{2}
            \end{equation}
        
        \subsection{Proof of the correction formula of equation \ref{eqn:correction_formula}}
        \label{proof:formula_contaminated}
            
        Let $Y = (\delta_1 \odot X^{(1)} + (1-\delta_1)\odot \xi^{(1)}, \dots , \delta_n \odot X^{(n)} + (1-\delta_n)\odot \xi^{(n)})$ with $X$ and $\xi$ in $\real^{n\times p}$ and $\delta$ some $p$ dimensional binary vector.
            
        Thus, 
        \begin{equation}
            (Y \otimes Y)_{jk} = \begin{cases}
                \left(X^{(j)}\right)^2 & \text{ if $j = k$ and $\delta_j = 1$}\\
                \left(\xi^{(j)}\right)^2 & \text{ if $j = k$ and $\delta_j = 0$}\\
                \delta_j \delta_k X^{(j)}X^{(k)}& \text{ otherwise}
            \end{cases}
        \end{equation}
            
        This means that we have:
        \begin{equation}
            \Sigma^Y_{jk} = \expectation \left( Y \otimes Y \right)_{jk} = \begin{cases}
                \delta \Sigma_{jj} + (1 - \delta) V_j & \text{ if $j = k$}\\
                \delta^2 \Sigma_{jk}& \text{ otherwise}
            \end{cases}
        \end{equation}
        Thus:
        \begin{equation}
            \Sigma_{jk} = \begin{cases}
                \delta^{-1} \left(\Sigma^Y_{jj} - (1-\delta) V_j\right) & \text{ if $j = k$}\\
                \delta^{-2} \Sigma^Y_{jk}& \text{ otherwise}
            \end{cases}
        \end{equation}
        Which in turn means that:
        \begin{equation}
            \Sigma = (\delta^{-1} - \delta^{-2}) \diag(\Sigma^Y) + \delta^{-2} \Sigma^Y + (1 - \delta^{-1}) V
        \end{equation}
        Which gives the general correction formula with independent contamination. For the missing values correction, simply set $V = 0_P$ the $p\times p$ zero matrix.
        
        \subsection{bounds on the determinant of in equation \ref{eqn:pinsker}}
        \label{proof:determinant}
        
            Theorem 13 of  \cite{thompsonPrincipalSubmatricesNormal1966} states that, for any matrix $A$ of size $p$ with eigenvalues $\lambda_1, \dots \lambda_s$, each with multiplicity $\mu_1, \dots \mu_s$ such that $\sum_{i=1}^s \mu_i = p$, then any principal submatrix $A(j\vert j)$, that is, a matrix created by removing line $j$ and column $j$ from $A$, has eigenvalues $\lambda_i$ with multiplicity $\max (0, \mu_i - 1)$. The other eigenvalues have values between $\min_i \lambda_i$ and $\max_i \lambda_i$. 
            
            In our case, the matrix $\Sigma_j$ has only two eigenvalues: $\gamma$ and $\sigma$, with multiplicity $r$ and $p-r$ respectively. One will easily find by recurrence on the number of deleted dimensions that, with $d_i = \sum_{j=1}^p \delta_i^{(j)}$
            \begin{equation}
            \det \Sigma_j^{(\delta_i)} = \gamma^{\max (0, r-d_i)} \sigma^{\max(0, p-r-d_i)} \prod_{k=1}^{d_i} \mu_k
            \end{equation}
            where $\forall k \in \lbrace 1, d_i \rbrace$, $\sigma \leq \mu_k \leq \gamma$.
            
            This means, in particular, that:
            \begin{equation}
                \gamma^{\max (0, r-d_i)} \sigma^{\max(p-r, p-d_i)} \leq \det \Sigma_j^{(\delta_i)} \leq \gamma^{\max (r, p-d_i)} \sigma^{\max(0, p-r-d_i)} 
            \end{equation}
            Now, let us demonstrate the statement in equation \ref{eqn:pinsker}. We have $\Sigma_1$ and $\Sigma_j$ having the same eigenvalues $\gamma$ and $\sigma$ with multiplicity respectively $r$ and $p-r$. Let $d_i = \sum_{k=1}^p \delta_i^k$ be the number of deleted components after applying the boolean filter $\delta_i$. Since $\Sigma_1$ is diagonal, we know that $\Sigma_1^{(\delta_i)}$ will also have eigenvalues $\gamma$ and $\sigma$, with multiplicity $a_i$ and $b_i$ respectively, where $a_i \sim \mathcal{B}(r, \delta)$ and $b_i \sim \mathcal{B}(p-r, \delta)$ where $\mathcal{B}$ is the binomial distribution.
            
            Then, using the lower bound we just demonstrated, we get that:
            \begin{equation}
                \begin{split}
                - \expectation_\delta \log \left(\det \left(\Sigma_1^{(\delta_i)-1} \Sigma_j^{(\delta_i)} \right) \right) & = \expectation_\delta a_i \log(\gamma) + b_i \log(\sigma) - \log \left( \det \left(\Sigma_j^{(\delta_i)} \right)\right)\\
                & \leq \expectation_\delta a_i \log(\gamma) + b_i \log(\sigma) -  \max (0, r-d_i)\log(\gamma) - \max(p-r, p-d_i)\log ( \sigma ) \\
                & \leq \left( r\delta + \min (0, -r) \right) \log (\gamma) + \left( (p-r)\delta + \min(r-p, - p)\right) \log (\sigma)\\
                & \leq r \delta \log(\gamma) + (p-r)(\delta - 1) \log(\sigma)\\
                & \leq r \delta \log(\gamma)
                \end{split}
            \end{equation}
            since $\delta - 1 \leq 0$. In particular, one can easily see that $\log(x) \leq \frac{1}{2} x^2$ for all positive $x$, thus
            \begin{equation}
                \expectation_\delta \log \left(\det \left(\Sigma_1^{(\delta_i)-1} \Sigma_j^{(\delta_i)} \right) \right) \leq r \delta \gamma^2
            \end{equation}
            hence the result.
        
        \subsection{Proof of the upper bound of the frobenius norm with missing values}
        \label{proof:frobenius}
        
            Let $P\in \real^{p\times p}$ be any matrix, then, using the fact that the $\delta_i$ are boolean vectors:
            \begin{equation}
            \begin{split}
            \expectation_\delta\norm{(\delta_i \otimes \delta_i) \odot P}_F^2 & = \expectation_\delta \trace{\left((\delta_i \otimes \delta_i) \odot P\right)^\top \left((\delta_i \otimes \delta_i) \odot P\right)}\\
            & = \expectation_\delta \sum_{k=1}^p \sum_{l = 1}^p \delta_i^k\delta_i^l P_{kl}^2\\
            & = \sum_{k = 1}^p \left( \delta P_{kk} + \sum_{\substack{l=1\\l\neq k}}^p \delta^2 P_{kl}^2\right)\\
            & \leq \delta\norm{P}_F^2
            \end{split}
            \end{equation}

\begin{table}
    \caption{Relative spectral difference between estimated covariance matrices of the 13 features of sklearn's Wine dataset. Notice the difference between the classical covariance matrix and the other robust estimators.}
    \begin{tabular*}[t]{c | c c c c }
        \toprule
        relative & \multirow{2}{c}{DDCMV95} & \multirow{2}{c}{DDCMV90} & \multirow{2}{c}{TSGS} & \multirow{2}{c}{DI}\\
        error to & \\
        \midrule
        Classical & 25.2 & 26.3 & 18.7 & 26.8\\
        DDCMV95 & - & 3.80 & 9.95 & 10.1\\
        TSGS & - & - & - & 11.8 \\
        \bottomrule
    \end{tabular*}
\end{table}


                
\bibliographystyle{apalike}
\bibliography{biblio.bib}

\end{document}