
@unpublished{akhtarThreatAdversarialAttacks2018,
  title = {Threat of {{Adversarial Attacks}} on {{Deep Learning}} in {{Computer Vision}}: {{A Survey}}},
  shorttitle = {Threat of {{Adversarial Attacks}} on {{Deep Learning}} in {{Computer Vision}}},
  author = {Akhtar, Naveed and Mian, Ajmal},
  date = {2018-02-26},
  eprint = {1801.00553},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1801.00553},
  urldate = {2022-03-10},
  abstract = {Deep learning is at the heart of the current rise of artificial intelligence. In the field of Computer Vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has recently lead to a large influx of contributions in this direction. This article presents the first comprehensive survey on adversarial attacks on deep learning in Computer Vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, drawing on the reviewed literature, we provide a broader outlook of this research direction.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/gregoirepacreau/Zotero/storage/ZZMHICSJ/Akhtar et Mian - 2018 - Threat of Adversarial Attacks on Deep Learning in .pdf}
}

@misc{arroyoDynamicPortfolioCuts2021,
  title = {Dynamic {{Portfolio Cuts}}: {{A Spectral Approach}} to {{Graph-Theoretic Diversification}}},
  shorttitle = {Dynamic {{Portfolio Cuts}}},
  author = {Arroyo, Alvaro and Scalzo, Bruno and Stankovic, Ljubisa and Mandic, Danilo P.},
  date = {2021-06-07},
  number = {arXiv:2106.03417},
  eprint = {2106.03417},
  eprinttype = {arxiv},
  primaryclass = {eess, q-fin},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.03417},
  url = {http://arxiv.org/abs/2106.03417},
  urldate = {2022-06-30},
  abstract = {Stock market returns are typically analyzed using standard regression, yet they reside on irregular domains which is a natural scenario for graph signal processing. To this end, we consider a market graph as an intuitive way to represent the relationships between financial assets. Traditional methods for estimating asset-return covariance operate under the assumption of statistical time-invariance, and are thus unable to appropriately infer the underlying true structure of the market graph. This work introduces a class of graph spectral estimators which cater for the nonstationarity inherent to asset price movements, and serve as a basis to represent the time-varying interactions between assets through a dynamic spectral market graph. Such an account of the time-varying nature of the asset-return covariance allows us to introduce the notion of dynamic spectral portfolio cuts, whereby the graph is partitioned into time-evolving clusters, allowing for online and robust asset allocation. The advantages of the proposed framework over traditional methods are demonstrated through numerical case studies using real-world price data.},
  archiveprefix = {arXiv},
  keywords = {Electrical Engineering and Systems Science - Signal Processing,Quantitative Finance - Portfolio Management},
  file = {/Users/gregoirepacreau/Zotero/storage/HKQIWAAQ/Arroyo et al. - 2021 - Dynamic Portfolio Cuts A Spectral Approach to Gra.pdf;/Users/gregoirepacreau/Zotero/storage/5UL4E65B/2106.html}
}

@unpublished{bartlettAdversarialExamplesMultiLayer2021,
  title = {Adversarial {{Examples}} in {{Multi-Layer Random ReLU Networks}}},
  author = {Bartlett, Peter L. and Bubeck, Sébastien and Cherapanamjeri, Yeshwanth},
  date = {2021-06-23},
  eprint = {2106.12611},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2106.12611},
  urldate = {2022-03-10},
  abstract = {We consider the phenomenon of adversarial examples in ReLU networks with independent gaussian parameters. For networks of constant depth and with a large range of widths (for instance, it suffices if the width of each layer is polynomial in that of any other layer), small perturbations of input vectors lead to large changes of outputs. This generalizes results of Daniely and Schacham (2020) for networks of rapidly decreasing width and of Bubeck et al (2021) for two-layer networks. The proof shows that adversarial examples arise in these networks because the functions that they compute are very close to linear. Bottleneck layers in the network play a key role: the minimal width up to some point in the network determines scales and sensitivities of mappings computed up to that point. The main result is for networks with constant depth, but we also show that some constraint on depth is necessary for a result of this kind, because there are suitably deep networks that, with constant probability, compute a function that is close to constant.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/QXUCLJNM/Bartlett et al. - 2021 - Adversarial Examples in Multi-Layer Random ReLU Ne.pdf}
}

@article{bianchiGraphNeuralNetworks2021,
  title = {Graph {{Neural Networks}} with Convolutional {{ARMA}} Filters},
  author = {Bianchi, Filippo Maria and Grattarola, Daniele and Livi, Lorenzo and Alippi, Cesare},
  date = {2021},
  journaltitle = {IEEE Trans. Pattern Anal. Mach. Intell.},
  eprint = {1901.01343},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {1--1},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2021.3054830},
  url = {http://arxiv.org/abs/1901.01343},
  urldate = {2022-06-28},
  abstract = {Popular graph neural networks implement convolution operations on graphs based on polynomial spectral filters. In this paper, we propose a novel graph convolutional layer inspired by the auto-regressive moving average (ARMA) filter that, compared to polynomial ones, provides a more flexible frequency response, is more robust to noise, and better captures the global graph structure. We propose a graph neural network implementation of the ARMA filter with a recursive and distributed formulation, obtaining a convolutional layer that is efficient to train, localized in the node space, and can be transferred to new graphs at test time. We perform a spectral analysis to study the filtering effect of the proposed ARMA layer and report experiments on four downstream tasks: semi-supervised node classification, graph signal classification, graph classification, and graph regression. Results show that the proposed ARMA layer brings significant improvements over graph neural networks based on polynomial filters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/QUY3FLIL/Bianchi et al. - 2021 - Graph Neural Networks with convolutional ARMA filt.pdf;/Users/gregoirepacreau/Zotero/storage/GMPSKQKZ/1901.html}
}

@misc{bronsteinGeometricDeepLearning2021,
  title = {Geometric {{Deep Learning}}: {{Grids}}, {{Groups}}, {{Graphs}}, {{Geodesics}}, and {{Gauges}}},
  shorttitle = {Geometric {{Deep Learning}}},
  author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar},
  date = {2021-05-02},
  number = {arXiv:2104.13478},
  eprint = {2104.13478},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.13478},
  url = {http://arxiv.org/abs/2104.13478},
  urldate = {2022-06-28},
  abstract = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Geometry,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/K7ZAUCKA/Bronstein et al. - 2021 - Geometric Deep Learning Grids, Groups, Graphs, Ge.pdf;/Users/gregoirepacreau/Zotero/storage/L6IZAQLB/2104.html}
}

@misc{calkinStochasticFlowDiagrams2014,
  type = {SSRN Scholarly Paper},
  title = {Stochastic {{Flow Diagrams}}},
  author = {Calkin, Neil and Lopez de Prado, Marcos},
  date = {2014-02-08},
  number = {2379314},
  location = {{Rochester, NY}},
  doi = {10.2139/ssrn.2379314},
  url = {https://papers.ssrn.com/abstract=2379314},
  urldate = {2022-06-30},
  abstract = {Inspired by visualization techniques à la Feynman, we introduce Stochastic Flow Diagrams (SFDs), a new mathematical approach to represent complex dynamic systems into a single weighted digraph. This topological representation provides a way to visualize what otherwise would be a morass of equations in differences. SFDs model the propagation and reverberation that follows a shock. For example, reverberation explains how a shock to a financial system can initiate a sequence of events that lead to a crash long after the occurrence of the shock. SFDs can simulate systems in stable, steady or explosive state. SFDs add Topology to the Statistical and Econometric toolkit. We believe that SFDs will help policy makers, investors and researchers communicate and discuss better the complexity of dynamic systems.},
  langid = {english},
  keywords = {Financial Flows,Graph Theory,Macro Trading,Time Series,Topology},
  file = {/Users/gregoirepacreau/Zotero/storage/INAV5IQX/Calkin et Lopez de Prado - 2014 - Stochastic Flow Diagrams.pdf;/Users/gregoirepacreau/Zotero/storage/TUZ4WA6G/papers.html}
}

@unpublished{chenRobustCovarianceScatter2017,
  title = {Robust {{Covariance}} and {{Scatter Matrix Estimation}} under {{Huber}}'s {{Contamination Model}}},
  author = {Chen, Mengjie and Gao, Chao and Ren, Zhao},
  date = {2017-06-12},
  eprint = {1506.00691},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  url = {http://arxiv.org/abs/1506.00691},
  urldate = {2022-04-06},
  abstract = {Covariance matrix estimation is one of the most important problems in statistics. To accommodate the complexity of modern datasets, it is desired to have estimation procedures that not only can incorporate the structural assumptions of covariance matrices, but are also robust to outliers from arbitrary sources. In this paper, we define a new concept called matrix depth and then propose a robust covariance matrix estimator by maximizing the empirical depth function. The proposed estimator is shown to achieve minimax optimal rate under Huber's \$\textbackslash epsilon\$-contamination model for estimating covariance/scatter matrices with various structures including bandedness and sparsity.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/Users/gregoirepacreau/Zotero/storage/6SK4W7JS/Chen et al. - 2017 - Robust Covariance and Scatter Matrix Estimation un.pdf;/Users/gregoirepacreau/Zotero/storage/D52XC582/1506.html}
}

@misc{cohenEconomicLinksPredictable2007,
  type = {SSRN Scholarly Paper},
  title = {Economic {{Links}} and {{Predictable Returns}}},
  author = {Cohen, Lauren and Frazzini, Andrea},
  date = {2007-05-23},
  number = {2758776},
  location = {{Rochester, NY}},
  doi = {10.2139/ssrn.2758776},
  url = {https://papers.ssrn.com/abstract=2758776},
  urldate = {2022-06-30},
  abstract = {This paper finds evidence of return predictability across economically linked firms. We test the hypothesis that in the presence of investors subject to attention constraints, stock prices do not promptly incorporate news about economically related firms, generating return predictability across assets. We use a dataset of firms’ principal customers to identify a set of economically related firms, and show that stock prices do not incorporate news involving related firms, generating predictable subsequent price moves. A long/short equity strategy based on this effect yields monthly alphas of over 150 basis points.},
  langid = {english},
  keywords = {G10,G11,G14},
  file = {/Users/gregoirepacreau/Zotero/storage/9GEARBEM/Cohen et Frazzini - 2007 - Economic Links and Predictable Returns.pdf}
}

@inproceedings{deesPortfolioCutsGraphTheoretic2020,
  title = {Portfolio {{Cuts}}: {{A Graph-Theoretic Framework}} to {{Diversification}}},
  shorttitle = {Portfolio {{Cuts}}},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Dees, Bruno Scalzo and Stanković, Ljubiša and Constantinides, Anthony G. and Mandic, Danilo P.},
  date = {2020-05},
  pages = {8454--8458},
  issn = {2379-190X},
  doi = {10.1109/ICASSP40776.2020.9054371},
  abstract = {Investment returns naturally reside on irregular domains, however, standard multivariate portfolio optimization methods are agnostic to data structure. To this end, we investigate ways for domain knowledge to be conveniently incorporated into the analysis, by means of graphs. Next, to relax the assumption of the completeness of graph topology and to equip the graph model with practically relevant physical intuition, we introduce the portfolio cut paradigm. Such a graph-theoretic portfolio partitioning technique is shown to allow the investor to devise robust and tractable asset allocation schemes, by virtue of a rigorous graph framework for considering smaller, computationally feasible, and economically meaningful clusters of assets, based on graph cuts. In turn, this makes it possible to fully utilize the asset returns covariance matrix for constructing the portfolio, even without the requirement for its inversion. The advantages of the proposed framework over traditional methods are demonstrated through numerical simulations based on real-world price data.},
  eventtitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  keywords = {Covariance matrices,Financial signal processing,graph cut,graph signal processing,portfolio optimization,Portfolios,Resource management,Simulation,Speech processing,Standards,Topology,vertex clustering},
  file = {/Users/gregoirepacreau/Zotero/storage/UP79WWCR/Dees et al. - 2020 - Portfolio Cuts A Graph-Theoretic Framework to Div.pdf;/Users/gregoirepacreau/Zotero/storage/HESTG73X/9054371.html}
}

@misc{defferrardConvolutionalNeuralNetworks2017,
  title = {Convolutional {{Neural Networks}} on {{Graphs}} with {{Fast Localized Spectral Filtering}}},
  author = {Defferrard, Michaël and Bresson, Xavier and Vandergheynst, Pierre},
  date = {2017-02-05},
  number = {arXiv:1606.09375},
  eprint = {1606.09375},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1606.09375},
  url = {http://arxiv.org/abs/1606.09375},
  urldate = {2022-06-28},
  abstract = {In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/JWH8YSZG/Defferrard et al. - 2017 - Convolutional Neural Networks on Graphs with Fast .pdf;/Users/gregoirepacreau/Zotero/storage/AABTTANR/1606.html}
}

@misc{derrSignedGraphConvolutional2018,
  title = {Signed {{Graph Convolutional Network}}},
  author = {Derr, Tyler and Ma, Yao and Tang, Jiliang},
  date = {2018-08-20},
  number = {arXiv:1808.06354},
  eprint = {1808.06354},
  eprinttype = {arxiv},
  primaryclass = {physics},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1808.06354},
  url = {http://arxiv.org/abs/1808.06354},
  urldate = {2022-06-28},
  abstract = {Due to the fact much of today's data can be represented as graphs, there has been a demand for generalizing neural network models for graph data. One recent direction that has shown fruitful results, and therefore growing interest, is the usage of graph convolutional neural networks (GCNs). They have been shown to provide a significant improvement on a wide range of tasks in network analysis, one of which being node representation learning. The task of learning low-dimensional node representations has shown to increase performance on a plethora of other tasks from link prediction and node classification, to community detection and visualization. Simultaneously, signed networks (or graphs having both positive and negative links) have become ubiquitous with the growing popularity of social media. However, since previous GCN models have primarily focused on unsigned networks (or graphs consisting of only positive links), it is unclear how they could be applied to signed networks due to the challenges presented by negative links. The primary challenges are based on negative links having not only a different semantic meaning as compared to positive links, but their principles are inherently different and they form complex relations with positive links. Therefore we propose a dedicated and principled effort that utilizes balance theory to correctly aggregate and propagate the information across layers of a signed GCN model. We perform empirical experiments comparing our proposed signed GCN against state-of-the-art baselines for learning node representations in signed networks. More specifically, our experiments are performed on four real-world datasets for the classical link sign prediction problem that is commonly used as the benchmark for signed network embeddings algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Social and Information Networks,Physics - Physics and Society},
  file = {/Users/gregoirepacreau/Zotero/storage/928YM93P/Derr et al. - 2018 - Signed Graph Convolutional Network.pdf;/Users/gregoirepacreau/Zotero/storage/NRFZBHWX/1808.html}
}

@article{fazlyabEfficientAccurateEstimation,
  title = {Efficient and {{Accurate Estimation}} of {{Lipschitz Constants}} for {{Deep Neural Networks}}},
  author = {Fazlyab, Mahyar and Robey, Alexander and Hassani, Hamed and Morari, Manfred and Pappas, George J},
  pages = {12},
  abstract = {Tight estimation of the Lipschitz constant for deep neural networks (DNNs) is useful in many applications ranging from robustness certification of classifiers to stability analysis of closed-loop systems with reinforcement learning controllers. Existing methods in the literature for estimating the Lipschitz constant suffer from either lack of accuracy or poor scalability. In this paper, we present a convex optimization framework to compute guaranteed upper bounds on the Lipschitz constant of DNNs both accurately and efficiently. Our main idea is to interpret activation functions as gradients of convex potential functions. Hence, they satisfy certain properties that can be described by quadratic constraints. This particular description allows us to pose the Lipschitz constant estimation problem as a semidefinite program (SDP). The resulting SDP can be adapted to increase either the estimation accuracy (by capturing the interaction between activation functions of different layers) or scalability (by decomposition and parallel implementation). We illustrate the utility of our approach with a variety of experiments on randomly generated networks and on classifiers trained on the MNIST and Iris datasets. In particular, we experimentally demonstrate that our Lipschitz bounds are the most accurate compared to those in the literature. We also study the impact of adversarial training methods on the Lipschitz bounds of the resulting classifiers and show that our bounds can be used to efficiently provide robustness guarantees.},
  langid = {english},
  file = {/Users/gregoirepacreau/Zotero/storage/NEHU3NAI/Fazlyab et al. - Efﬁcient and Accurate Estimation of Lipschitz Cons.pdf}
}

@unpublished{goodfellowExplainingHarnessingAdversarial2015,
  title = {Explaining and {{Harnessing Adversarial Examples}}},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  date = {2015-03-20},
  eprint = {1412.6572},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1412.6572},
  urldate = {2022-03-10},
  abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/MLVNJ2IJ/Goodfellow et al. - 2015 - Explaining and Harnessing Adversarial Examples.pdf;/Users/gregoirepacreau/Zotero/storage/D9FHPI3F/1412.html}
}

@misc{hamiltonInductiveRepresentationLearning2018,
  title = {Inductive {{Representation Learning}} on {{Large Graphs}}},
  author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
  date = {2018-09-10},
  number = {arXiv:1706.02216},
  eprint = {1706.02216},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.02216},
  url = {http://arxiv.org/abs/1706.02216},
  urldate = {2022-06-28},
  abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/I73B7576/Hamilton et al. - 2018 - Inductive Representation Learning on Large Graphs.pdf;/Users/gregoirepacreau/Zotero/storage/4N5XRFAP/1706.html}
}

@misc{hippert-ferrerRobustLowrankCovariance2021,
  title = {Robust Low-Rank Covariance Matrix Estimation with a General Pattern of Missing Values},
  author = {Hippert-Ferrer, Alexandre and Korso, Mohammed Nabil El and Breloy, Arnaud and Ginolhac, Guillaume},
  date = {2021-11-23},
  number = {arXiv:2107.10505},
  eprint = {2107.10505},
  eprinttype = {arxiv},
  primaryclass = {eess, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.10505},
  url = {http://arxiv.org/abs/2107.10505},
  urldate = {2022-07-04},
  abstract = {This paper tackles the problem of robust covariance matrix estimation when the data is incomplete. Classical statistical estimation methodologies are usually built upon the Gaussian assumption, whereas existing robust estimation ones assume unstructured signal models. The former can be inaccurate in real-world data sets in which heterogeneity causes heavy-tail distributions, while the latter does not profit from the usual low-rank structure of the signal. Taking advantage of both worlds, a covariance matrix estimation procedure is designed on a robust (mixture of scaled Gaussian) low-rank model by leveraging the observed-data likelihood function within an expectation-maximization algorithm. It is also designed to handle general pattern of missing values. The proposed procedure is first validated on simulated data sets. Then, its interest for classification and clustering applications is assessed on two real data sets with missing values, which include multispectral and hyperspectral time series.},
  archiveprefix = {arXiv},
  keywords = {Electrical Engineering and Systems Science - Signal Processing,Statistics - Methodology},
  file = {/Users/gregoirepacreau/Zotero/storage/Q7S9PLJZ/Hippert-Ferrer et al. - 2021 - Robust low-rank covariance matrix estimation with .pdf;/Users/gregoirepacreau/Zotero/storage/66AVIXNM/2107.html}
}

@unpublished{huOpenGraphBenchmark2021,
  title = {Open {{Graph Benchmark}}: {{Datasets}} for {{Machine Learning}} on {{Graphs}}},
  shorttitle = {Open {{Graph Benchmark}}},
  author = {Hu, Weihua and Fey, Matthias and Zitnik, Marinka and Dong, Yuxiao and Ren, Hongyu and Liu, Bowen and Catasta, Michele and Leskovec, Jure},
  date = {2021-02-24},
  eprint = {2005.00687},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2005.00687},
  urldate = {2022-03-10},
  abstract = {We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale, encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at https://ogb.stanford.edu .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/L3FGLQWL/Hu et al. - 2021 - Open Graph Benchmark Datasets for Machine Learnin.pdf;/Users/gregoirepacreau/Zotero/storage/A8F5262N/2005.html}
}

@unpublished{josseConsistencySupervisedLearning2020,
  title = {On the Consistency of Supervised Learning with Missing Values},
  author = {Josse, Julie and Prost, Nicolas and Scornet, Erwan and Varoquaux, Gaël},
  date = {2020-07-03},
  eprint = {1902.06931},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1902.06931},
  urldate = {2022-05-09},
  abstract = {In many application settings, the data have missing entries which make analysis challenging. An abundant literature addresses missing values in an inferential framework: estimating parameters and their variance from incomplete tables. Here, we consider supervised-learning settings: predicting a target when missing values appear in both training and testing data. We show the consistency of two approaches in prediction. A striking result is that the widely-used method of imputing with a constant, such as the mean prior to learning is consistent when missing values are not informative. This contrasts with inferential settings where mean imputation is pointed at for distorting the distribution of the data. That such a simple approach can be consistent is important in practice. We also show that a predictor suited for complete observations can predict optimally on incomplete data,through multiple imputation.Finally, to compare imputation with learning directly with a model that accounts for missing values, we analyze further decision trees. These can naturally tackle empirical risk minimization with missing values, due to their ability to handle the half-discrete nature of incomplete variables. After comparing theoretically and empirically different missing values strategies in trees, we recommend using the "missing incorporated in attribute" method as it can handle both non-informative and informative missing values.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/XLQR46AB/Josse et al. - 2020 - On the consistency of supervised learning with mis.pdf;/Users/gregoirepacreau/Zotero/storage/MWAHE82A/1902.html}
}

@article{katzProvingAdversarialRobustness2017,
  title = {Towards {{Proving}} the {{Adversarial Robustness}} of {{Deep Neural Networks}}},
  author = {Katz, Guy and Barrett, Clark and Dill, David L. and Julian, Kyle and Kochenderfer, Mykel J.},
  date = {2017-09-07},
  journaltitle = {Electron. Proc. Theor. Comput. Sci.},
  volume = {257},
  pages = {19--26},
  issn = {2075-2180},
  doi = {10.4204/EPTCS.257.3},
  url = {http://arxiv.org/abs/1709.02802v1},
  urldate = {2022-03-10},
  langid = {english},
  file = {/Users/gregoirepacreau/Zotero/storage/3EDG47BC/Katz et al. - 2017 - Towards Proving the Adversarial Robustness of Deep.pdf}
}

@misc{kayaEccentricityAssetManagement2013,
  type = {SSRN Scholarly Paper},
  title = {Eccentricity in {{Asset Management}}},
  author = {Kaya, Hakan},
  date = {2013-12-05},
  number = {2350429},
  location = {{Rochester, NY}},
  doi = {10.2139/ssrn.2350429},
  url = {https://papers.ssrn.com/abstract=2350429},
  urldate = {2022-06-30},
  abstract = {We describe how networks based on information theory can help measure and visualize systemic risk, enhance diversification, and help price assets.  To do this, we first define a distance measure based on the mutual information between asset pairs and use this measure in the construction of minimum spanning trees.  The dynamics of the shape and the descriptive statistics of these trees are analyzed in various investment domains. The method provides evidence of regime changes in dependency structures prior to market sell-offs, and as such, it is a potential candidate for monitoring systemic risk.  We also provide empirical evidence that the assets that are located towards the center of the network tend to have higher returns.  Finally, an investment strategy that utilizes network centrality information is shown to add value historically.},
  langid = {english},
  keywords = {minimum,portfolio allocation,risk management,Risk measurement},
  file = {/Users/gregoirepacreau/Zotero/storage/4ECCZDLX/Kaya - 2013 - Eccentricity in Asset Management.pdf;/Users/gregoirepacreau/Zotero/storage/BJFXHCJC/papers.html}
}

@unpublished{kipfSemiSupervisedClassificationGraph2017,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  author = {Kipf, Thomas N. and Welling, Max},
  date = {2017-02-22},
  eprint = {1609.02907},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1609.02907},
  urldate = {2022-03-10},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/99A8VZ4S/Kipf et Welling - 2017 - Semi-Supervised Classification with Graph Convolut.pdf;/Users/gregoirepacreau/Zotero/storage/A23J73VW/1609.html}
}

@misc{koltchinskiiConcentrationInequalitiesMoment2014,
  title = {Concentration {{Inequalities}} and {{Moment Bounds}} for {{Sample Covariance Operators}}},
  author = {Koltchinskii, Vladimir and Lounici, Karim},
  date = {2014-07-01},
  number = {arXiv:1405.2468},
  eprint = {1405.2468},
  eprinttype = {arxiv},
  primaryclass = {math},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1405.2468},
  url = {http://arxiv.org/abs/1405.2468},
  urldate = {2022-07-04},
  abstract = {Let \$X,X\_1,\textbackslash dots, X\_n,\textbackslash dots\$ be i.i.d. centered Gaussian random variables in a separable Banach space \$E\$ with covariance operator \$\textbackslash Sigma:\$ \$\$ \textbackslash Sigma:E\^\{\textbackslash ast\}\textbackslash mapsto E,\textbackslash{} \textbackslash{} \textbackslash Sigma u = \{\textbackslash mathbb E\}\textbackslash langle X,u\textbackslash rangle, u\textbackslash in E\^\{\textbackslash ast\}. \$\$ The sample covariance operator \$\textbackslash hat \textbackslash Sigma:E\^\{\textbackslash ast\}\textbackslash mapsto E\$ is defined as \$\$ \textbackslash hat \textbackslash Sigma u := n\^\{-1\}\textbackslash sum\_\{j=1\}\^n \textbackslash langle X\_j,u\textbackslash rangle X\_j, u\textbackslash in E\^\{\textbackslash ast\}. \$\$ The goal of the paper is to obtain concentration inequalities and expectation bounds for the operator norm \$\textbackslash |\textbackslash hat \textbackslash Sigma-\textbackslash Sigma\textbackslash |\$ of the deviation of the sample covariance operator from the true covariance operator. In particular, it is shown that \$\$ \{\textbackslash mathbb E\}\textbackslash |\textbackslash hat \textbackslash Sigma-\textbackslash Sigma\textbackslash |\textbackslash asymp \textbackslash |\textbackslash Sigma\textbackslash |\textbackslash biggl(\textbackslash sqrt\{\textbackslash frac\{\{\textbackslash bf r\}(\textbackslash Sigma)\}\{n\}\}\textbackslash bigvee \textbackslash frac\{\{\textbackslash bf r\}(\textbackslash Sigma)\}\{n\}\textbackslash biggr), \$\$ where \$\$ \{\textbackslash bf r\}(\textbackslash Sigma):=\textbackslash frac\{\textbackslash Bigl(\{\textbackslash mathbb E\}\textbackslash |X\textbackslash |\textbackslash Bigr)\^2\}\{\textbackslash |\textbackslash Sigma\textbackslash |\}. \$\$ Moreover, under the assumption that \$\{\textbackslash bf r\}(\textbackslash Sigma)\textbackslash lesssim n,\$ it is proved that, for all \$t\textbackslash geq 1,\$ with probability at least \$1-e\^\{-t\}\$ \textbackslash begin\{align*\} \textbackslash Bigl|\textbackslash |\textbackslash hat\textbackslash Sigma - \textbackslash Sigma\textbackslash |-\{\textbackslash mathbb E\}\textbackslash |\textbackslash hat\textbackslash Sigma - \textbackslash Sigma\textbackslash |\textbackslash Bigr| \textbackslash lesssim \textbackslash |\textbackslash Sigma\textbackslash |\textbackslash biggl(\textbackslash sqrt\{\textbackslash frac\{t\}\{n\}\}\textbackslash bigvee \textbackslash frac\{t\}\{n\}\textbackslash biggr). \textbackslash end\{align*\}},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Probability},
  file = {/Users/gregoirepacreau/Zotero/storage/6JLHUEPS/Koltchinskii et Lounici - 2014 - Concentration Inequalities and Moment Bounds for S.pdf;/Users/gregoirepacreau/Zotero/storage/AJ2W9PKY/1405.html}
}

@article{koltchinskiiLowRankEstimation2013,
  title = {Low Rank Estimation of Smooth Kernels on Graphs},
  author = {Koltchinskii, Vladimir and Rangel, Pedro},
  date = {2013-04},
  journaltitle = {The Annals of Statistics},
  volume = {41},
  number = {2},
  pages = {604--640},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/13-AOS1088},
  url = {https://projecteuclid.org/journals/annals-of-statistics/volume-41/issue-2/Low-rank-estimation-of-smooth-kernels-on-graphs/10.1214/13-AOS1088.full},
  urldate = {2022-03-11},
  abstract = {Let \$(V,A)\$ be a weighted graph with a finite vertex set \$V\$, with a symmetric matrix of nonnegative weights \$A\$ and with Laplacian \$\textbackslash Delta\$. Let \$S\_\{\textbackslash ast\}: V\textbackslash times V\textbackslash mapsto\{\textbackslash mathbb\{R\}\}\$ be a symmetric kernel defined on the vertex set \$V\$. Consider \$n\$ i.i.d. observations \$(X\_\{j\},X\_\{j\}',Y\_\{j\})\$, \$j=1,\textbackslash ldots,n\$, where \$X\_\{j\}\$, \$X\_\{j\}'\$ are independent random vertices sampled from the uniform distribution in \$V\$ and \$Y\_\{j\}\textbackslash in\{\textbackslash mathbb\{R\}\}\$ is a real valued response variable such that \$\{\textbackslash mathbb\{E\}\}(Y\_\{j\}|X\_\{j\},X\_\{j\}')=S\_\{\textbackslash ast\}(X\_\{j\},X\_\{j\}')\$, \$j=1,\textbackslash ldots,n\$. The goal is to estimate the kernel \$S\_\{\textbackslash ast\}\$ based on the data \$(X\_\{1\},X\_\{1\}',Y\_\{1\}),\textbackslash ldots,(X\_\{n\},X\_\{n\}',Y\_\{n\})\$ and under the assumption that \$S\_\{\textbackslash ast\}\$ is low rank and, at the same time, smooth on the graph (the smoothness being characterized by discrete Sobolev norms defined in terms of the graph Laplacian). We obtain several results for such problems including minimax lower bounds on the \$L\_\{2\}\$-error and upper bounds for penalized least squares estimators both with nonconvex and with convex penalties.},
  keywords = {60B20,60G15,62H12,62J99,discrete Sobolev norm,graph Laplacian,low-rank matrix estimation,Matrix completion,matrix Lasso,minimax error bound,nuclear norm,optimal error rate},
  file = {/Users/gregoirepacreau/Zotero/storage/7UFN9BVD/Koltchinskii et Rangel - 2013 - Low rank estimation of smooth kernels on graphs.pdf;/Users/gregoirepacreau/Zotero/storage/7VHVYWS2/13-AOS1088.html}
}

@article{louniciHighdimensionalCovarianceMatrix2014,
  title = {High-Dimensional Covariance Matrix Estimation with Missing Observations},
  author = {Lounici, Karim},
  date = {2014-08},
  journaltitle = {Bernoulli},
  volume = {20},
  number = {3},
  pages = {1029--1058},
  publisher = {{Bernoulli Society for Mathematical Statistics and Probability}},
  issn = {1350-7265},
  doi = {10.3150/12-BEJ487},
  url = {https://projecteuclid.org/journals/bernoulli/volume-20/issue-3/High-dimensional-covariance-matrix-estimation-with-missing-observations/10.3150/12-BEJ487.full},
  urldate = {2022-03-24},
  abstract = {In this paper, we study the problem of high-dimensional covariance matrix estimation with missing observations. We propose a simple procedure computationally tractable in high-dimension and that does not require imputation of the missing data. We establish non-asymptotic sparsity oracle inequalities for the estimation of the covariance matrix involving the Frobenius and the spectral norms which are valid for any setting of the sample size, probability of a missing observation and the dimensionality of the covariance matrix. We further establish minimax lower bounds showing that our rates are minimax optimal up to a logarithmic factor.},
  keywords = {Covariance matrix,Lasso,low-rank matrix estimation,missing observations,non-commutative Bernstein inequality,Optimal rate of convergence},
  file = {/Users/gregoirepacreau/Zotero/storage/6JB9WFM7/Lounici - 2014 - High-dimensional covariance matrix estimation with.pdf;/Users/gregoirepacreau/Zotero/storage/ILYFUVPB/12-BEJ487.html}
}

@inproceedings{mitliagkasMemoryLimitedStreaming2013,
  title = {Memory {{Limited}}, {{Streaming PCA}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Mitliagkas, Ioannis and Caramanis, Constantine and Jain, Prateek},
  date = {2013},
  volume = {26},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2013/hash/76cf99d3614e23eabab16fb27e944bf9-Abstract.html},
  urldate = {2022-06-21},
  file = {/Users/gregoirepacreau/Zotero/storage/99VR3REA/Mitliagkas et al. - 2013 - Memory Limited, Streaming PCA.pdf}
}

@book{mohriFoundationsMachineLearning2012,
  title = {Foundations of {{Machine Learning}}},
  author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  date = {2012-08-17},
  series = {Adaptive {{Computation}} and {{Machine Learning}} Series},
  publisher = {{MIT Press}},
  location = {{Cambridge, MA, USA}},
  abstract = {Fundamental topics in machine learning are presented along with theoretical and conceptual tools for the discussion and proof of algorithms.},
  editorb = {Bach, Francis},
  editorbtype = {redactor},
  isbn = {978-0-262-01825-8},
  langid = {english},
  pagetotal = {432}
}

@unpublished{mukherjeeLepskiMethodAdaptive2016,
  title = {Lepski's {{Method}} and {{Adaptive Estimation}} of {{Nonlinear Integral Functionals}} of {{Density}}},
  author = {Mukherjee, Rajarshi and Tchetgen, Eric Tchetgen and Robins, James},
  date = {2016-01-11},
  eprint = {1508.00249},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  url = {http://arxiv.org/abs/1508.00249},
  urldate = {2022-04-06},
  abstract = {We study the adaptive minimax estimation of non-linear integral functionals of a density and extend the results obtained for linear and quadratic functionals to general functionals. The typical rate optimal non-adaptive minimax estimators of "smooth" non-linear functionals are higher order U-statistics. Since Lepski's method requires tight control of tails of such estimators, we bypass such calculations by a modification of Lepski's method which is applicable in such situations. As a necessary ingredient, we also provide a method to control higher order moments of minimax estimator of cubic integral functionals. Following a standard constrained risk inequality method, we also show the optimality of our adaptation rates.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory},
  file = {/Users/gregoirepacreau/Zotero/storage/SZ6HEHA5/Mukherjee et al. - 2016 - Lepski's Method and Adaptive Estimation of Nonline.pdf;/Users/gregoirepacreau/Zotero/storage/879EWPN4/1508.html}
}

@article{pauliTrainingRobustNeural2022,
  title = {Training Robust Neural Networks Using {{Lipschitz}} Bounds},
  author = {Pauli, Patricia and Koch, Anne and Berberich, Julian and Kohler, Paul and Allgöwer, Frank},
  date = {2022},
  journaltitle = {IEEE Control Syst. Lett.},
  volume = {6},
  eprint = {2005.02929},
  eprinttype = {arxiv},
  pages = {121--126},
  issn = {2475-1456},
  doi = {10.1109/LCSYS.2021.3050444},
  url = {http://arxiv.org/abs/2005.02929},
  urldate = {2022-03-10},
  abstract = {Due to their susceptibility to adversarial perturbations, neural networks (NNs) are hardly used in safety-critical applications. One measure of robustness to such perturbations in the input is the Lipschitz constant of the input-output map defined by an NN. In this work, we propose a framework to train multi-layer NNs while at the same time encouraging robustness by keeping their Lipschitz constant small, thus addressing the robustness issue. More specifically, we design an optimization scheme based on the Alternating Direction Method of Multipliers that minimizes not only the training loss of an NN but also its Lipschitz constant resulting in a semidefinite programming based training procedure that promotes robustness. We design two versions of this training procedure. The first one includes a regularizer that penalizes an accurate upper bound on the Lipschitz constant. The second one allows to enforce a desired Lipschitz bound on the NN at all times during training. Finally, we provide two examples to show that the proposed framework successfully increases the robustness of NNs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/LISPR8AB/Pauli et al. - 2022 - Training robust neural networks using Lipschitz bo.pdf}
}

@book{pearlCausality2009,
  title = {Causality},
  author = {Pearl, Judea},
  date = {2009},
  edition = {2},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}},
  doi = {10.1017/CBO9780511803161},
  url = {https://www.cambridge.org/core/books/causality/B0046844FAE10CBF274D4ACBDAEB5F5B},
  urldate = {2022-06-30},
  abstract = {Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. Cited in more than 2,100 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interest to students and professionals in a wide variety of fields. Dr Judea Pearl has received the 2011 Rumelhart Prize for his leading research in Artificial Intelligence (AI) and systems from The Cognitive Science Society.},
  isbn = {978-0-521-89560-6},
  file = {/Users/gregoirepacreau/Zotero/storage/NMSLGMLA/B0046844FAE10CBF274D4ACBDAEB5F5B.html}
}

@article{pinotTheoreticalEvidenceAdversarial,
  title = {Theoretical Evidence for Adversarial Robustness through Randomization},
  author = {Pinot, Rafael and Meunier, Laurent and Araujo, Alexandre and Kashima, Hisashi and Yger, Florian and Gouy-Pailler, Cédric and Atif, Jamal},
  pages = {11},
  abstract = {This paper investigates the theory of robustness against adversarial attacks. It focuses on the family of randomization techniques that consist in injecting noise in the network at inference time. These techniques have proven effective in many contexts, but lack theoretical arguments. We close this gap by presenting a theoretical analysis of these approaches, hence explaining why they perform well in practice. More precisely, we make two new contributions. The first one relates the randomization rate to robustness to adversarial attacks. This result applies for the general family of exponential distributions, and thus extends and unifies the previous approaches. The second contribution consists in devising a new upper bound on the adversarial risk gap of randomized neural networks. We support our theoretical claims with a set of experiments.},
  langid = {english},
  file = {/Users/gregoirepacreau/Zotero/storage/ASV3SBTG/Pinot et al. - Theoretical evidence for adversarial robustness th.pdf}
}

@online{ProblemAdaptiveEstimation,
  title = {On a {{Problem}} of {{Adaptive Estimation}} in {{Gaussian White Noise}}},
  doi = {10.1137/1135065},
  url = {https://epubs.siam.org/doi/epdf/10.1137/1135065},
  urldate = {2022-04-07},
  langid = {english},
  file = {/Users/gregoirepacreau/Zotero/storage/MCL97QNJ/1135065.html}
}

@article{quFindingSparseVector2016,
  title = {Finding a Sparse Vector in a Subspace: {{Linear}} Sparsity Using Alternating Directions},
  shorttitle = {Finding a Sparse Vector in a Subspace},
  author = {Qu, Qing and Sun, Ju and Wright, John},
  date = {2016-10},
  journaltitle = {IEEE Trans. Inform. Theory},
  volume = {62},
  number = {10},
  eprint = {1412.4659},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  pages = {5855--5880},
  issn = {0018-9448, 1557-9654},
  doi = {10.1109/TIT.2016.2601599},
  url = {http://arxiv.org/abs/1412.4659},
  urldate = {2022-05-24},
  abstract = {Is it possible to find the sparsest vector (direction) in a generic subspace \$\textbackslash mathcal\{S\} \textbackslash subseteq \textbackslash mathbb\{R\}\^p\$ with \$\textbackslash mathrm\{dim\}(\textbackslash mathcal\{S\})= n {$<$} p\$? This problem can be considered a homogeneous variant of the sparse recovery problem, and finds connections to sparse dictionary learning, sparse PCA, and many other problems in signal processing and machine learning. In this paper, we focus on a **planted sparse model** for the subspace: the target sparse vector is embedded in an otherwise random subspace. Simple convex heuristics for this planted recovery problem provably break down when the fraction of nonzero entries in the target sparse vector substantially exceeds \$O(1/\textbackslash sqrt\{n\})\$. In contrast, we exhibit a relatively simple nonconvex approach based on alternating directions, which provably succeeds even when the fraction of nonzero entries is \$\textbackslash Omega(1)\$. To the best of our knowledge, this is the first practical algorithm to achieve linear scaling under the planted sparse model. Empirically, our proposed algorithm also succeeds in more challenging data models, e.g., sparse dictionary learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/MJ5UCLG9/Qu et al. - 2016 - Finding a sparse vector in a subspace Linear spar.pdf;/Users/gregoirepacreau/Zotero/storage/5N4RSLSA/1412.html}
}

@unpublished{raymaekersHandlingCellwiseOutliers2020,
  title = {Handling Cellwise Outliers by Sparse Regression and Robust Covariance},
  author = {Raymaekers, Jakob and Rousseeuw, Peter J.},
  date = {2020-12-07},
  eprint = {1912.12446},
  eprinttype = {arxiv},
  primaryclass = {stat},
  url = {http://arxiv.org/abs/1912.12446},
  urldate = {2022-03-29},
  abstract = {We propose a data-analytic method for detecting cellwise outliers. Given a robust covariance matrix, outlying cells (entries) in a row are found by the cellHandler technique which combines lasso regression with a stepwise application of constructed cutoff values. The penalty term of the lasso has a physical interpretation as the total distance that suspicious cells need to move in order to bring their row into the fold. For estimating a cellwise robust covariance matrix we construct a detection-imputation method which alternates between flagging outlying cells and updating the covariance matrix as in the EM algorithm. The proposed methods are illustrated by simulations and on real data about volatile organic compounds in children.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {/Users/gregoirepacreau/Zotero/storage/C3HIHA3B/Raymaekers et Rousseeuw - 2020 - Handling cellwise outliers by sparse regression an.pdf;/Users/gregoirepacreau/Zotero/storage/4NJLLBKP/1912.html}
}

@article{rousseeuwDetectingDeviatingData2018,
  title = {Detecting Deviating Data Cells},
  author = {Rousseeuw, Peter J. and den Bossche, Wannes Van},
  date = {2018-04-03},
  journaltitle = {Technometrics},
  volume = {60},
  number = {2},
  eprint = {1601.07251},
  eprinttype = {arxiv},
  primaryclass = {stat},
  pages = {135--145},
  issn = {0040-1706, 1537-2723},
  doi = {10.1080/00401706.2017.1340909},
  url = {http://arxiv.org/abs/1601.07251},
  urldate = {2022-07-18},
  abstract = {A multivariate dataset consists of \$n\$ cases in \$d\$ dimensions, and is often stored in an \$n\$ by \$d\$ data matrix. It is well-known that real data may contain outliers. Depending on the situation, outliers may be (a) undesirable errors which can adversely affect the data analysis, or (b) valuable nuggets of unexpected information. In statistics and data analysis the word outlier usually refers to a row of the data matrix, and the methods to detect such outliers only work when at least half the rows are clean. But often many rows have a few contaminated cell values, which may not be visible by looking at each variable (column) separately. We propose the first method to detect deviating data cells in a multivariate sample which takes the correlations between the variables into account. It has no restriction on the number of clean rows, and can deal with high dimensions. Other advantages are that it provides estimates of the `expected' values of the outlying cells, while imputing missing values at the same time. We illustrate the method on several real data sets, where it uncovers more structure than found by purely columnwise methods or purely rowwise methods. The proposed method can help to diagnose why a certain row is outlying, e.g. in process control. It may also serve as an initial step for estimating multivariate location and scatter matrices.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/gregoirepacreau/Zotero/storage/GQ2UPGXG/Rousseeuw et Bossche - 2018 - Detecting deviating data cells.pdf;/Users/gregoirepacreau/Zotero/storage/2GLUECA7/1601.html}
}

@unpublished{suRobustVariableSelection2021,
  title = {Robust {{Variable Selection}} under {{Cellwise Contamination}}},
  author = {Su, Peng and Tarr, Garth and Muller, Samuel},
  date = {2021-10-24},
  eprint = {2110.12406},
  eprinttype = {arxiv},
  primaryclass = {stat},
  url = {http://arxiv.org/abs/2110.12406},
  urldate = {2022-03-29},
  abstract = {Cellwise outliers are widespread in data and traditional robust methods may fail when applied to datasets under such contamination. We propose a variable selection procedure, that uses a pairwise robust estimator to obtain an initial empirical covariance matrix among the response and potentially many predictors. Then we replace the primary design matrix and the response vector with their robust counterparts based on the estimated covariance matrix. Finally, we adopt the adaptive Lasso to obtain variable selection results. The proposed approach is robust to cellwise outliers in regular and high dimensional settings and empirical results show good performance in comparison with recently proposed alternative robust approaches, particularly in the challenging setting when contamination rates are high but the magnitude of outliers is moderate. Real data applications demonstrate the practical utility of the proposed method.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {/Users/gregoirepacreau/Zotero/storage/YD4FH53L/Su et al. - 2021 - Robust Variable Selection under Cellwise Contamina.pdf;/Users/gregoirepacreau/Zotero/storage/FWBWNEAL/2110.html}
}

@incollection{tsybakovAsymptoticEfficiencyAdaptation2009,
  title = {Asymptotic Efficiency and Adaptation},
  booktitle = {Introduction to {{Nonparametric Estimation}}},
  author = {Tsybakov, Alexandre B.},
  editor = {Tsybakov, Alexandre B.},
  date = {2009},
  series = {Springer {{Series}} in {{Statistics}}},
  pages = {137--190},
  publisher = {{Springer}},
  location = {{New York, NY}},
  doi = {10.1007/978-0-387-79052-7_3},
  url = {https://doi.org/10.1007/978-0-387-79052-7_3},
  urldate = {2022-03-24},
  isbn = {978-0-387-79052-7},
  langid = {english},
  keywords = {Adaptive Estimator,Linear Estimator,Minimax Risk,Sobolev Class,Unbiased Estimator}
}

@incollection{tsybakovLowerBoundsMinimax2009,
  title = {Lower Bounds on the Minimax Risk},
  booktitle = {Introduction to {{Nonparametric Estimation}}},
  author = {Tsybakov, Alexandre B.},
  editor = {Tsybakov, Alexandre B.},
  date = {2009},
  series = {Springer {{Series}} in {{Statistics}}},
  pages = {77--135},
  publisher = {{Springer}},
  location = {{New York, NY}},
  doi = {10.1007/978-0-387-79052-7_2},
  url = {https://doi.org/10.1007/978-0-387-79052-7_2},
  urldate = {2022-03-24},
  isbn = {978-0-387-79052-7},
  langid = {english},
  keywords = {Hellinger Distance,Lower Bound,Optimal Rate,Probability Measure,Sobolev Class}
}

@incollection{tsybakovNonparametricEstimators2009,
  title = {Nonparametric Estimators},
  booktitle = {Introduction to {{Nonparametric Estimation}}},
  author = {Tsybakov, Alexandre B.},
  editor = {Tsybakov, Alexandre B.},
  date = {2009},
  series = {Springer {{Series}} in {{Statistics}}},
  pages = {1--76},
  publisher = {{Springer}},
  location = {{New York, NY}},
  doi = {10.1007/978-0-387-79052-7_1},
  url = {https://doi.org/10.1007/978-0-387-79052-7_1},
  urldate = {2022-03-24},
  isbn = {978-0-387-79052-7},
  langid = {english},
  keywords = {Kernel Estimator,Nonparametric Estimator,Nonparametric Regression,Sobolev Class,Unbiased Estimator}
}

@misc{velickovicGraphAttentionNetworks2018,
  title = {Graph {{Attention Networks}}},
  author = {Veličković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Liò, Pietro and Bengio, Yoshua},
  date = {2018-02-04},
  number = {arXiv:1710.10903},
  eprint = {1710.10903},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1710.10903},
  url = {http://arxiv.org/abs/1710.10903},
  urldate = {2022-06-28},
  abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/4DREZIWR/Veličković et al. - 2018 - Graph Attention Networks.pdf;/Users/gregoirepacreau/Zotero/storage/8QSQ7VC4/1710.html}
}

@misc{vershyninIntroductionNonasymptoticAnalysis2011,
  title = {Introduction to the Non-Asymptotic Analysis of Random Matrices},
  author = {Vershynin, Roman},
  date = {2011-11-23},
  number = {arXiv:1011.3027},
  eprint = {1011.3027},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1011.3027},
  url = {http://arxiv.org/abs/1011.3027},
  urldate = {2022-07-01},
  abstract = {This is a tutorial on some basic non-asymptotic methods and concepts in random matrix theory. The reader will learn several tools for the analysis of the extreme singular values of random matrices with independent rows or columns. Many of these methods sprung off from the development of geometric functional analysis since the 1970's. They have applications in several fields, most notably in theoretical computer science, statistics and signal processing. A few basic applications are covered in this text, particularly for the problem of estimating covariance matrices in statistics and for validating probabilistic constructions of measurement matrices in compressed sensing. These notes are written particularly for graduate students and beginning researchers in different areas, including functional analysts, probabilists, theoretical statisticians, electrical engineers, and theoretical computer scientists.},
  archiveprefix = {arXiv},
  keywords = {60B20; 46B09,Mathematics - Functional Analysis,Mathematics - Numerical Analysis,Mathematics - Probability},
  file = {/Users/gregoirepacreau/Zotero/storage/SX69RAWG/Vershynin - 2011 - Introduction to the non-asymptotic analysis of ran.pdf;/Users/gregoirepacreau/Zotero/storage/EC7B8QM4/1011.html}
}

@article{virmauxLipschitzRegularityDeep,
  title = {Lipschitz Regularity of Deep Neural Networks: Analysis and Efficient Estimation},
  author = {Virmaux, Aladin and Scaman, Kevin},
  pages = {10},
  abstract = {Deep neural networks are notorious for being sensitive to small well-chosen perturbations, and estimating the regularity of such architectures is of utmost importance for safe and robust practical applications. In this paper, we investigate one of the key characteristics to assess the regularity of such methods: the Lipschitz constant of deep learning architectures. First, we show that, even for two layer neural networks, the exact computation of this quantity is NP-hard and state-ofart methods may significantly overestimate it. Then, we both extend and improve previous estimation methods by providing AutoLip, the first generic algorithm for upper bounding the Lipschitz constant of any automatically differentiable function. We provide a power method algorithm working with automatic differentiation, allowing efficient computations even on large convolutions. Second, for sequential neural networks, we propose an improved algorithm named SeqLip that takes advantage of the linear computation graph to split the computation per pair of consecutive layers. Third we propose heuristics on SeqLip in order to tackle very large networks. Our experiments show that SeqLip can significantly improve on the existing upper bounds. Finally, we provide an implementation of AutoLip in the PyTorch environment that may be used to better estimate the robustness of a given neural network to small perturbations or regularize it using more precise Lipschitz estimations.},
  langid = {english},
  file = {/Users/gregoirepacreau/Zotero/storage/DWPSK7ZM/Virmaux et Scaman - Lipschitz regularity of deep neural networks anal.pdf}
}

@unpublished{wangReviewGraphNeural2022,
  title = {A {{Review}} on {{Graph Neural Network Methods}} in {{Financial Applications}}},
  author = {Wang, Jianian and Zhang, Sheng and Xiao, Yanghua and Song, Rui},
  date = {2022-04-26},
  number = {arXiv:2111.15367},
  eprint = {2111.15367},
  eprinttype = {arxiv},
  primaryclass = {cs, q-fin, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2111.15367},
  urldate = {2022-05-19},
  abstract = {With multiple components and relations, financial data are often presented as graph data, since it could represent both the individual features and the complicated relations. Due to the complexity and volatility of the financial market, the graph constructed on the financial data is often heterogeneous or time-varying, which imposes challenges on modeling technology. Among the graph modeling technologies, graph neural network (GNN) models are able to handle the complex graph structure and achieve great performance and thus could be used to solve financial tasks. In this work, we provide a comprehensive review of GNN models in recent financial context. We first categorize the commonly-used financial graphs and summarize the feature processing step for each node. Then we summarize the GNN methodology for each graph type, application in each area, and propose some potential research areas.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Quantitative Finance - Statistical Finance,Statistics - Applications},
  file = {/Users/gregoirepacreau/Zotero/storage/VL6Q3JZW/Wang et al. - 2022 - A Review on Graph Neural Network Methods in Financ.pdf;/Users/gregoirepacreau/Zotero/storage/6MJNEC2V/2111.html}
}

@misc{wuSupplyChainNetwork2014,
  type = {SSRN Scholarly Paper},
  title = {Supply {{Chain Network Structure}} and {{Firm Returns}}},
  author = {Wu, Jing and Birge, John R.},
  date = {2014-01-24},
  number = {2385217},
  location = {{Rochester, NY}},
  doi = {10.2139/ssrn.2385217},
  url = {https://papers.ssrn.com/abstract=2385217},
  urldate = {2022-06-30},
  abstract = {The complexity and opacity of the network of interconnections among firms and their supply chains inhibits understanding of the impact of management decisions concerning the boundaries of the firm and the number and intensity of its relationships with suppliers and customers. Using recently available data on the relationships of public US firms, this paper investigates the effects of supply chain connections on firm performance as reflected in stock returns. The paper finds that supply chain structure is closely related to firm returns at two levels, a first-order effect from direct connections and a second-order impact from systemic exposures through the network. For the first order effect, using a cross-sectional data set of the supply chain network and monthly returns, we show that a firm’s return can be explained by its concurrent supplier returns, concurrent customer returns, own momentum, and supplier momentum, whereas customer momentum has little impact. A long-short equity strategy based on the supplier momentum yields monthly abnormal returns of 56 basis points. This result implies investors’ limited attention to supplier firms relative to customer firms and gradual diffusion of information downstream as opposed to upstream in the supply chain. For the second-order effect, we find a market anomaly by grouping firms according to their centrality in the supply chain. Specifically, manufacturing firms that are more central in the network earn lower returns, while logistics firms that are more central in the network earn higher returns. This result holds for both eigenvector centrality and in-degree centrality (number of suppliers). We argue that centrality and multiplicity of suppliers have different risk implications for firms operating in different industries. More central firms in manufacturing choose their suppliers to operationally hedge shocks transmitted from other firms and earn lower returns due to lower systematic risk. On the contrary, more central firms in logistics are shock aggregators, earning higher returns due to their exposure to greater systematic risk. Our results are robust after controlling for common asset pricing factors.},
  langid = {english},
  keywords = {Lead-lag Effect,Network Centrality,Supply Chain,Systematic Risk},
  file = {/Users/gregoirepacreau/Zotero/storage/AZZ94HCQ/Wu et Birge - 2014 - Supply Chain Network Structure and Firm Returns.pdf;/Users/gregoirepacreau/Zotero/storage/PHXXNSCR/papers.html}
}

@article{wuWuJingBirge2014,
  title = {Wu, {{Jing}} and {{Birge}}, {{John R}}., {{Supply Chain Network Structure}} and {{Firm Returns}} ({{January}} 24, 2014). {{Available}} at {{SSRN}}: {{https://ssrn.com/abstract=2385217}} or {{http://dx.doi.org/10.2139/ssrn.2385217}}},
  author = {Wu, Jing and Birge, John R.},
  date = {2014-01-24},
  journaltitle = {SSRN},
  doi = {http://dx.doi.org/10.2139/ssrn.2385217}
}

@unpublished{xuAdversarialAttacksDefenses2019,
  title = {Adversarial {{Attacks}} and {{Defenses}} in {{Images}}, {{Graphs}} and {{Text}}: {{A Review}}},
  shorttitle = {Adversarial {{Attacks}} and {{Defenses}} in {{Images}}, {{Graphs}} and {{Text}}},
  author = {Xu, Han and Ma, Yao and Liu, Haochen and Deb, Debayan and Liu, Hui and Tang, Jiliang and Jain, Anil K.},
  date = {2019-10-09},
  eprint = {1909.08072},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1909.08072},
  urldate = {2022-03-10},
  abstract = {Deep neural networks (DNN) have achieved unprecedented success in numerous machine learning tasks in various domains. However, the existence of adversarial examples has raised concerns about applying deep learning to safety-critical applications. As a result, we have witnessed increasing interests in studying attack and defense mechanisms for DNN models on different data types, such as images, graphs and text. Thus, it is necessary to provide a systematic and comprehensive overview of the main threats of attacks and the success of corresponding countermeasures. In this survey, we review the state of the art algorithms for generating adversarial examples and the countermeasures against adversarial examples, for the three popular data types, i.e., images, graphs and text.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/IDQRMIAB/Xu et al. - 2019 - Adversarial Attacks and Defenses in Images, Graphs.pdf;/Users/gregoirepacreau/Zotero/storage/6CTFPN8B/1909.html}
}

@misc{yangHistoryPCANew2018,
  title = {History {{PCA}}: {{A New Algorithm}} for {{Streaming PCA}}},
  shorttitle = {History {{PCA}}},
  author = {Yang, Puyudi and Hsieh, Cho-Jui and Wang, Jane-Ling},
  date = {2018-02-15},
  number = {arXiv:1802.05447},
  eprint = {1802.05447},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1802.05447},
  url = {http://arxiv.org/abs/1802.05447},
  urldate = {2022-06-21},
  abstract = {In this paper we propose a new algorithm for streaming principal component analysis. With limited memory, small devices cannot store all the samples in the high-dimensional regime. Streaming principal component analysis aims to find the \$k\$-dimensional subspace which can explain the most variation of the \$d\$-dimensional data points that come into memory sequentially. In order to deal with large \$d\$ and large \$N\$ (number of samples), most streaming PCA algorithms update the current model using only the incoming sample and then dump the information right away to save memory. However the information contained in previously streamed data could be useful. Motivated by this idea, we develop a new streaming PCA algorithm called History PCA that achieves this goal. By using \$O(Bd)\$ memory with \$B\textbackslash approx 10\$ being the block size, our algorithm converges much faster than existing streaming PCA algorithms. By changing the number of inner iterations, the memory usage can be further reduced to \$O(d)\$ while maintaining a comparable convergence speed. We provide theoretical guarantees for the convergence of our algorithm along with the rate of convergence. We also demonstrate on synthetic and real world data sets that our algorithm compares favorably with other state-of-the-art streaming PCA methods in terms of the convergence speed and performance.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/DHFTBD39/Yang et al. - 2018 - History PCA A New Algorithm for Streaming PCA.pdf;/Users/gregoirepacreau/Zotero/storage/EZDD3R7L/1802.html}
}

@article{zotero-47,
  entrysubtype = {newspaper}
}


