@misc{180601261Relational,
  title = {[1806.01261] {{Relational}} Inductive Biases, Deep Learning, and Graph Networks},
  howpublished = {https://arxiv.org/abs/1806.01261},
  file = {/Users/gregoirepacreau/Zotero/storage/N5NCR2JV/1806.html}
}

@inproceedings{abrahamHybridIntelligentSystems2001,
  title = {Hybrid {{Intelligent Systems}} for {{Stock Market Analysis}}},
  booktitle = {Computational {{Science}} - {{ICCS}} 2001},
  author = {Abraham, Ajith and Nath, Baikunth and Mahanti, P. K.},
  editor = {Alexandrov, Vassil N. and Dongarra, Jack J. and Juliano, Benjoe A. and Renner, Ren{\'e} S. and Tan, C. J. Kenneth},
  year = {2001},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {337--345},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-45718-6_38},
  abstract = {The use of intelligent systems for stock market predictions has been widely established. This paper deals with the application of hybridized soft computing techniques for automated stock market forecasting and trend analysis. We make use of a neural network for one day ahead stock forecasting and a neuro-fuzzy system for analyzing the trend of the predicted stock values. To demonstrate the proposed technique, we considered the popular Nasdaq-100 index of Nasdaq Stock MarketSM. We analyzed the 24 months stock data for Nasdaq-100 main index as well as six of the companies listed in the Nasdaq-100 index. Input data were preprocessed using principal component analysis and fed to an artificial neural network for stock forecasting. The predicted stock values are further fed to a neuro-fuzzy system to analyze the trend of the market. The forecasting and trend prediction results using the proposed hybrid system are promising and certainly warrant further research and analysis.},
  isbn = {978-3-540-45718-3},
  langid = {english},
  file = {/Users/gregoirepacreau/Zotero/storage/DPG5D8VF/Abraham et al. - 2001 - Hybrid Intelligent Systems for Stock Market Analys.pdf}
}

@misc{agostinelliRobustEstimationMultivariate2014,
  title = {Robust Estimation of Multivariate Location and Scatter in the Presence of Cellwise and Casewise Contamination},
  author = {Agostinelli, Claudio and Leung, Andy and Yohai, Victor J. and Zamar, Ruben H.},
  year = {2014},
  month = jun,
  number = {arXiv:1406.6031},
  eprint = {1406.6031},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1406.6031},
  abstract = {Multivariate location and scatter matrix estimation is a cornerstone in multivariate data analysis. We consider this problem when the data may contain independent cellwise and casewise outliers. Flat data sets with a large number of variables and a relatively small number of cases are common place in modern statistical applications. In these cases global down-weighting of an entire case, as performed by traditional robust procedures, may lead to poor results. We highlight the need for a new generation of robust estimators that can efficiently deal with cellwise outliers and at the same time show good performance under casewise outliers.},
  archiveprefix = {arXiv},
  keywords = {62G35 (Primary); 62G05 (Secondary),Mathematics - Statistics Theory},
  file = {/Users/gregoirepacreau/Zotero/storage/JEL5ZJJ3/Agostinelli et al. - 2014 - Robust estimation of multivariate location and sca.pdf;/Users/gregoirepacreau/Zotero/storage/SBGKXX3A/1406.html}
}

@article{akhtarThreatAdversarialAttacks2018,
  title = {Threat of {{Adversarial Attacks}} on {{Deep Learning}} in {{Computer Vision}}: {{A Survey}}},
  shorttitle = {Threat of {{Adversarial Attacks}} on {{Deep Learning}} in {{Computer Vision}}},
  author = {Akhtar, Naveed and Mian, Ajmal},
  year = {2018},
  month = feb,
  journal = {arXiv:1801.00553 [cs]},
  eprint = {1801.00553},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deep learning is at the heart of the current rise of artificial intelligence. In the field of Computer Vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has recently lead to a large influx of contributions in this direction. This article presents the first comprehensive survey on adversarial attacks on deep learning in Computer Vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, drawing on the reviewed literature, we provide a broader outlook of this research direction.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/gregoirepacreau/Zotero/storage/ZZMHICSJ/Akhtar et Mian - 2018 - Threat of Adversarial Attacks on Deep Learning in .pdf}
}

@article{alqallafPropagationOutliersMultivariate2009,
  title = {Propagation of Outliers in Multivariate Data},
  author = {Alqallaf, Fatemah and Aelst, Stefan Van and Yohai, Victor J. and Zamar, Ruben H.},
  year = {2009},
  month = feb,
  journal = {The Annals of Statistics},
  volume = {37},
  number = {1},
  pages = {311--331},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/07-AOS588},
  abstract = {We investigate the performance of robust estimates of multivariate location under nonstandard data contamination models such as componentwise outliers (i.e., contamination in each variable is independent from the other variables). This model brings up a possible new source of statistical error that we call ``propagation of outliers.'' This source of error is unusual in the sense that it is generated by the data processing itself and takes place after the data has been collected. We define and derive the influence function of robust multivariate location estimates under flexible contamination models and use it to investigate the effect of propagation of outliers. Furthermore, we show that standard high-breakdown affine equivariant estimators propagate outliers and therefore show poor breakdown behavior under componentwise contamination when the dimension d is high.},
  keywords = {62F35,62H12,Breakdown point,contamination model,independent contamination,influence function,robustness},
  file = {/Users/gregoirepacreau/Zotero/storage/F4J79BKY/Alqallaf et al. - 2009 - Propagation of outliers in multivariate data.pdf;/Users/gregoirepacreau/Zotero/storage/ZNMW29AE/07-AOS588.html}
}

@misc{arroyoDynamicPortfolioCuts2021,
  title = {Dynamic {{Portfolio Cuts}}: {{A Spectral Approach}} to {{Graph-Theoretic Diversification}}},
  shorttitle = {Dynamic {{Portfolio Cuts}}},
  author = {Arroyo, Alvaro and Scalzo, Bruno and Stankovic, Ljubisa and Mandic, Danilo P.},
  year = {2021},
  month = jun,
  number = {arXiv:2106.03417},
  eprint = {2106.03417},
  eprinttype = {arxiv},
  primaryclass = {eess, q-fin},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.03417},
  abstract = {Stock market returns are typically analyzed using standard regression, yet they reside on irregular domains which is a natural scenario for graph signal processing. To this end, we consider a market graph as an intuitive way to represent the relationships between financial assets. Traditional methods for estimating asset-return covariance operate under the assumption of statistical time-invariance, and are thus unable to appropriately infer the underlying true structure of the market graph. This work introduces a class of graph spectral estimators which cater for the nonstationarity inherent to asset price movements, and serve as a basis to represent the time-varying interactions between assets through a dynamic spectral market graph. Such an account of the time-varying nature of the asset-return covariance allows us to introduce the notion of dynamic spectral portfolio cuts, whereby the graph is partitioned into time-evolving clusters, allowing for online and robust asset allocation. The advantages of the proposed framework over traditional methods are demonstrated through numerical case studies using real-world price data.},
  archiveprefix = {arXiv},
  keywords = {Electrical Engineering and Systems Science - Signal Processing,Quantitative Finance - Portfolio Management},
  file = {/Users/gregoirepacreau/Zotero/storage/HKQIWAAQ/Arroyo et al. - 2021 - Dynamic Portfolio Cuts A Spectral Approach to Gra.pdf;/Users/gregoirepacreau/Zotero/storage/5UL4E65B/2106.html}
}

@article{bartlettAdversarialExamplesMultiLayer2021,
  title = {Adversarial {{Examples}} in {{Multi-Layer Random ReLU Networks}}},
  author = {Bartlett, Peter L. and Bubeck, S{\'e}bastien and Cherapanamjeri, Yeshwanth},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.12611 [cs, stat]},
  eprint = {2106.12611},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We consider the phenomenon of adversarial examples in ReLU networks with independent gaussian parameters. For networks of constant depth and with a large range of widths (for instance, it suffices if the width of each layer is polynomial in that of any other layer), small perturbations of input vectors lead to large changes of outputs. This generalizes results of Daniely and Schacham (2020) for networks of rapidly decreasing width and of Bubeck et al (2021) for two-layer networks. The proof shows that adversarial examples arise in these networks because the functions that they compute are very close to linear. Bottleneck layers in the network play a key role: the minimal width up to some point in the network determines scales and sensitivities of mappings computed up to that point. The main result is for networks with constant depth, but we also show that some constraint on depth is necessary for a result of this kind, because there are suitably deep networks that, with constant probability, compute a function that is close to constant.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/QXUCLJNM/Bartlett et al. - 2021 - Adversarial Examples in Multi-Layer Random ReLU Ne.pdf}
}

@misc{battagliaInteractionNetworksLearning2016,
  title = {Interaction {{Networks}} for {{Learning}} about {{Objects}}, {{Relations}} and {{Physics}}},
  author = {Battaglia, Peter W. and Pascanu, Razvan and Lai, Matthew and Rezende, Danilo and Kavukcuoglu, Koray},
  year = {2016},
  month = dec,
  number = {arXiv:1612.00222},
  eprint = {1612.00222},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1612.00222},
  abstract = {Reasoning about objects, relations, and physics is central to human intelligence, and a key goal of artificial intelligence. Here we introduce the interaction network, a model which can reason about how objects in complex systems interact, supporting dynamical predictions, as well as inferences about the abstract properties of the system. Our model takes graphs as input, performs object- and relation-centric reasoning in a way that is analogous to a simulation, and is implemented using deep neural networks. We evaluate its ability to reason about several challenging physical domains: n-body problems, rigid-body collision, and non-rigid dynamics. Our results show it can be trained to accurately simulate the physical trajectories of dozens of objects over thousands of time steps, estimate abstract quantities such as energy, and generalize automatically to systems with different numbers and configurations of objects and relations. Our interaction network implementation is the first general-purpose, learnable physics engine, and a powerful general framework for reasoning about object and relations in a wide variety of complex real-world domains.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/2PAZZIHZ/Battaglia et al. - 2016 - Interaction Networks for Learning about Objects, R.pdf;/Users/gregoirepacreau/Zotero/storage/PD3ADZDD/1612.html}
}

@article{battagliaSimulationEnginePhysical2013,
  title = {Simulation as an Engine of Physical Scene Understanding},
  author = {Battaglia, Peter W. and Hamrick, Jessica B. and Tenenbaum, Joshua B.},
  year = {2013},
  month = nov,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {110},
  number = {45},
  pages = {18327--18332},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1306572110},
  file = {/Users/gregoirepacreau/Zotero/storage/RTKT67VM/Battaglia et al. - 2013 - Simulation as an engine of physical scene understa.pdf}
}

@article{bianchiGraphNeuralNetworks2021,
  title = {Graph {{Neural Networks}} with Convolutional {{ARMA}} Filters},
  author = {Bianchi, Filippo Maria and Grattarola, Daniele and Livi, Lorenzo and Alippi, Cesare},
  year = {2021},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  eprint = {1901.01343},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {1--1},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2021.3054830},
  abstract = {Popular graph neural networks implement convolution operations on graphs based on polynomial spectral filters. In this paper, we propose a novel graph convolutional layer inspired by the auto-regressive moving average (ARMA) filter that, compared to polynomial ones, provides a more flexible frequency response, is more robust to noise, and better captures the global graph structure. We propose a graph neural network implementation of the ARMA filter with a recursive and distributed formulation, obtaining a convolutional layer that is efficient to train, localized in the node space, and can be transferred to new graphs at test time. We perform a spectral analysis to study the filtering effect of the proposed ARMA layer and report experiments on four downstream tasks: semi-supervised node classification, graph signal classification, graph classification, and graph regression. Results show that the proposed ARMA layer brings significant improvements over graph neural networks based on polynomial filters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/QUY3FLIL/Bianchi et al. - 2021 - Graph Neural Networks with convolutional ARMA filt.pdf;/Users/gregoirepacreau/Zotero/storage/GMPSKQKZ/1901.html}
}

@misc{bianchiSpectralClusteringGraph2020,
  title = {Spectral {{Clustering}} with {{Graph Neural Networks}} for {{Graph Pooling}}},
  author = {Bianchi, Filippo Maria and Grattarola, Daniele and Alippi, Cesare},
  year = {2020},
  month = dec,
  number = {arXiv:1907.00481},
  eprint = {1907.00481},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1907.00481},
  abstract = {Spectral clustering (SC) is a popular clustering technique to find strongly connected communities on a graph. SC can be used in Graph Neural Networks (GNNs) to implement pooling operations that aggregate nodes belonging to the same cluster. However, the eigendecomposition of the Laplacian is expensive and, since clustering results are graph-specific, pooling methods based on SC must perform a new optimization for each new sample. In this paper, we propose a graph clustering approach that addresses these limitations of SC. We formulate a continuous relaxation of the normalized minCUT problem and train a GNN to compute cluster assignments that minimize this objective. Our GNN-based implementation is differentiable, does not require to compute the spectral decomposition, and learns a clustering function that can be quickly evaluated on out-of-sample graphs. From the proposed clustering method, we design a graph pooling operator that overcomes some important limitations of state-of-the-art graph pooling techniques and achieves the best performance in several supervised and unsupervised tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/AHTPPB9Z/Bianchi et al. - 2020 - Spectral Clustering with Graph Neural Networks for.pdf;/Users/gregoirepacreau/Zotero/storage/4XDELDSV/1907.html}
}

@misc{bronsteinGeometricDeepLearning2021,
  title = {Geometric {{Deep Learning}}: {{Grids}}, {{Groups}}, {{Graphs}}, {{Geodesics}}, and {{Gauges}}},
  shorttitle = {Geometric {{Deep Learning}}},
  author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veli{\v c}kovi{\'c}, Petar},
  year = {2021},
  month = may,
  number = {arXiv:2104.13478},
  eprint = {2104.13478},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.13478},
  abstract = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Geometry,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/K7ZAUCKA/Bronstein et al. - 2021 - Geometric Deep Learning Grids, Groups, Graphs, Ge.pdf;/Users/gregoirepacreau/Zotero/storage/L6IZAQLB/2104.html}
}

@misc{calkinStochasticFlowDiagrams2014,
  type = {{{SSRN Scholarly Paper}}},
  title = {Stochastic {{Flow Diagrams}}},
  author = {Calkin, Neil and {Lopez de Prado}, Marcos},
  year = {2014},
  month = feb,
  number = {2379314},
  address = {{Rochester, NY}},
  doi = {10.2139/ssrn.2379314},
  abstract = {Inspired by visualization techniques \`a la Feynman, we introduce Stochastic Flow Diagrams (SFDs), a new mathematical approach to represent complex dynamic systems into a single weighted digraph. This topological representation provides a way to visualize what otherwise would be a morass of equations in differences. SFDs model the propagation and reverberation that follows a shock. For example, reverberation explains how a shock to a financial system can initiate a sequence of events that lead to a crash long after the occurrence of the shock. SFDs can simulate systems in stable, steady or explosive state. SFDs add Topology to the Statistical and Econometric toolkit. We believe that SFDs will help policy makers, investors and researchers communicate and discuss better the complexity of dynamic systems.},
  langid = {english},
  keywords = {Financial Flows,Graph Theory,Macro Trading,Time Series,Topology},
  file = {/Users/gregoirepacreau/Zotero/storage/INAV5IQX/Calkin et Lopez de Prado - 2014 - Stochastic Flow Diagrams.pdf;/Users/gregoirepacreau/Zotero/storage/TUZ4WA6G/papers.html}
}

@misc{changCompositionalObjectBasedApproach2017,
  title = {A {{Compositional Object-Based Approach}} to {{Learning Physical Dynamics}}},
  author = {Chang, Michael B. and Ullman, Tomer and Torralba, Antonio and Tenenbaum, Joshua B.},
  year = {2017},
  month = mar,
  number = {arXiv:1612.00341},
  eprint = {1612.00341},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1612.00341},
  abstract = {We present the Neural Physics Engine (NPE), a framework for learning simulators of intuitive physics that naturally generalize across variable object count and different scene configurations. We propose a factorization of a physical scene into composable object-based representations and a neural network architecture whose compositional structure factorizes object dynamics into pairwise interactions. Like a symbolic physics engine, the NPE is endowed with generic notions of objects and their interactions; realized as a neural network, it can be trained via stochastic gradient descent to adapt to specific object properties and dynamics of different worlds. We evaluate the efficacy of our approach on simple rigid body dynamics in two-dimensional worlds. By comparing to less structured architectures, we show that the NPE's compositional representation of the structure in physical interactions improves its ability to predict movement, generalize across variable object count and different scene configurations, and infer latent properties of objects such as mass.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/42A7GX79/Chang et al. - 2017 - A Compositional Object-Based Approach to Learning .pdf;/Users/gregoirepacreau/Zotero/storage/6QR58QZM/1612.html}
}

@misc{chenGeneralDecisionTheory2017,
  title = {A {{General Decision Theory}} for {{Huber}}'s \$\textbackslash epsilon\$-{{Contamination Model}}},
  author = {Chen, Mengjie and Gao, Chao and Ren, Zhao},
  year = {2017},
  month = jan,
  number = {arXiv:1511.04144},
  eprint = {1511.04144},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1511.04144},
  abstract = {Today's data pose unprecedented challenges to statisticians. It may be incomplete, corrupted or exposed to some unknown source of contamination. We need new methods and theories to grapple with these challenges. Robust estimation is one of the revived fields with potential to accommodate such complexity and glean useful information from modern datasets. Following our recent work on high dimensional robust covariance matrix estimation, we establish a general decision theory for robust statistics under Huber's \$\textbackslash epsilon\$-contamination model. We propose a solution using Scheff\{\textbackslash 'e\} estimate to a robust two-point testing problem that leads to the construction of robust estimators adaptive to the proportion of contamination. Applying the general theory, we construct robust estimators for nonparametric density estimation, sparse linear regression and low-rank trace regression. We show that these new estimators achieve the minimax rate with optimal dependence on the contamination proportion. This testing procedure, Scheff\{\textbackslash 'e\} estimate, also enjoys an optimal rate in the exponent of the testing error, which may be of independent interest.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory},
  file = {/Users/gregoirepacreau/Zotero/storage/36QRQHA3/Chen et al. - 2017 - A General Decision Theory for Huber's $epsilon$-C.pdf;/Users/gregoirepacreau/Zotero/storage/2NFIP49I/1511.html}
}

@article{chenRobustCovarianceScatter2017,
  title = {Robust {{Covariance}} and {{Scatter Matrix Estimation}} under {{Huber}}'s {{Contamination Model}}},
  author = {Chen, Mengjie and Gao, Chao and Ren, Zhao},
  year = {2017},
  month = jun,
  journal = {arXiv:1506.00691 [math, stat]},
  eprint = {1506.00691},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {Covariance matrix estimation is one of the most important problems in statistics. To accommodate the complexity of modern datasets, it is desired to have estimation procedures that not only can incorporate the structural assumptions of covariance matrices, but are also robust to outliers from arbitrary sources. In this paper, we define a new concept called matrix depth and then propose a robust covariance matrix estimator by maximizing the empirical depth function. The proposed estimator is shown to achieve minimax optimal rate under Huber's \$\textbackslash epsilon\$-contamination model for estimating covariance/scatter matrices with various structures including bandedness and sparsity.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/Users/gregoirepacreau/Zotero/storage/6SK4W7JS/Chen et al. - 2017 - Robust Covariance and Scatter Matrix Estimation un.pdf;/Users/gregoirepacreau/Zotero/storage/D52XC582/1506.html}
}

@misc{cohenEconomicLinksPredictable2007,
  type = {{{SSRN Scholarly Paper}}},
  title = {Economic {{Links}} and {{Predictable Returns}}},
  author = {Cohen, Lauren and Frazzini, Andrea},
  year = {2007},
  month = may,
  number = {2758776},
  address = {{Rochester, NY}},
  doi = {10.2139/ssrn.2758776},
  abstract = {This paper finds evidence of return predictability across economically linked firms. We test the hypothesis that in the presence of investors subject to attention constraints, stock prices do not promptly incorporate news about economically related firms, generating return predictability across assets. We use a dataset of firms' principal customers to identify a set of economically related firms, and show that stock prices do not incorporate news involving related firms, generating predictable subsequent price moves. A long/short equity strategy based on this effect yields monthly alphas of over 150 basis points.},
  langid = {english},
  keywords = {G10,G11,G14},
  file = {/Users/gregoirepacreau/Zotero/storage/9GEARBEM/Cohen et Frazzini - 2007 - Economic Links and Predictable Returns.pdf}
}

@article{dalalyanAllinoneRobustEstimator2022,
  title = {All-in-One Robust Estimator of the {{Gaussian}} Mean},
  author = {Dalalyan, Arnak S. and Minasyan, Arshak},
  year = {2022},
  journal = {The Annals of Statistics},
  volume = {50},
  number = {2},
  pages = {1193--1219},
  publisher = {{Institute of Mathematical Statistics}}
}

@inproceedings{deesPortfolioCutsGraphTheoretic2020,
  title = {Portfolio {{Cuts}}: {{A Graph-Theoretic Framework}} to {{Diversification}}},
  shorttitle = {Portfolio {{Cuts}}},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Dees, Bruno Scalzo and Stankovi{\'c}, Ljubi{\v s}a and Constantinides, Anthony G. and Mandic, Danilo P.},
  year = {2020},
  month = may,
  pages = {8454--8458},
  issn = {2379-190X},
  doi = {10.1109/ICASSP40776.2020.9054371},
  abstract = {Investment returns naturally reside on irregular domains, however, standard multivariate portfolio optimization methods are agnostic to data structure. To this end, we investigate ways for domain knowledge to be conveniently incorporated into the analysis, by means of graphs. Next, to relax the assumption of the completeness of graph topology and to equip the graph model with practically relevant physical intuition, we introduce the portfolio cut paradigm. Such a graph-theoretic portfolio partitioning technique is shown to allow the investor to devise robust and tractable asset allocation schemes, by virtue of a rigorous graph framework for considering smaller, computationally feasible, and economically meaningful clusters of assets, based on graph cuts. In turn, this makes it possible to fully utilize the asset returns covariance matrix for constructing the portfolio, even without the requirement for its inversion. The advantages of the proposed framework over traditional methods are demonstrated through numerical simulations based on real-world price data.},
  keywords = {Covariance matrices,Financial signal processing,graph cut,graph signal processing,portfolio optimization,Portfolios,Resource management,Simulation,Speech processing,Standards,Topology,vertex clustering},
  file = {/Users/gregoirepacreau/Zotero/storage/UP79WWCR/Dees et al. - 2020 - Portfolio Cuts A Graph-Theoretic Framework to Div.pdf;/Users/gregoirepacreau/Zotero/storage/HESTG73X/9054371.html}
}

@misc{defferrardConvolutionalNeuralNetworks2017,
  title = {Convolutional {{Neural Networks}} on {{Graphs}} with {{Fast Localized Spectral Filtering}}},
  author = {Defferrard, Micha{\"e}l and Bresson, Xavier and Vandergheynst, Pierre},
  year = {2017},
  month = feb,
  number = {arXiv:1606.09375},
  eprint = {1606.09375},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1606.09375},
  abstract = {In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/JWH8YSZG/Defferrard et al. - 2017 - Convolutional Neural Networks on Graphs with Fast .pdf;/Users/gregoirepacreau/Zotero/storage/AABTTANR/1606.html}
}

@misc{deledalleSteinUnbiasedGrAdient2014,
  title = {Stein {{Unbiased GrAdient}} Estimator of the {{Risk}} ({{SUGAR}}) for Multiple Parameter Selection},
  author = {Deledalle, Charles-Alban and Vaiter, Samuel and Fadili, Jalal M. and Peyr{\'e}, Gabriel},
  year = {2014},
  month = aug,
  number = {arXiv:1405.1164},
  eprint = {1405.1164},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1405.1164},
  abstract = {Algorithms to solve variational regularization of ill-posed inverse problems usually involve operators that depend on a collection of continuous parameters. When these operators enjoy some (local) regularity, these parameters can be selected using the so-called Stein Unbiased Risk Estimate (SURE). While this selection is usually performed by exhaustive search, we address in this work the problem of using the SURE to efficiently optimize for a collection of continuous parameters of the model. When considering non-smooth regularizers, such as the popular l1-norm corresponding to soft-thresholding mapping, the SURE is a discontinuous function of the parameters preventing the use of gradient descent optimization techniques. Instead, we focus on an approximation of the SURE based on finite differences as proposed in (Ramani et al., 2008). Under mild assumptions on the estimation mapping, we show that this approximation is a weakly differentiable function of the parameters and its weak gradient, coined the Stein Unbiased GrAdient estimator of the Risk (SUGAR), provides an asymptotically (with respect to the data dimension) unbiased estimate of the gradient of the risk. Moreover, in the particular case of soft-thresholding, it is proved to be also a consistent estimator. This gradient estimate can then be used as a basis to perform a quasi-Newton optimization. The computation of the SUGAR relies on the closed-form (weak) differentiation of the non-smooth function. We provide its expression for a large class of iterative methods including proximal splitting ones and apply our strategy to regularizations involving non-smooth convex structured penalties. Illustrations on various image restoration and matrix completion problems are given.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Applications},
  file = {/Users/gregoirepacreau/Zotero/storage/G4I2YZXK/Deledalle et al. - 2014 - Stein Unbiased GrAdient estimator of the Risk (SUG.pdf;/Users/gregoirepacreau/Zotero/storage/DQV259TY/1405.html}
}

@misc{derrSignedGraphConvolutional2018,
  title = {Signed {{Graph Convolutional Network}}},
  author = {Derr, Tyler and Ma, Yao and Tang, Jiliang},
  year = {2018},
  month = aug,
  number = {arXiv:1808.06354},
  eprint = {1808.06354},
  eprinttype = {arxiv},
  primaryclass = {physics},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1808.06354},
  abstract = {Due to the fact much of today's data can be represented as graphs, there has been a demand for generalizing neural network models for graph data. One recent direction that has shown fruitful results, and therefore growing interest, is the usage of graph convolutional neural networks (GCNs). They have been shown to provide a significant improvement on a wide range of tasks in network analysis, one of which being node representation learning. The task of learning low-dimensional node representations has shown to increase performance on a plethora of other tasks from link prediction and node classification, to community detection and visualization. Simultaneously, signed networks (or graphs having both positive and negative links) have become ubiquitous with the growing popularity of social media. However, since previous GCN models have primarily focused on unsigned networks (or graphs consisting of only positive links), it is unclear how they could be applied to signed networks due to the challenges presented by negative links. The primary challenges are based on negative links having not only a different semantic meaning as compared to positive links, but their principles are inherently different and they form complex relations with positive links. Therefore we propose a dedicated and principled effort that utilizes balance theory to correctly aggregate and propagate the information across layers of a signed GCN model. We perform empirical experiments comparing our proposed signed GCN against state-of-the-art baselines for learning node representations in signed networks. More specifically, our experiments are performed on four real-world datasets for the classical link sign prediction problem that is commonly used as the benchmark for signed network embeddings algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Social and Information Networks,Physics - Physics and Society},
  file = {/Users/gregoirepacreau/Zotero/storage/928YM93P/Derr et al. - 2018 - Signed Graph Convolutional Network.pdf;/Users/gregoirepacreau/Zotero/storage/NRFZBHWX/1808.html}
}

@misc{duvenaudConvolutionalNetworksGraphs2015,
  title = {Convolutional {{Networks}} on {{Graphs}} for {{Learning Molecular Fingerprints}}},
  author = {Duvenaud, David and Maclaurin, Dougal and {Aguilera-Iparraguirre}, Jorge and {G{\'o}mez-Bombarelli}, Rafael and Hirzel, Timothy and {Aspuru-Guzik}, Al{\'a}n and Adams, Ryan P.},
  year = {2015},
  month = nov,
  number = {arXiv:1509.09292},
  eprint = {1509.09292},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1509.09292},
  abstract = {We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/2FTDFX3Q/Duvenaud et al. - 2015 - Convolutional Networks on Graphs for Learning Mole.pdf;/Users/gregoirepacreau/Zotero/storage/PE5A4UBR/1509.html}
}

@article{farcomeniRobustConstrainedClustering2014,
  title = {Robust {{Constrained Clustering}} in {{Presence}} of {{Entry-Wise Outliers}}},
  author = {Farcomeni, Alessio},
  year = {2014},
  month = feb,
  journal = {Technometrics},
  volume = {56},
  doi = {10.1080/00401706.2013.826148},
  abstract = {We propose a robust heteroscedastic model-based clustering method based on snipping. An observation is snipped when some of its dimensions are discarded, but the remaining are used for estimation. An expectation-maximization algorithm augmented with a stochastic optimization step is used to derive inference, and its convergence properties are studied. We show global robustness of our resulting sclust procedure also when outliers arise entry-wise. The method is robust to contamination, even when most or even all of the observations contain outliers. Simulations and two real data applications illustrate and compare the approach with existing methods.},
  file = {/Users/gregoirepacreau/Zotero/storage/4P3WYYEG/Farcomeni - 2014 - Robust Constrained Clustering in Presence of Entry.pdf}
}

@article{fazlyabEfficientAccurateEstimation,
  title = {Efficient and {{Accurate Estimation}} of {{Lipschitz Constants}} for {{Deep Neural Networks}}},
  author = {Fazlyab, Mahyar and Robey, Alexander and Hassani, Hamed and Morari, Manfred and Pappas, George J},
  pages = {12},
  abstract = {Tight estimation of the Lipschitz constant for deep neural networks (DNNs) is useful in many applications ranging from robustness certification of classifiers to stability analysis of closed-loop systems with reinforcement learning controllers. Existing methods in the literature for estimating the Lipschitz constant suffer from either lack of accuracy or poor scalability. In this paper, we present a convex optimization framework to compute guaranteed upper bounds on the Lipschitz constant of DNNs both accurately and efficiently. Our main idea is to interpret activation functions as gradients of convex potential functions. Hence, they satisfy certain properties that can be described by quadratic constraints. This particular description allows us to pose the Lipschitz constant estimation problem as a semidefinite program (SDP). The resulting SDP can be adapted to increase either the estimation accuracy (by capturing the interaction between activation functions of different layers) or scalability (by decomposition and parallel implementation). We illustrate the utility of our approach with a variety of experiments on randomly generated networks and on classifiers trained on the MNIST and Iris datasets. In particular, we experimentally demonstrate that our Lipschitz bounds are the most accurate compared to those in the literature. We also study the impact of adversarial training methods on the Lipschitz bounds of the resulting classifiers and show that our bounds can be used to efficiently provide robustness guarantees.},
  langid = {english},
  file = {/Users/gregoirepacreau/Zotero/storage/NEHU3NAI/Fazlyab et al. - EfÔ¨Åcient and Accurate Estimation of Lipschitz Cons.pdf}
}

@book{GenericChaining2005,
  title = {The {{Generic Chaining}}},
  year = {2005},
  series = {Springer {{Monographs}} in {{Mathematics}}},
  publisher = {{Springer-Verlag}},
  address = {{Berlin/Heidelberg}},
  doi = {10.1007/3-540-27499-5},
  isbn = {978-3-540-24518-6},
  langid = {english},
  keywords = {Gaussian process,Gaussian processes,infinitely divisible processes,matchings,stochastic process,Stochastic processes},
  file = {/Users/gregoirepacreau/Zotero/storage/UVJYQ3LK/2005 - The Generic Chaining.pdf}
}

@article{goodfellowExplainingHarnessingAdversarial2015,
  title = {Explaining and {{Harnessing Adversarial Examples}}},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  year = {2015},
  month = mar,
  journal = {arXiv:1412.6572 [cs, stat]},
  eprint = {1412.6572},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/MLVNJ2IJ/Goodfellow et al. - 2015 - Explaining and Harnessing Adversarial Examples.pdf;/Users/gregoirepacreau/Zotero/storage/D9FHPI3F/1412.html}
}

@misc{hamiltonInductiveRepresentationLearning2018,
  title = {Inductive {{Representation Learning}} on {{Large Graphs}}},
  author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
  year = {2018},
  month = sep,
  number = {arXiv:1706.02216},
  eprint = {1706.02216},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.02216},
  abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/I73B7576/Hamilton et al. - 2018 - Inductive Representation Learning on Large Graphs.pdf;/Users/gregoirepacreau/Zotero/storage/4N5XRFAP/1706.html}
}

@misc{hamiltonInductiveRepresentationLearning2018a,
  title = {Inductive {{Representation Learning}} on {{Large Graphs}}},
  author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
  year = {2018},
  month = sep,
  number = {arXiv:1706.02216},
  eprint = {1706.02216},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1706.02216},
  abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/58GCEFNG/Hamilton et al. - 2018 - Inductive Representation Learning on Large Graphs.pdf;/Users/gregoirepacreau/Zotero/storage/6WJWYUKZ/1706.html}
}

@misc{hippert-ferrerRobustLowrankCovariance2021,
  title = {Robust Low-Rank Covariance Matrix Estimation with a General Pattern of Missing Values},
  author = {{Hippert-Ferrer}, Alexandre and Korso, Mohammed Nabil El and Breloy, Arnaud and Ginolhac, Guillaume},
  year = {2021},
  month = nov,
  number = {arXiv:2107.10505},
  eprint = {2107.10505},
  eprinttype = {arxiv},
  primaryclass = {eess, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.10505},
  abstract = {This paper tackles the problem of robust covariance matrix estimation when the data is incomplete. Classical statistical estimation methodologies are usually built upon the Gaussian assumption, whereas existing robust estimation ones assume unstructured signal models. The former can be inaccurate in real-world data sets in which heterogeneity causes heavy-tail distributions, while the latter does not profit from the usual low-rank structure of the signal. Taking advantage of both worlds, a covariance matrix estimation procedure is designed on a robust (mixture of scaled Gaussian) low-rank model by leveraging the observed-data likelihood function within an expectation-maximization algorithm. It is also designed to handle general pattern of missing values. The proposed procedure is first validated on simulated data sets. Then, its interest for classification and clustering applications is assessed on two real data sets with missing values, which include multispectral and hyperspectral time series.},
  archiveprefix = {arXiv},
  keywords = {Electrical Engineering and Systems Science - Signal Processing,Statistics - Methodology},
  file = {/Users/gregoirepacreau/Zotero/storage/Q7S9PLJZ/Hippert-Ferrer et al. - 2021 - Robust low-rank covariance matrix estimation with .pdf;/Users/gregoirepacreau/Zotero/storage/66AVIXNM/2107.html}
}

@article{huberRobustEstimationLocation1964,
  title = {Robust {{Estimation}} of a {{Location Parameter}}},
  author = {Huber, Peter J.},
  year = {1964},
  journal = {The Annals of Mathematical Statistics},
  volume = {35},
  number = {1},
  pages = {73--101},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851},
  abstract = {This paper contains a new approach toward a theory of robust estimation; it treats in detail the asymptotic theory of estimating a location parameter for contaminated normal distributions, and exhibits estimators--intermediaries between sample mean and sample median--that are asymptotically most robust (in a sense to be specified) among all translation invariant estimators. For the general background, see Tukey (1960) (p. 448 ff.) Let x1, {$\cdots$}, xn be independent random variables with common distribution function F(t - {$\xi$}). The problem is to estimate the location parameter {$\xi$}, but with the complication that the prototype distribution F(t) is only approximately known. I shall primarily be concerned with the model of indeterminacy F = (1 - {$\epsilon$}){$\Phi$} + {$\epsilon$} H, where \$0 \textbackslash leqq \textbackslash epsilon {$<$} 1\$ is a known number, {$\Phi$}(t) = (2{$\pi$})-1/2 {$\int$}t -{$\infty$} exp(-1/2s2) ds is the standard normal cumulative and H is an unknown contaminating distribution. This model arises for instance if the observations are assumed to be normal with variance 1, but a fraction {$\epsilon$} of them is affected by gross errors. Later on, I shall also consider other models of indeterminacy, e.g., \$\textbackslash sup\_t |F(t) - \textbackslash Phi(t)| \textbackslash leqq \textbackslash epsilon\$. Some inconvenience is caused by the fact that location and scale parameters are not uniquely determined: in general, for fixed {$\epsilon$}, there will be several values of {$\xi$} and {$\sigma$} such that \$\textbackslash sup\_t|F(t) - \textbackslash Phi((t - \textbackslash xi)/\textbackslash sigma)| \textbackslash leqq \textbackslash epsilon\$, and similarly for the contaminated case. Although this inherent and unavoidable indeterminacy is small if {$\epsilon$} is small and is rather irrelevant for practical purposes, it poses awkward problems for the theory, especially for optimality questions. To remove this difficulty, one may either (i) restrict attention to symmetric distributions, and estimate the location of the center of symmetry (this works for {$\xi$} but not for {$\sigma$}); or (ii) one may define the parameter to be estimated in terms of the estimator itself, namely by its asymptotic value for sample size n \textrightarrow{} {$\infty$}; or (iii) one may define the parameters by arbitrarily chosen functionals of the distribution (e.g., by the expectation, or the median of F). All three possibilities have unsatisfactory aspects, and I shall usually choose the variant which is mathematically most convenient. It is interesting to look back to the very origin of the theory of estimation, namely to Gauss and his theory of least squares. Gauss was fully aware that his main reason for assuming an underlying normal distribution and a quadratic loss function was mathematical, i.e., computational, convenience. In later times, this was often forgotten, partly because of the central limit theorem. However, if one wants to be honest, the central limit theorem can at most explain why many distributions occurring in practice are approximately normal. The stress is on the word "approximately." This raises a question which could have been asked already by Gauss, but which was, as far as I know, only raised a few years ago (notably by Tukey): What happens if the true distribution deviates slightly from the assumed normal one? As is now well known, the sample mean then may have a catastrophically bad performance: seemingly quite mild deviations may already explode its variance. Tukey and others proposed several more robust substitutes--trimmed means, Winsorized means, etc.--and explored their performance for a few typical violations of normality. A general theory of robust estimation is still lacking; it is hoped that the present paper will furnish the first few steps toward such a theory. At the core of the method of least squares lies the idea to minimize the sum of the squared "errors," that is, to adjust the unknown parameters such that the sum of the squares of the differences between observed and computed values is minimized. In the simplest case, with which we are concerned here, namely the estimation of a location parameter, one has to minimize the expression {$\sum$}i (xi - T)2; this is of course achieved by the sample mean T = {$\sum$}i xi/n. I should like to emphasize that no loss function is involved here; I am only describing how the least squares estimator is defined, and neither the underlying family of distributions nor the true value of the parameter to be estimated enters so far. It is quite natural to ask whether one can obtain more robustness by minimizing another function of the errors than the sum of their squares. We shall therefore concentrate our attention to estimators that can be defined by a minimum principle of the form (for a location parameter): T = Tn(x1, {$\cdots$}, xn) minimizes {$\sum$}i {$\rho$}(xi - T), \textbackslash begin\{equation*\} \textbackslash tag\{M\} where \textbackslash rho is a non-constant function. \textbackslash end\{equation*\} Of course, this definition generalizes at once to more general least squares type problems, where several parameters have to be determined. This class of estimators contains in particular (i) the sample mean ({$\rho$}(t) = t2), (ii) the sample median ({$\rho$}(t) = |t|), and more generally, (iii) all maximum likelihood estimators ({$\rho$}(t) = -log f(t), where f is the assumed density of the untranslated distribution). These (M)-estimators, as I shall call them for short, have rather pleasant asymptotic properties; sufficient conditions for asymptotic normality and an explicit expression for their asymptotic variance will be given. How should one judge the robustness of an estimator Tn(x) = Tn(x1, {$\cdots$}, xn)? Since ill effects from contamination are mainly felt for large sample sizes, it seems that one should primarily optimize large sample robustness properties. Therefore, a convenient measure of robustness for asymptotically normal estimators seems to be the supremum of the asymptotic variance (n \textrightarrow{} {$\infty$}) when F ranges over some suitable set of underlying distributions, in particular over the set of all F = (1 - {$\epsilon$}){$\Phi$} + {$\epsilon$} H for fixed {$\epsilon$} and symmetric H. On second thought, it turns out that the asymptotic variance is not only easier to handle, but that even for moderate values of n it is a better measure of performance than the actual variance, because (i) the actual variance of an estimator depends very much on the behavior of the tails of H, and the supremum of the actual variance is infinite for any estimator whose value is always contained in the convex hull of the observations. (ii) If an estimator is asymptotically normal, then the important central part of its distribution and confidence intervals for moderate confidence levels can better be approximated in terms of the asymptotic variance than in terms of the actual variance. If we adopt this measure of robustness, and if we restrict attention to (M)-estimators, then it will be shown that the most robust estimator is uniquely determined and corresponds to the following {$\rho$}:{$\rho$}(t) = 1/2t2 for \$|t| {$<$} k, \textbackslash rho(t) = k|t| - \textbackslash frac\{1\}\{2\}k\^2\$ for |t| {$\geq$} k, with k depending on {$\epsilon$}. This estimator is most robust even among all translation invariant estimators. Sample mean (k = {$\infty$}) and sample median (k = 0) are limiting cases corresponding to {$\epsilon$} = 0 and {$\epsilon$} = 1, respectively, and the estimator is closely related and asymptotically equivalent to Winsorizing. I recall the definition of Winsorizing: assume that the observations have been ordered, x1 {$\leq$} x2 {$\leq$} {$\cdots$} {$\leq$} xn, then the statistic T = n-1(gxg + 1 + xg + 1 + xg + 2 + {$\cdots$} + xn - h + hxn - h) is called the Winsorized mean, obtained by Winsorizing the g leftmost and the h rightmost observations. The above most robust (M)-estimators can be described by the same formula, except that in the first and in the last summand, the factors xg + 1 and xn - h have to be replaced by some numbers u, v satisfying xg {$\leq$} u {$\leq$} xg + 1 and xn - h {$\leq$} v {$\leq$} xn - h + 1, respectively; g, h, u and v depend on the sample. In fact, this (M)-estimator is the maximum likelihood estimator corresponding to a unique least favorable distribution F0 with density f0(t) = (1 - {$\epsilon$})(2{$\pi$})-1/2e-{$\rho$}(t). This f0 behaves like a normal density for small t, like an exponential density for large t. At least for me, this was rather surprising--I would have expected an f0 with much heavier tails. This result is a particular case of a more general one that can be stated roughly as follows: Assume that F belongs to some convex set C of distribution functions. Then the most robust (M)-estimator for the set C coincides with the maximum likelihood estimator for the unique F0 {$\epsilon$} C which has the smallest Fisher information number I(F) = {$\int$} (f'/f)2f dt among all F {$\epsilon$} C. Miscellaneous related problems will also be treated: the case of non-symmetric contaminating distributions; the most robust estimator for the model of indeterminacy \$\textbackslash sup\_t|F(t) - \textbackslash Phi(t)| \textbackslash leqq \textbackslash epsilon\$; robust estimation of a scale parameter; how to estimate location, if scale and {$\epsilon$} are unknown; numerical computation of the estimators; more general estimators, e.g., minimizing \$\textbackslash sum\_\{i {$<$} j\} \textbackslash rho(x\_i - T, x\_j - T)\$, where {$\rho$} is a function of two arguments. Questions of small sample size theory will not be touched in this paper.}
}

@article{hubertMinimumCovarianceDeterminant2018,
  title = {Minimum {{Covariance Determinant}} and {{Extensions}}},
  author = {Hubert, Mia and Debruyne, Michiel and Rousseeuw, Peter J.},
  year = {2018},
  month = may,
  journal = {WIREs Computational Statistics},
  volume = {10},
  number = {3},
  eprint = {1709.07045},
  eprinttype = {arxiv},
  primaryclass = {stat},
  issn = {1939-5108, 1939-0068},
  doi = {10.1002/wics.1421},
  abstract = {The Minimum Covariance Determinant (MCD) method is a highly robust estimator of multivariate location and scatter, for which a fast algorithm is available. Since estimating the covariance matrix is the cornerstone of many multivariate statistical methods, the MCD is an important building block when developing robust multivariate techniques. It also serves as a convenient and efficient tool for outlier detection. The MCD estimator is reviewed, along with its main properties such as affine equivariance, breakdown value, and influence function. We discuss its computation, and list applications and extensions of the MCD in applied and methodological multivariate statistics. Two recent extensions of the MCD are described. The first one is a fast deterministic algorithm which inherits the robustness of the MCD while being almost affine equivariant. The second is tailored to high-dimensional data, possibly with more dimensions than cases, and incorporates regularization to prevent singular matrices.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/gregoirepacreau/Zotero/storage/TKCX678N/Hubert et al. - 2018 - Minimum Covariance Determinant and Extensions.pdf;/Users/gregoirepacreau/Zotero/storage/ZBKQ4S7M/1709.html}
}

@article{hubertMinimumCovarianceDeterminant2018a,
  title = {Minimum {{Covariance Determinant}} and {{Extensions}}},
  author = {Hubert, Mia and Debruyne, Michiel and Rousseeuw, Peter J.},
  year = {2018},
  month = may,
  journal = {WIREs Computational Statistics},
  volume = {10},
  number = {3},
  eprint = {1709.07045},
  eprinttype = {arxiv},
  primaryclass = {stat},
  issn = {1939-5108, 1939-0068},
  doi = {10.1002/wics.1421},
  abstract = {The Minimum Covariance Determinant (MCD) method is a highly robust estimator of multivariate location and scatter, for which a fast algorithm is available. Since estimating the covariance matrix is the cornerstone of many multivariate statistical methods, the MCD is an important building block when developing robust multivariate techniques. It also serves as a convenient and efficient tool for outlier detection. The MCD estimator is reviewed, along with its main properties such as affine equivariance, breakdown value, and influence function. We discuss its computation, and list applications and extensions of the MCD in applied and methodological multivariate statistics. Two recent extensions of the MCD are described. The first one is a fast deterministic algorithm which inherits the robustness of the MCD while being almost affine equivariant. The second is tailored to high-dimensional data, possibly with more dimensions than cases, and incorporates regularization to prevent singular matrices.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/gregoirepacreau/Zotero/storage/DHS9EQ96/Hubert et al. - 2018 - Minimum Covariance Determinant and Extensions.pdf;/Users/gregoirepacreau/Zotero/storage/R94YJXTN/1709.html}
}

@article{huOpenGraphBenchmark2021,
  title = {Open {{Graph Benchmark}}: {{Datasets}} for {{Machine Learning}} on {{Graphs}}},
  shorttitle = {Open {{Graph Benchmark}}},
  author = {Hu, Weihua and Fey, Matthias and Zitnik, Marinka and Dong, Yuxiao and Ren, Hongyu and Liu, Bowen and Catasta, Michele and Leskovec, Jure},
  year = {2021},
  month = feb,
  journal = {arXiv:2005.00687 [cs, stat]},
  eprint = {2005.00687},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale, encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at https://ogb.stanford.edu .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/L3FGLQWL/Hu et al. - 2021 - Open Graph Benchmark Datasets for Machine Learnin.pdf;/Users/gregoirepacreau/Zotero/storage/A8F5262N/2005.html}
}

@article{jamshidianMLEstimationMean1999,
  title = {{{ML Estimation}} of {{Mean}} and {{Covariance Structures}} with {{Missing Data Using Complete Data Routines}}},
  author = {Jamshidian, Mortaza and Bentler, Peter M.},
  year = {1999},
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {24},
  number = {1},
  pages = {21--41},
  publisher = {{[American Educational Research Association, Sage Publications, Inc., American Statistical Association]}},
  issn = {1076-9986},
  doi = {10.2307/1165260},
  abstract = {We consider maximum likelihood (ML) estimation of mean and covariance structure models when data are missing. Expectation maximization (EM), generalized expectation maximization (GEM), Fletcher-Powell, and Fisher-scoring algorithms are described for parameter estimation. It is shown how the machinery within a software that handles the complete data problem can be utilized to implement each algorithm. A numerical differentiation method for obtaining the observed information matrix and the standard errors is given. This method also uses the complete data program machinery. The likelihood ratio test is discussed for testing hypotheses. Three examples are used to compare the cost of the four algorithms mentioned above, as well as to illustrate the standard error estimation and the test of hypothesis considered. The sensitivity of the ML estimates as well as the mean imputed and listwise deletion estimates to missing data mechanisms is investigated using three artificial data sets that are missing completely at random (MCAR), missing at random (MAR), and neither MCAR nor MAR.},
  file = {/Users/gregoirepacreau/Zotero/storage/LVYW77C2/Jamshidian et Bentler - 1999 - ML Estimation of Mean and Covariance Structures wi.pdf}
}

@article{josseConsistencySupervisedLearning2020,
  title = {On the Consistency of Supervised Learning with Missing Values},
  author = {Josse, Julie and Prost, Nicolas and Scornet, Erwan and Varoquaux, Ga{\"e}l},
  year = {2020},
  month = jul,
  journal = {arXiv:1902.06931 [cs, math, stat]},
  eprint = {1902.06931},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {In many application settings, the data have missing entries which make analysis challenging. An abundant literature addresses missing values in an inferential framework: estimating parameters and their variance from incomplete tables. Here, we consider supervised-learning settings: predicting a target when missing values appear in both training and testing data. We show the consistency of two approaches in prediction. A striking result is that the widely-used method of imputing with a constant, such as the mean prior to learning is consistent when missing values are not informative. This contrasts with inferential settings where mean imputation is pointed at for distorting the distribution of the data. That such a simple approach can be consistent is important in practice. We also show that a predictor suited for complete observations can predict optimally on incomplete data,through multiple imputation.Finally, to compare imputation with learning directly with a model that accounts for missing values, we analyze further decision trees. These can naturally tackle empirical risk minimization with missing values, due to their ability to handle the half-discrete nature of incomplete variables. After comparing theoretically and empirically different missing values strategies in trees, we recommend using the "missing incorporated in attribute" method as it can handle both non-informative and informative missing values.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/XLQR46AB/Josse et al. - 2020 - On the consistency of supervised learning with mis.pdf;/Users/gregoirepacreau/Zotero/storage/MWAHE82A/1902.html}
}

@misc{josseConsistencySupervisedLearning2020a,
  title = {On the Consistency of Supervised Learning with Missing Values},
  author = {Josse, Julie and Prost, Nicolas and Scornet, Erwan and Varoquaux, Ga{\"e}l},
  year = {2020},
  month = jul,
  number = {arXiv:1902.06931},
  eprint = {1902.06931},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1902.06931},
  abstract = {In many application settings, the data have missing entries which make analysis challenging. An abundant literature addresses missing values in an inferential framework: estimating parameters and their variance from incomplete tables. Here, we consider supervised-learning settings: predicting a target when missing values appear in both training and testing data. We show the consistency of two approaches in prediction. A striking result is that the widely-used method of imputing with a constant, such as the mean prior to learning is consistent when missing values are not informative. This contrasts with inferential settings where mean imputation is pointed at for distorting the distribution of the data. That such a simple approach can be consistent is important in practice. We also show that a predictor suited for complete observations can predict optimally on incomplete data,through multiple imputation.Finally, to compare imputation with learning directly with a model that accounts for missing values, we analyze further decision trees. These can naturally tackle empirical risk minimization with missing values, due to their ability to handle the half-discrete nature of incomplete variables. After comparing theoretically and empirically different missing values strategies in trees, we recommend using the "missing incorporated in attribute" method as it can handle both non-informative and informative missing values.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/YV27TAAX/Josse et al. - 2020 - On the consistency of supervised learning with mis.pdf;/Users/gregoirepacreau/Zotero/storage/8S6MIPKB/1902.html}
}

@misc{katayamaRobustSparseGaussian2018,
  title = {Robust and Sparse {{Gaussian}} Graphical Modeling under Cell-Wise Contamination},
  author = {Katayama, Shota and Fujisawa, Hironori and Drton, Mathias},
  year = {2018},
  month = feb,
  number = {arXiv:1802.05475},
  eprint = {1802.05475},
  eprinttype = {arxiv},
  primaryclass = {stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1802.05475},
  abstract = {Graphical modeling explores dependences among a collection of variables by inferring a graph that encodes pairwise conditional independences. For jointly Gaussian variables, this translates into detecting the support of the precision matrix. Many modern applications feature high-dimensional and contaminated data that complicate this task. In particular, traditional robust methods that down-weight entire observation vectors are often inappropriate as high-dimensional data may feature partial contamination in many observations. We tackle this problem by giving a robust method for sparse precision matrix estimation based on the \$\textbackslash gamma\$-divergence under a cell-wise contamination model. Simulation studies demonstrate that our procedure outperforms existing methods especially for highly contaminated data.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/gregoirepacreau/Zotero/storage/GP5PGCAU/Katayama et al. - 2018 - Robust and sparse Gaussian graphical modeling unde.pdf;/Users/gregoirepacreau/Zotero/storage/ICQ6V59H/1802.html}
}

@article{katzProvingAdversarialRobustness2017,
  title = {Towards {{Proving}} the {{Adversarial Robustness}} of {{Deep Neural Networks}}},
  author = {Katz, Guy and Barrett, Clark and Dill, David L. and Julian, Kyle and Kochenderfer, Mykel J.},
  year = {2017},
  month = sep,
  journal = {Electronic Proceedings in Theoretical Computer Science},
  volume = {257},
  pages = {19--26},
  issn = {2075-2180},
  doi = {10.4204/EPTCS.257.3},
  langid = {english},
  file = {/Users/gregoirepacreau/Zotero/storage/3EDG47BC/Katz et al. - 2017 - Towards Proving the Adversarial Robustness of Deep.pdf}
}

@misc{kayaEccentricityAssetManagement2013,
  type = {{{SSRN Scholarly Paper}}},
  title = {Eccentricity in {{Asset Management}}},
  author = {Kaya, Hakan},
  year = {2013},
  month = dec,
  number = {2350429},
  address = {{Rochester, NY}},
  doi = {10.2139/ssrn.2350429},
  abstract = {We describe how networks based on information theory can help measure and visualize systemic risk, enhance diversification, and help price assets.  To do this, we first define a distance measure based on the mutual information between asset pairs and use this measure in the construction of minimum spanning trees.  The dynamics of the shape and the descriptive statistics of these trees are analyzed in various investment domains. The method provides evidence of regime changes in dependency structures prior to market sell-offs, and as such, it is a potential candidate for monitoring systemic risk.  We also provide empirical evidence that the assets that are located towards the center of the network tend to have higher returns.  Finally, an investment strategy that utilizes network centrality information is shown to add value historically.},
  langid = {english},
  keywords = {minimum,portfolio allocation,risk management,Risk measurement},
  file = {/Users/gregoirepacreau/Zotero/storage/4ECCZDLX/Kaya - 2013 - Eccentricity in Asset Management.pdf;/Users/gregoirepacreau/Zotero/storage/BJFXHCJC/papers.html}
}

@article{kipfSemiSupervisedClassificationGraph2017,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  author = {Kipf, Thomas N. and Welling, Max},
  year = {2017},
  month = feb,
  journal = {arXiv:1609.02907 [cs, stat]},
  eprint = {1609.02907},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/99A8VZ4S/Kipf et Welling - 2017 - Semi-Supervised Classification with Graph Convolut.pdf;/Users/gregoirepacreau/Zotero/storage/A23J73VW/1609.html}
}

@misc{koltchinskiiConcentrationInequalitiesMoment2014,
  title = {Concentration {{Inequalities}} and {{Moment Bounds}} for {{Sample Covariance Operators}}},
  author = {Koltchinskii, Vladimir and Lounici, Karim},
  year = {2014},
  month = jul,
  number = {arXiv:1405.2468},
  eprint = {1405.2468},
  eprinttype = {arxiv},
  primaryclass = {math},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1405.2468},
  abstract = {Let \$X,X\_1,\textbackslash dots, X\_n,\textbackslash dots\$ be i.i.d. centered Gaussian random variables in a separable Banach space \$E\$ with covariance operator \$\textbackslash Sigma:\$ \$\$ \textbackslash Sigma:E\^\{\textbackslash ast\}\textbackslash mapsto E,\textbackslash{} \textbackslash{} \textbackslash Sigma u = \{\textbackslash mathbb E\}\textbackslash langle X,u\textbackslash rangle, u\textbackslash in E\^\{\textbackslash ast\}. \$\$ The sample covariance operator \$\textbackslash hat \textbackslash Sigma:E\^\{\textbackslash ast\}\textbackslash mapsto E\$ is defined as \$\$ \textbackslash hat \textbackslash Sigma u := n\^\{-1\}\textbackslash sum\_\{j=1\}\^n \textbackslash langle X\_j,u\textbackslash rangle X\_j, u\textbackslash in E\^\{\textbackslash ast\}. \$\$ The goal of the paper is to obtain concentration inequalities and expectation bounds for the operator norm \$\textbackslash |\textbackslash hat \textbackslash Sigma-\textbackslash Sigma\textbackslash |\$ of the deviation of the sample covariance operator from the true covariance operator. In particular, it is shown that \$\$ \{\textbackslash mathbb E\}\textbackslash |\textbackslash hat \textbackslash Sigma-\textbackslash Sigma\textbackslash |\textbackslash asymp \textbackslash |\textbackslash Sigma\textbackslash |\textbackslash biggl(\textbackslash sqrt\{\textbackslash frac\{\{\textbackslash bf r\}(\textbackslash Sigma)\}\{n\}\}\textbackslash bigvee \textbackslash frac\{\{\textbackslash bf r\}(\textbackslash Sigma)\}\{n\}\textbackslash biggr), \$\$ where \$\$ \{\textbackslash bf r\}(\textbackslash Sigma):=\textbackslash frac\{\textbackslash Bigl(\{\textbackslash mathbb E\}\textbackslash |X\textbackslash |\textbackslash Bigr)\^2\}\{\textbackslash |\textbackslash Sigma\textbackslash |\}. \$\$ Moreover, under the assumption that \$\{\textbackslash bf r\}(\textbackslash Sigma)\textbackslash lesssim n,\$ it is proved that, for all \$t\textbackslash geq 1,\$ with probability at least \$1-e\^\{-t\}\$ \textbackslash begin\{align*\} \textbackslash Bigl|\textbackslash |\textbackslash hat\textbackslash Sigma - \textbackslash Sigma\textbackslash |-\{\textbackslash mathbb E\}\textbackslash |\textbackslash hat\textbackslash Sigma - \textbackslash Sigma\textbackslash |\textbackslash Bigr| \textbackslash lesssim \textbackslash |\textbackslash Sigma\textbackslash |\textbackslash biggl(\textbackslash sqrt\{\textbackslash frac\{t\}\{n\}\}\textbackslash bigvee \textbackslash frac\{t\}\{n\}\textbackslash biggr). \textbackslash end\{align*\}},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Probability},
  file = {/Users/gregoirepacreau/Zotero/storage/6JLHUEPS/Koltchinskii et Lounici - 2014 - Concentration Inequalities and Moment Bounds for S.pdf;/Users/gregoirepacreau/Zotero/storage/AJ2W9PKY/1405.html}
}

@article{koltchinskiiConcentrationInequalitiesMoment2017,
  title = {Concentration Inequalities and Moment Bounds for Sample Covariance Operators},
  author = {Koltchinskii, Vladimir and Lounici, Karim},
  year = {2017},
  month = feb,
  journal = {Bernoulli},
  volume = {23},
  number = {1},
  pages = {110--133},
  publisher = {{Bernoulli Society for Mathematical Statistics and Probability}},
  issn = {1350-7265},
  doi = {10.3150/15-BEJ730},
  abstract = {Let \$X,X\_\{1\},\textbackslash dots,X\_\{n\},\textbackslash dots\$ be i.i.d. centered Gaussian random variables in a separable Banach space \$E\$ with covariance operator \$\textbackslash Sigma\$: \textbackslash [\textbackslash Sigma:E\^\{\textbackslash ast\}\textbackslash mapsto E,\textbackslash qquad\textbackslash Sigma u=\textbackslash mathbb\{E\}\textbackslash langle X,u\textbackslash rangle X,\textbackslash qquad u\textbackslash in E\^\{\textbackslash ast\}.\textbackslash ] The sample covariance operator \$\textbackslash hat\{\textbackslash Sigma\}:E\^\{\textbackslash ast\}\textbackslash mapsto E\$ is defined as \textbackslash [\textbackslash hat\{\textbackslash Sigma\}u:=n\^\{-1\}\textbackslash sum\_\{j=1\}\^\{n\}\textbackslash langle X\_\{j\},u\textbackslash rangle X\_\{j\},\textbackslash qquad u\textbackslash in E\^\{\textbackslash ast\}.\textbackslash ] The goal of the paper is to obtain concentration inequalities and expectation bounds for the operator norm \$\textbackslash Vert \textbackslash hat\{\textbackslash Sigma\}-\textbackslash Sigma\textbackslash Vert \$ of the deviation of the sample covariance operator from the true covariance operator. In particular, it is shown that \textbackslash [\textbackslash mathbb\{E\}\textbackslash Vert \textbackslash hat\{\textbackslash Sigma\}-\textbackslash Sigma\textbackslash Vert \textbackslash asymp\textbackslash Vert \textbackslash Sigma\textbackslash Vert (\textbackslash sqrt\{\textbackslash frac\{\{\textbackslash mathbf\{r\}\}(\textbackslash Sigma)\}\{n\}\}\textbackslash vee \textbackslash frac\{\{\textbackslash mathbf\{r\}\}(\textbackslash Sigma)\}\{n\}),\textbackslash ] where \textbackslash [\{\textbackslash mathbf\{r\}\}(\textbackslash Sigma):=\textbackslash frac\{(\textbackslash mathbb\{E\}\textbackslash Vert X\textbackslash Vert )\^\{2\}\}\{\textbackslash Vert \textbackslash Sigma\textbackslash Vert \}.\textbackslash ] Moreover, it is proved that, under the assumption that \$\{\textbackslash mathbf\{r\}\}(\textbackslash Sigma)\textbackslash leq n\$, for all \$t\textbackslash geq1\$, with probability at least \$1-e\^\{-t\}\$ \textbackslash [\textbackslash vert \textbackslash Vert \textbackslash hat\{\textbackslash Sigma\}-\textbackslash Sigma\textbackslash Vert -M\textbackslash vert \textbackslash lesssim\textbackslash Vert \textbackslash Sigma\textbackslash Vert (\textbackslash sqrt\{\textbackslash frac\{t\}\{n\}\}\textbackslash vee \textbackslash frac\{t\}\{n\}),\textbackslash ] where \$M\$ is either the median, or the expectation of \$\textbackslash Vert \textbackslash hat\{\textbackslash Sigma\}-\textbackslash Sigma\textbackslash Vert \$. On the other hand, under the assumption that \$\{\textbackslash mathbf\{r\}\}(\textbackslash Sigma)\textbackslash geq n\$, for all \$t\textbackslash geq1\$, with probability at least \$1-e\^\{-t\}\$ \textbackslash [\textbackslash vert \textbackslash Vert \textbackslash hat\{\textbackslash Sigma\}-\textbackslash Sigma\textbackslash Vert -M\textbackslash vert \textbackslash lesssim\textbackslash Vert \textbackslash Sigma\textbackslash Vert (\textbackslash sqrt\{\textbackslash frac\{\{\textbackslash mathbf\{r\}\}(\textbackslash Sigma)\}\{n\}\}\textbackslash sqrt\{\textbackslash frac\{t\}\{n\}\}\textbackslash vee \textbackslash frac\{t\}\{n\}).\textbackslash ]},
  keywords = {Concentration inequalities,Effective rank,moment bounds,Sample covariance},
  file = {/Users/gregoirepacreau/Zotero/storage/VJ2BYGHR/Koltchinskii et Lounici - 2017 - Concentration inequalities and moment bounds for s.pdf;/Users/gregoirepacreau/Zotero/storage/TCCP3HI5/15-BEJ730.html}
}

@misc{koltchinskiiEstimationLowRankCovariance2015,
  title = {Estimation of {{Low-Rank Covariance Function}}},
  author = {Koltchinskii, Vladimir and Lounici, Karim and Tsybakov, Alexander B.},
  year = {2015},
  month = apr,
  number = {arXiv:1504.03009},
  eprint = {1504.03009},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1504.03009},
  abstract = {We consider the problem of estimating a low rank covariance function \$K(t,u)\$ of a Gaussian process \$S(t), t\textbackslash in [0,1]\$ based on \$n\$ i.i.d. copies of \$S\$ observed in a white noise. We suggest a new estimation procedure adapting simultaneously to the low rank structure and the smoothness of the covariance function. The new procedure is based on nuclear norm penalization and exhibits superior performances as compared to the sample covariance function by a polynomial factor in the sample size \$n\$. Other results include a minimax lower bound for estimation of low-rank covariance functions showing that our procedure is optimal as well as a scheme to estimate the unknown noise variance of the Gaussian process.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory},
  file = {/Users/gregoirepacreau/Zotero/storage/IN83AC3S/Koltchinskii et al. - 2015 - Estimation of Low-Rank Covariance Function.pdf;/Users/gregoirepacreau/Zotero/storage/IJJRU6UP/1504.html}
}

@article{koltchinskiiLowRankEstimation2013,
  title = {Low Rank Estimation of Smooth Kernels on Graphs},
  author = {Koltchinskii, Vladimir and Rangel, Pedro},
  year = {2013},
  month = apr,
  journal = {The Annals of Statistics},
  volume = {41},
  number = {2},
  pages = {604--640},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/13-AOS1088},
  abstract = {Let \$(V,A)\$ be a weighted graph with a finite vertex set \$V\$, with a symmetric matrix of nonnegative weights \$A\$ and with Laplacian \$\textbackslash Delta\$. Let \$S\_\{\textbackslash ast\}: V\textbackslash times V\textbackslash mapsto\{\textbackslash mathbb\{R\}\}\$ be a symmetric kernel defined on the vertex set \$V\$. Consider \$n\$ i.i.d. observations \$(X\_\{j\},X\_\{j\}',Y\_\{j\})\$, \$j=1,\textbackslash ldots,n\$, where \$X\_\{j\}\$, \$X\_\{j\}'\$ are independent random vertices sampled from the uniform distribution in \$V\$ and \$Y\_\{j\}\textbackslash in\{\textbackslash mathbb\{R\}\}\$ is a real valued response variable such that \$\{\textbackslash mathbb\{E\}\}(Y\_\{j\}|X\_\{j\},X\_\{j\}')=S\_\{\textbackslash ast\}(X\_\{j\},X\_\{j\}')\$, \$j=1,\textbackslash ldots,n\$. The goal is to estimate the kernel \$S\_\{\textbackslash ast\}\$ based on the data \$(X\_\{1\},X\_\{1\}',Y\_\{1\}),\textbackslash ldots,(X\_\{n\},X\_\{n\}',Y\_\{n\})\$ and under the assumption that \$S\_\{\textbackslash ast\}\$ is low rank and, at the same time, smooth on the graph (the smoothness being characterized by discrete Sobolev norms defined in terms of the graph Laplacian). We obtain several results for such problems including minimax lower bounds on the \$L\_\{2\}\$-error and upper bounds for penalized least squares estimators both with nonconvex and with convex penalties.},
  keywords = {60B20,60G15,62H12,62J99,discrete Sobolev norm,graph Laplacian,low-rank matrix estimation,Matrix completion,matrix Lasso,minimax error bound,nuclear norm,optimal error rate},
  file = {/Users/gregoirepacreau/Zotero/storage/7UFN9BVD/Koltchinskii et Rangel - 2013 - Low rank estimation of smooth kernels on graphs.pdf;/Users/gregoirepacreau/Zotero/storage/7VHVYWS2/13-AOS1088.html}
}

@article{louniciHighdimensionalCovarianceMatrix2014,
  title = {High-Dimensional Covariance Matrix Estimation with Missing Observations},
  author = {Lounici, Karim},
  year = {2014},
  month = aug,
  journal = {Bernoulli},
  volume = {20},
  number = {3},
  pages = {1029--1058},
  publisher = {{Bernoulli Society for Mathematical Statistics and Probability}},
  issn = {1350-7265},
  doi = {10.3150/12-BEJ487},
  abstract = {In this paper, we study the problem of high-dimensional covariance matrix estimation with missing observations. We propose a simple procedure computationally tractable in high-dimension and that does not require imputation of the missing data. We establish non-asymptotic sparsity oracle inequalities for the estimation of the covariance matrix involving the Frobenius and the spectral norms which are valid for any setting of the sample size, probability of a missing observation and the dimensionality of the covariance matrix. We further establish minimax lower bounds showing that our rates are minimax optimal up to a logarithmic factor.},
  keywords = {Covariance matrix,Lasso,low-rank matrix estimation,missing observations,non-commutative Bernstein inequality,Optimal rate of convergence},
  file = {/Users/gregoirepacreau/Zotero/storage/6JB9WFM7/Lounici - 2014 - High-dimensional covariance matrix estimation with.pdf;/Users/gregoirepacreau/Zotero/storage/ILYFUVPB/12-BEJ487.html}
}

@inproceedings{mitliagkasMemoryLimitedStreaming2013,
  title = {Memory {{Limited}}, {{Streaming PCA}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Mitliagkas, Ioannis and Caramanis, Constantine and Jain, Prateek},
  year = {2013},
  volume = {26},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/gregoirepacreau/Zotero/storage/99VR3REA/Mitliagkas et al. - 2013 - Memory Limited, Streaming PCA.pdf}
}

@book{mohriFoundationsMachineLearning2012,
  title = {Foundations of {{Machine Learning}}},
  author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  editor = {Bach, Francis},
  year = {2012},
  month = aug,
  series = {Adaptive {{Computation}} and {{Machine Learning}} Series},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  abstract = {Fundamental topics in machine learning are presented along with theoretical and conceptual tools for the discussion and proof of algorithms.},
  isbn = {978-0-262-01825-8},
  langid = {english}
}

@article{mukherjeeLepskiMethodAdaptive2016,
  title = {Lepski's {{Method}} and {{Adaptive Estimation}} of {{Nonlinear Integral Functionals}} of {{Density}}},
  author = {Mukherjee, Rajarshi and Tchetgen, Eric Tchetgen and Robins, James},
  year = {2016},
  month = jan,
  journal = {arXiv:1508.00249 [math, stat]},
  eprint = {1508.00249},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {We study the adaptive minimax estimation of non-linear integral functionals of a density and extend the results obtained for linear and quadratic functionals to general functionals. The typical rate optimal non-adaptive minimax estimators of "smooth" non-linear functionals are higher order U-statistics. Since Lepski's method requires tight control of tails of such estimators, we bypass such calculations by a modification of Lepski's method which is applicable in such situations. As a necessary ingredient, we also provide a method to control higher order moments of minimax estimator of cubic integral functionals. Following a standard constrained risk inequality method, we also show the optimality of our adaptation rates.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory},
  file = {/Users/gregoirepacreau/Zotero/storage/SZ6HEHA5/Mukherjee et al. - 2016 - Lepski's Method and Adaptive Estimation of Nonline.pdf;/Users/gregoirepacreau/Zotero/storage/879EWPN4/1508.html}
}

@article{olmoOptimalPortfolioAllocation2021,
  title = {Optimal Portfolio Allocation and Asset Centrality Revisited},
  author = {Olmo, Jose},
  year = {2021},
  month = sep,
  journal = {Quantitative Finance},
  volume = {21},
  number = {9},
  pages = {1475--1490},
  publisher = {{Routledge}},
  issn = {1469-7688},
  doi = {10.1080/14697688.2021.1937298},
  abstract = {This paper revisits the relationship between eigenvector asset centrality and optimal asset allocation in a minimum variance portfolio. We show that the standard definition of eigenvector centrality is misleading when the adjacency matrix in a network can take negative values. This is, for example, the case when the network topology is induced by the correlation matrix between assets in a portfolio. To correct for this, we introduce the concept of positive and negative eigenvector centrality. Our results show that the loss function associated to the minimum variance portfolio is positively/negatively related to the positive and negative eigenvector centrality under short-selling constraints but cannot be generalized beyond that. Furthermore, in contrast to what is claimed in the related literature, this relationship does not imply any monotonic relationship between the centrality of an asset and its optimal portfolio allocation. These theoretical insights are illustrated empirically in a portfolio allocation exercise with assets from U.S. and U.K. financial markets.},
  keywords = {Constant conditional correlation,Dynamic conditional correlation,Eigenvector centrality,Markowitz portfolio allocation,Spectral decomposition},
  annotation = {\_eprint: https://doi.org/10.1080/14697688.2021.1937298},
  file = {/Users/gregoirepacreau/Zotero/storage/K6465539/Olmo - 2021 - Optimal portfolio allocation and asset centrality .pdf}
}

@article{pajorMetricEntropyGrassmann1998,
  title = {Metric {{Entropy}} of the {{Grassmann Manifold}}},
  author = {Pajor, Alain},
  year = {1998},
  journal = {Complex Geometry Analysis},
  series = {{{MSRI Publications}}},
  volume = {34},
  pages = {181--188},
  abstract = {Abstract. The knowledge of the metric entropy of precompact subsets of operators on finite dimensional Euclidean space is important in particular in the probabilistic methods developped by E. D. Gluskin and S. Szarek for constructing certain random Banach spaces. We give a new argument for estimating the metric entropy of some subsets such as the Grassmann manifold equipped with natural metrics. Here, the Grassmann manifold is thought of as the set of orthogonal projection of given rank.},
  file = {/Users/gregoirepacreau/Zotero/storage/MV3GY8VL/Pajor - Metric Entropy of the Grassmann Manifold.pdf;/Users/gregoirepacreau/Zotero/storage/PAF4HA35/summary.html}
}

@misc{pamfilDYNOTEARSStructureLearning2020,
  title = {{{DYNOTEARS}}: {{Structure Learning}} from {{Time-Series Data}}},
  shorttitle = {{{DYNOTEARS}}},
  author = {Pamfil, Roxana and Sriwattanaworachai, Nisara and Desai, Shaan and Pilgerstorfer, Philip and Beaumont, Paul and Georgatzis, Konstantinos and Aragam, Bryon},
  year = {2020},
  month = apr,
  number = {arXiv:2002.00498},
  eprint = {2002.00498},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2002.00498},
  abstract = {We revisit the structure learning problem for dynamic Bayesian networks and propose a method that simultaneously estimates contemporaneous (intra-slice) and time-lagged (inter-slice) relationships between variables in a time-series. Our approach is score-based, and revolves around minimizing a penalized loss subject to an acyclicity constraint. To solve this problem, we leverage a recent algebraic result characterizing the acyclicity constraint as a smooth equality constraint. The resulting algorithm, which we call DYNOTEARS, outperforms other methods on simulated data, especially in high-dimensions as the number of variables increases. We also apply this algorithm on real datasets from two different domains, finance and molecular biology, and analyze the resulting output. Compared to state-of-the-art methods for learning dynamic Bayesian networks, our method is both scalable and accurate on real data. The simple formulation and competitive performance of our method make it suitable for a variety of problems where one seeks to learn connections between variables across time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/UPSWJEX8/Pamfil et al. - 2020 - DYNOTEARS Structure Learning from Time-Series Dat.pdf;/Users/gregoirepacreau/Zotero/storage/WUVXQ4N7/2002.html}
}

@article{pauliTrainingRobustNeural2022,
  title = {Training Robust Neural Networks Using {{Lipschitz}} Bounds},
  author = {Pauli, Patricia and Koch, Anne and Berberich, Julian and Kohler, Paul and Allg{\"o}wer, Frank},
  year = {2022},
  journal = {IEEE Control Systems Letters},
  volume = {6},
  eprint = {2005.02929},
  eprinttype = {arxiv},
  pages = {121--126},
  issn = {2475-1456},
  doi = {10.1109/LCSYS.2021.3050444},
  abstract = {Due to their susceptibility to adversarial perturbations, neural networks (NNs) are hardly used in safety-critical applications. One measure of robustness to such perturbations in the input is the Lipschitz constant of the input-output map defined by an NN. In this work, we propose a framework to train multi-layer NNs while at the same time encouraging robustness by keeping their Lipschitz constant small, thus addressing the robustness issue. More specifically, we design an optimization scheme based on the Alternating Direction Method of Multipliers that minimizes not only the training loss of an NN but also its Lipschitz constant resulting in a semidefinite programming based training procedure that promotes robustness. We design two versions of this training procedure. The first one includes a regularizer that penalizes an accurate upper bound on the Lipschitz constant. The second one allows to enforce a desired Lipschitz bound on the NN at all times during training. Finally, we provide two examples to show that the proposed framework successfully increases the robustness of NNs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/LISPR8AB/Pauli et al. - 2022 - Training robust neural networks using Lipschitz bo.pdf}
}

@book{pearlCausality2009,
  title = {Causality},
  author = {Pearl, Judea},
  year = {2009},
  edition = {Second},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511803161},
  abstract = {Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. Cited in more than 2,100 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interest to students and professionals in a wide variety of fields. Dr Judea Pearl has received the 2011 Rumelhart Prize for his leading research in Artificial Intelligence (AI) and systems from The Cognitive Science Society.},
  isbn = {978-0-521-89560-6},
  file = {/Users/gregoirepacreau/Zotero/storage/NMSLGMLA/B0046844FAE10CBF274D4ACBDAEB5F5B.html}
}

@article{pinotTheoreticalEvidenceAdversarial,
  title = {Theoretical Evidence for Adversarial Robustness through Randomization},
  author = {Pinot, Rafael and Meunier, Laurent and Araujo, Alexandre and Kashima, Hisashi and Yger, Florian and {Gouy-Pailler}, C{\'e}dric and Atif, Jamal},
  pages = {11},
  abstract = {This paper investigates the theory of robustness against adversarial attacks. It focuses on the family of randomization techniques that consist in injecting noise in the network at inference time. These techniques have proven effective in many contexts, but lack theoretical arguments. We close this gap by presenting a theoretical analysis of these approaches, hence explaining why they perform well in practice. More precisely, we make two new contributions. The first one relates the randomization rate to robustness to adversarial attacks. This result applies for the general family of exponential distributions, and thus extends and unifies the previous approaches. The second contribution consists in devising a new upper bound on the adversarial risk gap of randomized neural networks. We support our theoretical claims with a set of experiments.},
  langid = {english},
  file = {/Users/gregoirepacreau/Zotero/storage/ASV3SBTG/Pinot et al. - Theoretical evidence for adversarial robustness th.pdf}
}

@article{pradoBuildingDiversifiedPortfolios2016,
  title = {Building {{Diversified Portfolios}} That {{Outperform Out}} of {{Sample}}},
  author = {de Prado, Marcos L{\'o}pez},
  year = {2016},
  month = may,
  journal = {The Journal of Portfolio Management},
  volume = {42},
  number = {4},
  pages = {59--69},
  publisher = {{Institutional Investor Journals Umbrella}},
  issn = {0095-4918, 2168-8656},
  doi = {10.3905/jpm.2016.42.4.059},
  abstract = {In this article, the author introduces the Hierarchical Risk Parity (HRP) approach to address three major concerns of quadratic optimizers, in general, and Markowitz's critical line algorithm (CLA), in particular: instability, concentration, and underperformance. HRP applies modern mathematics (graph theory and machine-learning techniques) to build a diversified portfolio based on the information contained in the covariance matrix. However, unlike quadratic optimizers, HRP does not require the invertibility of the covariance matrix. In fact, HRP can compute a portfolio on an ill-degenerated or even a singular covariance matrix\textemdash an impossible feat for quadratic optimizers. Monte Carlo experiments show that HRP delivers lower out-ofsample variance than CLA, even though minimum variance is CLA's optimization objective. HRP also produces less risky portfolios out of sample compared to traditional risk parity methods. TOPICS: Statistical methods, portfolio construction},
  copyright = {\textcopyright{} 2016 Pageant Media Ltd},
  langid = {english},
  file = {/Users/gregoirepacreau/Zotero/storage/PQQI5MGP/59.html}
}

@misc{ProblemAdaptiveEstimation,
  title = {On a {{Problem}} of {{Adaptive Estimation}} in {{Gaussian White Noise}}},
  doi = {10.1137/1135065},
  howpublished = {https://epubs.siam.org/doi/epdf/10.1137/1135065},
  langid = {english},
  file = {/Users/gregoirepacreau/Zotero/storage/MCL97QNJ/1135065.html}
}

@article{quFindingSparseVector2016,
  title = {Finding a Sparse Vector in a Subspace: {{Linear}} Sparsity Using Alternating Directions},
  shorttitle = {Finding a Sparse Vector in a Subspace},
  author = {Qu, Qing and Sun, Ju and Wright, John},
  year = {2016},
  month = oct,
  journal = {IEEE Transactions on Information Theory},
  volume = {62},
  number = {10},
  eprint = {1412.4659},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  pages = {5855--5880},
  issn = {0018-9448, 1557-9654},
  doi = {10.1109/TIT.2016.2601599},
  abstract = {Is it possible to find the sparsest vector (direction) in a generic subspace \$\textbackslash mathcal\{S\} \textbackslash subseteq \textbackslash mathbb\{R\}\^p\$ with \$\textbackslash mathrm\{dim\}(\textbackslash mathcal\{S\})= n {$<$} p\$? This problem can be considered a homogeneous variant of the sparse recovery problem, and finds connections to sparse dictionary learning, sparse PCA, and many other problems in signal processing and machine learning. In this paper, we focus on a **planted sparse model** for the subspace: the target sparse vector is embedded in an otherwise random subspace. Simple convex heuristics for this planted recovery problem provably break down when the fraction of nonzero entries in the target sparse vector substantially exceeds \$O(1/\textbackslash sqrt\{n\})\$. In contrast, we exhibit a relatively simple nonconvex approach based on alternating directions, which provably succeeds even when the fraction of nonzero entries is \$\textbackslash Omega(1)\$. To the best of our knowledge, this is the first practical algorithm to achieve linear scaling under the planted sparse model. Empirically, our proposed algorithm also succeeds in more challenging data models, e.g., sparse dictionary learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/MJ5UCLG9/Qu et al. - 2016 - Finding a sparse vector in a subspace Linear spar.pdf;/Users/gregoirepacreau/Zotero/storage/5N4RSLSA/1412.html}
}

@article{raymaekersFastRobustCorrelation2021,
  title = {Fast Robust Correlation for High-Dimensional Data},
  author = {Raymaekers, Jakob and Rousseeuw, Peter J.},
  year = {2021},
  month = apr,
  journal = {Technometrics},
  volume = {63},
  number = {2},
  eprint = {1712.05151},
  eprinttype = {arxiv},
  primaryclass = {stat},
  pages = {184--198},
  issn = {0040-1706, 1537-2723},
  doi = {10.1080/00401706.2019.1677270},
  abstract = {The product moment covariance is a cornerstone of multivariate data analysis, from which one can derive correlations, principal components, Mahalanobis distances and many other results. Unfortunately the product moment covariance and the corresponding Pearson correlation are very susceptible to outliers (anomalies) in the data. Several robust measures of covariance have been developed, but few are suitable for the ultrahigh dimensional data that are becoming more prevalent nowadays. For that one needs methods whose computation scales well with the dimension, are guaranteed to yield a positive semidefinite covariance matrix, and are sufficiently robust to outliers as well as sufficiently accurate in the statistical sense of low variability. We construct such methods using data transformations. The resulting approach is simple, fast and widely applicable. We study its robustness by deriving influence functions and breakdown values, and computing the mean squared error on contaminated data. Using these results we select a method that performs well overall. This also allows us to construct a faster version of the DetectDeviatingCells method (Rousseeuw and Van den Bossche, 2018) to detect cellwise outliers, that can deal with much higher dimensions. The approach is illustrated on genomic data with 12,000 variables and color video data with 920,000 dimensions.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/gregoirepacreau/Zotero/storage/LE6NS632/Raymaekers et Rousseeuw - 2021 - Fast robust correlation for high-dimensional data.pdf;/Users/gregoirepacreau/Zotero/storage/MULQNA6D/1712.html}
}

@article{raymaekersHandlingCellwiseOutliers2020,
  title = {Handling Cellwise Outliers by Sparse Regression and Robust Covariance},
  author = {Raymaekers, Jakob and Rousseeuw, Peter J.},
  year = {2020},
  month = dec,
  journal = {arXiv:1912.12446 [stat]},
  eprint = {1912.12446},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {We propose a data-analytic method for detecting cellwise outliers. Given a robust covariance matrix, outlying cells (entries) in a row are found by the cellHandler technique which combines lasso regression with a stepwise application of constructed cutoff values. The penalty term of the lasso has a physical interpretation as the total distance that suspicious cells need to move in order to bring their row into the fold. For estimating a cellwise robust covariance matrix we construct a detection-imputation method which alternates between flagging outlying cells and updating the covariance matrix as in the EM algorithm. The proposed methods are illustrated by simulations and on real data about volatile organic compounds in children.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {/Users/gregoirepacreau/Zotero/storage/C3HIHA3B/Raymaekers et Rousseeuw - 2020 - Handling cellwise outliers by sparse regression an.pdf;/Users/gregoirepacreau/Zotero/storage/4NJLLBKP/1912.html}
}

@misc{ResearchGate,
  title = {{{ResearchGate}}},
  howpublished = {https://www.researchgate.net/publication/263503360\_Robust\_Constrained\_Clustering\_in\_Presence\_of\_Entry-Wise\_Outliers/link/53d37caf0cf228d363e97924/download},
  file = {/Users/gregoirepacreau/Zotero/storage/UVJWH57L/263503360_Robust_Constrained_Clustering_in_Presence_of_Entry-Wise_Outliers.html}
}

@misc{ResearchGatea,
  title = {{{ResearchGate}}},
  howpublished = {https://www.researchgate.net/publication/263503360\_Robust\_Constrained\_Clustering\_in\_Presence\_of\_Entry-Wise\_Outliers/link/53d37caf0cf228d363e97924/download}
}

@article{rosadiRobustCovarianceEstimators2020,
  title = {Robust Covariance Estimators for Mean-Variance Portfolio Optimization with Transaction Lots},
  author = {Rosadi, Dedi and Setiawan, Ezra Putranda and Templ, Matthias and Filzmoser, Peter},
  year = {2020},
  month = jan,
  journal = {Operations Research Perspectives},
  volume = {7},
  pages = {100154},
  issn = {2214-7160},
  doi = {10.1016/j.orp.2020.100154},
  abstract = {This study presents an improvement to the mean-variance portfolio optimization model, by considering both the integer transaction lots and a robust estimator of the covariance matrices. Four robust estimators were tested, namely the Minimum Covariance Determinant, the S, the MM, and the Orthogonalized Gnanadesikan\textendash Kettenring estimator. These integer optimization problems were solved using genetic algorithms. We introduce the lot turnover measure, a modified portfolio turnover, and the Robust Sharpe Ratio as the measure of portfolio performance. Based on the simulation studies and the empirical results, this study shows that the robust estimators outperform the classical MLE when data contain outliers and when the lots have moderate sizes, e.g.~500 shares or less per lot.},
  langid = {english},
  keywords = {Finance,Genetic algorithm,Markowitz portfolio,Robust estimation,Transaction lots},
  file = {/Users/gregoirepacreau/Zotero/storage/A7YNSUWG/Rosadi et al. - 2020 - Robust covariance estimators for mean-variance por.pdf;/Users/gregoirepacreau/Zotero/storage/FKKBN7K4/S2214716020300440.html}
}

@article{rousseeuwDetectingDeviatingData2018,
  title = {Detecting Deviating Data Cells},
  author = {Rousseeuw, Peter J. and den Bossche, Wannes Van},
  year = {2018},
  month = apr,
  journal = {Technometrics},
  volume = {60},
  number = {2},
  eprint = {1601.07251},
  eprinttype = {arxiv},
  primaryclass = {stat},
  pages = {135--145},
  issn = {0040-1706, 1537-2723},
  doi = {10.1080/00401706.2017.1340909},
  abstract = {A multivariate dataset consists of \$n\$ cases in \$d\$ dimensions, and is often stored in an \$n\$ by \$d\$ data matrix. It is well-known that real data may contain outliers. Depending on the situation, outliers may be (a) undesirable errors which can adversely affect the data analysis, or (b) valuable nuggets of unexpected information. In statistics and data analysis the word outlier usually refers to a row of the data matrix, and the methods to detect such outliers only work when at least half the rows are clean. But often many rows have a few contaminated cell values, which may not be visible by looking at each variable (column) separately. We propose the first method to detect deviating data cells in a multivariate sample which takes the correlations between the variables into account. It has no restriction on the number of clean rows, and can deal with high dimensions. Other advantages are that it provides estimates of the `expected' values of the outlying cells, while imputing missing values at the same time. We illustrate the method on several real data sets, where it uncovers more structure than found by purely columnwise methods or purely rowwise methods. The proposed method can help to diagnose why a certain row is outlying, e.g. in process control. It may also serve as an initial step for estimating multivariate location and scatter matrices.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/gregoirepacreau/Zotero/storage/GQ2UPGXG/Rousseeuw et Bossche - 2018 - Detecting deviating data cells.pdf;/Users/gregoirepacreau/Zotero/storage/2GLUECA7/1601.html}
}

@inproceedings{rousseeuwRobustRegressionMeans1984,
  title = {Robust {{Regression}} by {{Means}} of {{S-Estimators}}},
  booktitle = {Robust and {{Nonlinear Time Series Analysis}}},
  author = {Rousseeuw, P. and Yohai, V.},
  editor = {Franke, J{\"u}rgen and H{\"a}rdle, Wolfgang and Martin, Douglas},
  year = {1984},
  series = {Lecture {{Notes}} in {{Statistics}}},
  pages = {256--272},
  publisher = {{Springer US}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4615-7821-5_15},
  abstract = {There are at least two reasons why robust regression techniques are useful tools in robust time series analysis. First of all, one often wants to estimate autoregressive parameters in a robust way, and secondly, one sometimes has to fit a linear or nonlinear trend to a time series. In this paper we shall develop a class of methods for robust regression, and briefly comment on their use in time series. These new estimators are introduced because of their invulnerability to large fractions of contaminated data. We propose to call them ``S-estimators'' because they are based on estimators of scale.},
  isbn = {978-1-4615-7821-5},
  langid = {english},
  keywords = {American Statistical Association,Breakdown Point,Leverage Point,Projection Pursuit,Robust Regression},
  file = {/Users/gregoirepacreau/Zotero/storage/UYCEFQMM/Rousseeuw et Yohai - 1984 - Robust Regression by Means of S-Estimators.pdf}
}

@article{rubinInferenceMissingData1976,
  title = {Inference and {{Missing Data}}},
  author = {Rubin, Donald B.},
  year = {1976},
  journal = {Biometrika},
  volume = {63},
  number = {3},
  pages = {581--592},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.2307/2335739},
  abstract = {When making sampling distribution inferences about the parameter of the data, \texttheta, it is appropriate to ignore the process that causes missing data if the missing data are `missing at random' and the observed data are `observed at random', but these inferences are generally conditional on the observed pattern of missing data. When making direct-likelihood or Bayesian inferences about \texttheta, it is appropriate to ignore the process that causes missing data if the missing data are missing at random and the parameter of the missing data process is `distinct' from \texttheta. These conditions are the weakest general conditions under which ignoring the process that causes missing data always leads to correct inferences.}
}

@misc{santoroSimpleNeuralNetwork2017,
  title = {A Simple Neural Network Module for Relational Reasoning},
  author = {Santoro, Adam and Raposo, David and Barrett, David G. T. and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
  year = {2017},
  month = jun,
  number = {arXiv:1706.01427},
  eprint = {1706.01427},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1706.01427},
  abstract = {Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/AT2EFZIX/Santoro et al. - 2017 - A simple neural network module for relational reas.pdf;/Users/gregoirepacreau/Zotero/storage/BUCR983I/1706.html}
}

@article{shimizuFindingCausalOrdering2006,
  title = {Finding a Causal Ordering via Independent Component Analysis},
  author = {Shimizu, Shohei and Hyv{\"a}rinen, Aapo and Hoyer, Patrik O. and Kano, Yutaka},
  year = {2006},
  month = jul,
  journal = {Computational Statistics \& Data Analysis},
  volume = {50},
  number = {11},
  pages = {3278--3293},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2005.05.004},
  abstract = {The application of independent component analysis to discovery of a causal ordering between observed variables is studied. Path analysis is a widely-used method for causal analysis. It is of confirmatory nature and can provide statistical tests for assumed causal relations based on comparison of the implied covariance matrix with a sample covariance. However, it is based on the assumption of normality and only uses the covariance structure, which is why it has several problems, for example, one cannot find the causal direction between two variables if only those two variables are observed because the two models to be compared are equivalent to each other. A new statistical method for discovery of a causal ordering using non-normality of observed variables is developed to provide a partial solution to the problem.},
  langid = {english},
  keywords = {Causal inference,Independence,Independent component analysis,Non-experimental data,Non-normality},
  file = {/Users/gregoirepacreau/Zotero/storage/LPZ4Q3GB/S0167947305001234.html}
}

@inproceedings{socherRecursiveDeepModels2013,
  title = {Recursive {{Deep Models}} for {{Semantic Compositionality Over}} a {{Sentiment Treebank}}},
  booktitle = {Proceedings of the 2013 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D. and Ng, Andrew and Potts, Christopher},
  year = {2013},
  month = oct,
  pages = {1631--1642},
  publisher = {{Association for Computational Linguistics}},
  address = {{Seattle, Washington, USA}},
  file = {/Users/gregoirepacreau/Zotero/storage/XXQPALV4/Socher et al. - 2013 - Recursive Deep Models for Semantic Compositionalit.pdf}
}

@article{suRobustVariableSelection2021,
  title = {Robust {{Variable Selection}} under {{Cellwise Contamination}}},
  author = {Su, Peng and Tarr, Garth and Muller, Samuel},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.12406 [stat]},
  eprint = {2110.12406},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Cellwise outliers are widespread in data and traditional robust methods may fail when applied to datasets under such contamination. We propose a variable selection procedure, that uses a pairwise robust estimator to obtain an initial empirical covariance matrix among the response and potentially many predictors. Then we replace the primary design matrix and the response vector with their robust counterparts based on the estimated covariance matrix. Finally, we adopt the adaptive Lasso to obtain variable selection results. The proposed approach is robust to cellwise outliers in regular and high dimensional settings and empirical results show good performance in comparison with recently proposed alternative robust approaches, particularly in the challenging setting when contamination rates are high but the magnitude of outliers is moderate. Real data applications demonstrate the practical utility of the proposed method.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {/Users/gregoirepacreau/Zotero/storage/YD4FH53L/Su et al. - 2021 - Robust Variable Selection under Cellwise Contamina.pdf;/Users/gregoirepacreau/Zotero/storage/FWBWNEAL/2110.html}
}

@article{thompsonPrincipalSubmatricesNormal1966,
  title = {Principal Submatrices of Normal and {{Hermitian}} Matrices},
  author = {Thompson, R. C.},
  year = {1966},
  month = jun,
  journal = {Illinois Journal of Mathematics},
  volume = {10},
  number = {2},
  pages = {296--308},
  publisher = {{Duke University Press}},
  issn = {0019-2082, 1945-6581},
  doi = {10.1215/ijm/1256055111},
  abstract = {Illinois Journal of Mathematics},
  keywords = {15.55},
  file = {/Users/gregoirepacreau/Zotero/storage/VB2AZ9JM/Thompson - 1966 - Principal submatrices of normal and Hermitian matr.pdf;/Users/gregoirepacreau/Zotero/storage/6GFECPYK/1256055111.html}
}

@incollection{tsybakovAsymptoticEfficiencyAdaptation2009,
  title = {Asymptotic Efficiency and Adaptation},
  booktitle = {Introduction to {{Nonparametric Estimation}}},
  author = {Tsybakov, Alexandre B.},
  editor = {Tsybakov, Alexandre B.},
  year = {2009},
  series = {Springer {{Series}} in {{Statistics}}},
  pages = {137--190},
  publisher = {{Springer}},
  address = {{New York, NY}},
  doi = {10.1007/978-0-387-79052-7_3},
  isbn = {978-0-387-79052-7},
  langid = {english},
  keywords = {Adaptive Estimator,Linear Estimator,Minimax Risk,Sobolev Class,Unbiased Estimator}
}

@incollection{tsybakovLowerBoundsMinimax2009,
  title = {Lower Bounds on the Minimax Risk},
  booktitle = {Introduction to {{Nonparametric Estimation}}},
  author = {Tsybakov, Alexandre B.},
  editor = {Tsybakov, Alexandre B.},
  year = {2009},
  series = {Springer {{Series}} in {{Statistics}}},
  pages = {77--135},
  publisher = {{Springer}},
  address = {{New York, NY}},
  doi = {10.1007/978-0-387-79052-7_2},
  isbn = {978-0-387-79052-7},
  langid = {english},
  keywords = {Hellinger Distance,Lower Bound,Optimal Rate,Probability Measure,Sobolev Class}
}

@incollection{tsybakovNonparametricEstimators2009,
  title = {Nonparametric Estimators},
  booktitle = {Introduction to {{Nonparametric Estimation}}},
  author = {Tsybakov, Alexandre B.},
  editor = {Tsybakov, Alexandre B.},
  year = {2009},
  series = {Springer {{Series}} in {{Statistics}}},
  pages = {1--76},
  publisher = {{Springer}},
  address = {{New York, NY}},
  doi = {10.1007/978-0-387-79052-7_1},
  isbn = {978-0-387-79052-7},
  langid = {english},
  keywords = {Kernel Estimator,Nonparametric Estimator,Nonparametric Regression,Sobolev Class,Unbiased Estimator}
}

@incollection{tukeyNintherTechniqueLowEffort1978,
  title = {The {{Ninther}}, a {{Technique}} for {{Low-Effort Robust}} ({{Resistant}}) {{Location}} in {{Large Samples}}},
  booktitle = {Contributions to {{Survey Sampling}} and {{Applied Statistics}}},
  author = {Tukey, John W.},
  editor = {David, H. A.},
  year = {1978},
  month = jan,
  pages = {251--257},
  publisher = {{Academic Press}},
  doi = {10.1016/B978-0-12-204750-3.50024-1},
  abstract = {The ninther, a median of medians, has a number of desirable features as a constituent part of procedures to summarize a large number of values: it reduces the number of values by a factor of nine; it tends to shed the influence of ``wild'' values (unless they are clustered together fairly tightly); its Gaussian efficiency is 55\% (its stretched-tail efficiency is higher); it can be computed with no arithmetic operations and an average of about 1.1 comparisons per input data value. Various specific uses are suggested.},
  isbn = {978-0-12-204750-3},
  langid = {english},
  file = {/Users/gregoirepacreau/Zotero/storage/NRDY5J5S/B9780122047503500241.html}
}

@misc{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1706.03762},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/YMPBFVWF/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/Users/gregoirepacreau/Zotero/storage/N4HWHN52/1706.html}
}

@misc{velickovicGraphAttentionNetworks2018,
  title = {Graph {{Attention Networks}}},
  author = {Veli{\v c}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  year = {2018},
  month = feb,
  number = {arXiv:1710.10903},
  eprint = {1710.10903},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1710.10903},
  abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/4DREZIWR/Veliƒçkoviƒá et al. - 2018 - Graph Attention Networks.pdf;/Users/gregoirepacreau/Zotero/storage/8QSQ7VC4/1710.html}
}

@misc{vershyninIntroductionNonasymptoticAnalysis2011,
  title = {Introduction to the Non-Asymptotic Analysis of Random Matrices},
  author = {Vershynin, Roman},
  year = {2011},
  month = nov,
  number = {arXiv:1011.3027},
  eprint = {1011.3027},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1011.3027},
  abstract = {This is a tutorial on some basic non-asymptotic methods and concepts in random matrix theory. The reader will learn several tools for the analysis of the extreme singular values of random matrices with independent rows or columns. Many of these methods sprung off from the development of geometric functional analysis since the 1970's. They have applications in several fields, most notably in theoretical computer science, statistics and signal processing. A few basic applications are covered in this text, particularly for the problem of estimating covariance matrices in statistics and for validating probabilistic constructions of measurement matrices in compressed sensing. These notes are written particularly for graduate students and beginning researchers in different areas, including functional analysts, probabilists, theoretical statisticians, electrical engineers, and theoretical computer scientists.},
  archiveprefix = {arXiv},
  keywords = {60B20; 46B09,Mathematics - Functional Analysis,Mathematics - Numerical Analysis,Mathematics - Probability},
  file = {/Users/gregoirepacreau/Zotero/storage/SX69RAWG/Vershynin - 2011 - Introduction to the non-asymptotic analysis of ran.pdf;/Users/gregoirepacreau/Zotero/storage/EC7B8QM4/1011.html}
}

@article{virmauxLipschitzRegularityDeep,
  title = {Lipschitz Regularity of Deep Neural Networks: Analysis and Efficient Estimation},
  author = {Virmaux, Aladin and Scaman, Kevin},
  pages = {10},
  abstract = {Deep neural networks are notorious for being sensitive to small well-chosen perturbations, and estimating the regularity of such architectures is of utmost importance for safe and robust practical applications. In this paper, we investigate one of the key characteristics to assess the regularity of such methods: the Lipschitz constant of deep learning architectures. First, we show that, even for two layer neural networks, the exact computation of this quantity is NP-hard and state-ofart methods may significantly overestimate it. Then, we both extend and improve previous estimation methods by providing AutoLip, the first generic algorithm for upper bounding the Lipschitz constant of any automatically differentiable function. We provide a power method algorithm working with automatic differentiation, allowing efficient computations even on large convolutions. Second, for sequential neural networks, we propose an improved algorithm named SeqLip that takes advantage of the linear computation graph to split the computation per pair of consecutive layers. Third we propose heuristics on SeqLip in order to tackle very large networks. Our experiments show that SeqLip can significantly improve on the existing upper bounds. Finally, we provide an implementation of AutoLip in the PyTorch environment that may be used to better estimate the robustness of a given neural network to small perturbations or regularize it using more precise Lipschitz estimations.},
  langid = {english},
  file = {/Users/gregoirepacreau/Zotero/storage/DWPSK7ZM/Virmaux et Scaman - Lipschitz regularity of deep neural networks anal.pdf}
}

@article{vyrostNetworkbasedAssetAllocation2019,
  title = {Network-Based Asset Allocation Strategies},
  author = {V{\'y}rost, Tomas and Ly{\'o}csa, {\v S}tefan and Baum{\"o}hl, Eduard},
  year = {2019},
  month = jan,
  journal = {The North American Journal of Economics and Finance},
  volume = {47},
  pages = {516--536},
  issn = {1062-9408},
  doi = {10.1016/j.najef.2018.06.008},
  abstract = {In this study, we construct financial networks in which nodes are represented by assets and where edges are based on long-run correlations. We construct four networks (complete graph, a minimum spanning tree, a planar maximally filtered graph, and a threshold significance graph) and use three centrality measures (betweenness, eigenvalue centrality, and the expected force). To improve risk-return characteristics of well-known return maximization and risk minimization benchmark portfolios, we propose simple adjustments to portfolio selection strategies that utilize centralization measures from financial networks. From a sample of 45 assets (stock market indices, bond and money market instruments, commodities, and foreign exchange rates) and from data for 1999 to 2015, we show that irrespective of the network and centrality employed, the proposed network-based asset allocation strategies improve key portfolio return characteristics in an out-of-sample framework, most notably, risk and left-tail risk-adjusted returns. Resolving portfolio model selection uncertainties further improves risk-return characteristics. Improvements made to portfolio strategies based on risk minimization are also robust to transaction costs.},
  langid = {english},
  keywords = {Centrality,Networks,Portfolio,Risk-return profile},
  file = {/Users/gregoirepacreau/Zotero/storage/AJ3MSH8Z/V√Ωrost et al. - 2019 - Network-based asset allocation strategies.pdf;/Users/gregoirepacreau/Zotero/storage/Y4FU7WPJ/S106294081830072X.html}
}

@misc{wangNonlocalNeuralNetworks2018,
  title = {Non-Local {{Neural Networks}}},
  author = {Wang, Xiaolong and Girshick, Ross and Gupta, Abhinav and He, Kaiming},
  year = {2018},
  month = apr,
  number = {arXiv:1711.07971},
  eprint = {1711.07971},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1711.07971},
  abstract = {Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time. In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This building block can be plugged into many computer vision architectures. On the task of video classification, even without any bells and whistles, our non-local models can compete or outperform current competition winners on both Kinetics and Charades datasets. In static image recognition, our non-local models improve object detection/segmentation and pose estimation on the COCO suite of tasks. Code is available at https://github.com/facebookresearch/video-nonlocal-net .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/gregoirepacreau/Zotero/storage/VSPDDBCR/Wang et al. - 2018 - Non-local Neural Networks.pdf;/Users/gregoirepacreau/Zotero/storage/ES9AYS4D/1711.html}
}

@misc{wangReviewGraphNeural2022,
  title = {A {{Review}} on {{Graph Neural Network Methods}} in {{Financial Applications}}},
  author = {Wang, Jianian and Zhang, Sheng and Xiao, Yanghua and Song, Rui},
  year = {2022},
  month = apr,
  number = {arXiv:2111.15367},
  eprint = {2111.15367},
  eprinttype = {arxiv},
  primaryclass = {cs, q-fin, stat},
  institution = {{arXiv}},
  abstract = {With multiple components and relations, financial data are often presented as graph data, since it could represent both the individual features and the complicated relations. Due to the complexity and volatility of the financial market, the graph constructed on the financial data is often heterogeneous or time-varying, which imposes challenges on modeling technology. Among the graph modeling technologies, graph neural network (GNN) models are able to handle the complex graph structure and achieve great performance and thus could be used to solve financial tasks. In this work, we provide a comprehensive review of GNN models in recent financial context. We first categorize the commonly-used financial graphs and summarize the feature processing step for each node. Then we summarize the GNN methodology for each graph type, application in each area, and propose some potential research areas.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Quantitative Finance - Statistical Finance,Statistics - Applications},
  file = {/Users/gregoirepacreau/Zotero/storage/VL6Q3JZW/Wang et al. - 2022 - A Review on Graph Neural Network Methods in Financ.pdf;/Users/gregoirepacreau/Zotero/storage/6MJNEC2V/2111.html}
}

@inproceedings{waqarPredictionStockMarket2017,
  title = {Prediction of {{Stock Market}} by {{Principal Component Analysis}}},
  booktitle = {2017 13th {{International Conference}} on {{Computational Intelligence}} and {{Security}} ({{CIS}})},
  author = {Waqar, Muhammad and Dawood, Hassan and Guo, Ping and Shahnawaz, Muhammad Bilal and Ghazanfar, Mustansar Ali},
  year = {2017},
  month = dec,
  pages = {599--602},
  doi = {10.1109/CIS.2017.00139},
  abstract = {The categorization of high dimensional data present a fascinating challenge to machine learning models as frequent number of highly correlated dimensions or attributes can affect the accuracy of classification model. In this paper, the problem of high dimensionality of stock exchange is investigated to predict the market trends by applying the principal component analysis (PCA) with linear regression. PCA can help to improve the predictive performance of machine learning methods while reducing the redundancy among the data. Experiments are carried out on a high dimensional spectral of 3 stock exchanges such as: New York Stock Exchange, London Stock Exchange and Karachi Stock Exchange. The accuracy of linear regression classification model is compared before and after applying PCA. The experiments show that PCA can improve the performance of machine learning in general if and only if relative correlation among input features is investigated and careful selection is done while choosing principal components. Root mean square error (RMSE) is used as an evaluation metric to evaluate the classification model.},
  keywords = {Biological system modeling,linear regression,Linear regression,Market research,Predictive models,principal component analysis,Principal component analysis,root mean sqaure error,stock exchange prediction,Stock markets,Support vector machines},
  file = {/Users/gregoirepacreau/Zotero/storage/WE9M2A75/Waqar et al. - 2017 - Prediction of Stock Market by Principal Component .pdf;/Users/gregoirepacreau/Zotero/storage/FV35PNI7/8288561.html}
}

@misc{wardPracticalTutorialGraph2021,
  title = {A {{Practical Tutorial}} on {{Graph Neural Networks}}},
  author = {Ward, Isaac Ronald and Joyner, Jack and Lickfold, Casey and Guo, Yulan and Bennamoun, Mohammed},
  year = {2021},
  month = dec,
  number = {arXiv:2010.05234},
  eprint = {2010.05234},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2010.05234},
  abstract = {Graph neural networks (GNNs) have recently grown in popularity in the field of artificial intelligence (AI) due to their unique ability to ingest relatively unstructured data types as input data. Although some elements of the GNN architecture are conceptually similar in operation to traditional neural networks (and neural network variants), other elements represent a departure from traditional deep learning techniques. This tutorial exposes the power and novelty of GNNs to AI practitioners by collating and presenting details regarding the motivations, concepts, mathematics, and applications of the most common and performant variants of GNNs. Importantly, we present this tutorial concisely, alongside practical examples, thus providing a practical and accessible tutorial on the topic of GNNs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {/Users/gregoirepacreau/Zotero/storage/IP48M3BY/Ward et al. - 2021 - A Practical Tutorial on Graph Neural Networks.pdf;/Users/gregoirepacreau/Zotero/storage/LNVUL5AC/2010.html}
}

@misc{wehenkelGraphicalNormalizingFlows2021,
  title = {Graphical {{Normalizing Flows}}},
  author = {Wehenkel, Antoine and Louppe, Gilles},
  year = {2021},
  month = feb,
  number = {arXiv:2006.02548},
  eprint = {2006.02548},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2006.02548},
  abstract = {Normalizing flows model complex probability distributions by combining a base distribution with a series of bijective neural networks. State-of-the-art architectures rely on coupling and autoregressive transformations to lift up invertible functions from scalars to vectors. In this work, we revisit these transformations as probabilistic graphical models, showing they reduce to Bayesian networks with a pre-defined topology and a learnable density at each node. From this new perspective, we propose the graphical normalizing flow, a new invertible transformation with either a prescribed or a learnable graphical structure. This model provides a promising way to inject domain knowledge into normalizing flows while preserving both the interpretability of Bayesian networks and the representation capacity of normalizing flows. We show that graphical conditioners discover relevant graph structure when we cannot hypothesize it. In addition, we analyze the effect of \$\textbackslash ell\_1\$-penalization on the recovered structure and on the quality of the resulting density estimation. Finally, we show that graphical conditioners lead to competitive white box density estimators. Our implementation is available at https://github.com/AWehenkel/DAG-NF.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/3HVD5FHU/Wehenkel et Louppe - 2021 - Graphical Normalizing Flows.pdf;/Users/gregoirepacreau/Zotero/storage/NPL9UCM5/2006.html}
}

@misc{wuSupplyChainNetwork2014,
  type = {{{SSRN Scholarly Paper}}},
  title = {Supply {{Chain Network Structure}} and {{Firm Returns}}},
  author = {Wu, Jing and Birge, John R.},
  year = {2014},
  month = jan,
  number = {2385217},
  address = {{Rochester, NY}},
  doi = {10.2139/ssrn.2385217},
  abstract = {The complexity and opacity of the network of interconnections among firms and their supply chains inhibits understanding of the impact of management decisions concerning the boundaries of the firm and the number and intensity of its relationships with suppliers and customers. Using recently available data on the relationships of public US firms, this paper investigates the effects of supply chain connections on firm performance as reflected in stock returns. The paper finds that supply chain structure is closely related to firm returns at two levels, a first-order effect from direct connections and a second-order impact from systemic exposures through the network. For the first order effect, using a cross-sectional data set of the supply chain network and monthly returns, we show that a firm's return can be explained by its concurrent supplier returns, concurrent customer returns, own momentum, and supplier momentum, whereas customer momentum has little impact. A long-short equity strategy based on the supplier momentum yields monthly abnormal returns of 56 basis points. This result implies investors' limited attention to supplier firms relative to customer firms and gradual diffusion of information downstream as opposed to upstream in the supply chain. For the second-order effect, we find a market anomaly by grouping firms according to their centrality in the supply chain. Specifically, manufacturing firms that are more central in the network earn lower returns, while logistics firms that are more central in the network earn higher returns. This result holds for both eigenvector centrality and in-degree centrality (number of suppliers). We argue that centrality and multiplicity of suppliers have different risk implications for firms operating in different industries. More central firms in manufacturing choose their suppliers to operationally hedge shocks transmitted from other firms and earn lower returns due to lower systematic risk. On the contrary, more central firms in logistics are shock aggregators, earning higher returns due to their exposure to greater systematic risk. Our results are robust after controlling for common asset pricing factors.},
  langid = {english},
  keywords = {Lead-lag Effect,Network Centrality,Supply Chain,Systematic Risk},
  file = {/Users/gregoirepacreau/Zotero/storage/AZZ94HCQ/Wu et Birge - 2014 - Supply Chain Network Structure and Firm Returns.pdf;/Users/gregoirepacreau/Zotero/storage/PHXXNSCR/papers.html}
}

@article{wuWuJingBirge2014,
  title = {Wu, {{Jing}} and {{Birge}}, {{John R}}., {{Supply Chain Network Structure}} and {{Firm Returns}} ({{January}} 24, 2014). {{Available}} at {{SSRN}}: {{https://ssrn.com/abstract=2385217}} or {{http://dx.doi.org/10.2139/ssrn.2385217}}},
  author = {Wu, Jing and Birge, John R.},
  year = {2014},
  month = jan,
  journal = {SSRN},
  doi = {http://dx.doi.org/10.2139/ssrn.2385217}
}

@article{xiongPushingBoundariesMolecular2020,
  title = {Pushing the {{Boundaries}} of {{Molecular Representation}} for {{Drug Discovery}} with the {{Graph Attention Mechanism}}},
  author = {Xiong, Zhaoping and Wang, Dingyan and Liu, Xiaohong and Zhong, Feisheng and Wan, Xiaozhe and Li, Xutong and Li, Zhaojun and Luo, Xiaomin and Chen, Kaixian and Jiang, Hualiang and Zheng, Mingyue},
  year = {2020},
  month = aug,
  journal = {Journal of Medicinal Chemistry},
  volume = {63},
  number = {16},
  pages = {8749--8760},
  publisher = {{American Chemical Society}},
  issn = {0022-2623},
  doi = {10.1021/acs.jmedchem.9b00959},
  abstract = {Hunting for chemicals with favorable pharmacological, toxicological, and pharmacokinetic properties remains a formidable challenge for drug discovery. Deep learning provides us with powerful tools to build predictive models that are appropriate for the rising amounts of data, but the gap between what these neural networks learn and what human beings can comprehend is growing. Moreover, this gap may induce distrust and restrict deep learning applications in practice. Here, we introduce a new graph neural network architecture called Attentive FP for molecular representation that uses a graph attention mechanism to learn from relevant drug discovery data sets. We demonstrate that Attentive FP achieves state-of-the-art predictive performances on a variety of data sets and that what it learns is interpretable. The feature visualization for Attentive FP suggests that it automatically learns nonlocal intramolecular interactions from specified tasks, which can help us gain chemical insights directly from data beyond human perception.}
}

@article{xuAdversarialAttacksDefenses2019,
  title = {Adversarial {{Attacks}} and {{Defenses}} in {{Images}}, {{Graphs}} and {{Text}}: {{A Review}}},
  shorttitle = {Adversarial {{Attacks}} and {{Defenses}} in {{Images}}, {{Graphs}} and {{Text}}},
  author = {Xu, Han and Ma, Yao and Liu, Haochen and Deb, Debayan and Liu, Hui and Tang, Jiliang and Jain, Anil K.},
  year = {2019},
  month = oct,
  journal = {arXiv:1909.08072 [cs, stat]},
  eprint = {1909.08072},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Deep neural networks (DNN) have achieved unprecedented success in numerous machine learning tasks in various domains. However, the existence of adversarial examples has raised concerns about applying deep learning to safety-critical applications. As a result, we have witnessed increasing interests in studying attack and defense mechanisms for DNN models on different data types, such as images, graphs and text. Thus, it is necessary to provide a systematic and comprehensive overview of the main threats of attacks and the success of corresponding countermeasures. In this survey, we review the state of the art algorithms for generating adversarial examples and the countermeasures against adversarial examples, for the three popular data types, i.e., images, graphs and text.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/IDQRMIAB/Xu et al. - 2019 - Adversarial Attacks and Defenses in Images, Graphs.pdf;/Users/gregoirepacreau/Zotero/storage/6CTFPN8B/1909.html}
}

@misc{yangHistoryPCANew2018,
  title = {History {{PCA}}: {{A New Algorithm}} for {{Streaming PCA}}},
  shorttitle = {History {{PCA}}},
  author = {Yang, Puyudi and Hsieh, Cho-Jui and Wang, Jane-Ling},
  year = {2018},
  month = feb,
  number = {arXiv:1802.05447},
  eprint = {1802.05447},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1802.05447},
  abstract = {In this paper we propose a new algorithm for streaming principal component analysis. With limited memory, small devices cannot store all the samples in the high-dimensional regime. Streaming principal component analysis aims to find the \$k\$-dimensional subspace which can explain the most variation of the \$d\$-dimensional data points that come into memory sequentially. In order to deal with large \$d\$ and large \$N\$ (number of samples), most streaming PCA algorithms update the current model using only the incoming sample and then dump the information right away to save memory. However the information contained in previously streamed data could be useful. Motivated by this idea, we develop a new streaming PCA algorithm called History PCA that achieves this goal. By using \$O(Bd)\$ memory with \$B\textbackslash approx 10\$ being the block size, our algorithm converges much faster than existing streaming PCA algorithms. By changing the number of inner iterations, the memory usage can be further reduced to \$O(d)\$ while maintaining a comparable convergence speed. We provide theoretical guarantees for the convergence of our algorithm along with the rate of convergence. We also demonstrate on synthetic and real world data sets that our algorithm compares favorably with other state-of-the-art streaming PCA methods in terms of the convergence speed and performance.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/DHFTBD39/Yang et al. - 2018 - History PCA A New Algorithm for Streaming PCA.pdf;/Users/gregoirepacreau/Zotero/storage/EZDD3R7L/1802.html}
}

@misc{yangImpactRepresentationLearning2021,
  title = {Impact of {{Representation Learning}} in {{Linear Bandits}}},
  author = {Yang, Jiaqi and Hu, Wei and Lee, Jason D. and Du, Simon S.},
  year = {2021},
  month = may,
  number = {arXiv:2010.06531},
  eprint = {2010.06531},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2010.06531},
  abstract = {We study how representation learning can improve the efficiency of bandit problems. We study the setting where we play \$T\$ linear bandits with dimension \$d\$ concurrently, and these \$T\$ bandit tasks share a common \$k (\textbackslash ll d)\$ dimensional linear representation. For the finite-action setting, we present a new algorithm which achieves \$\textbackslash widetilde\{O\}(T\textbackslash sqrt\{kN\} + \textbackslash sqrt\{dkNT\})\$ regret, where \$N\$ is the number of rounds we play for each bandit. When \$T\$ is sufficiently large, our algorithm significantly outperforms the naive algorithm (playing \$T\$ bandits independently) that achieves \$\textbackslash widetilde\{O\}(T\textbackslash sqrt\{d N\})\$ regret. We also provide an \$\textbackslash Omega(T\textbackslash sqrt\{kN\} + \textbackslash sqrt\{dkNT\})\$ regret lower bound, showing that our algorithm is minimax-optimal up to poly-logarithmic factors. Furthermore, we extend our algorithm to the infinite-action setting and obtain a corresponding regret bound which demonstrates the benefit of representation learning in certain regimes. We also present experiments on synthetic and real-world data to illustrate our theoretical findings and demonstrate the effectiveness of our proposed algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/gregoirepacreau/Zotero/storage/XFY4EI9Y/Yang et al. - 2021 - Impact of Representation Learning in Linear Bandit.pdf;/Users/gregoirepacreau/Zotero/storage/3JFKVYKR/2010.html}
}

@article{zotero-47,
  type = {Article}
}
